{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training data: 2552082\n"
     ]
    }
   ],
   "source": [
    "root = '../'\n",
    "\n",
    "# Training data\n",
    "train = pd.read_csv(root + \"data_exports/joined_train_data_segmented.csv\")\n",
    "\n",
    "print(f'Total number of training data: {len(train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>time</th>\n",
       "      <th>watch_ratio</th>\n",
       "      <th>user_active_degree</th>\n",
       "      <th>is_lowactive_period</th>\n",
       "      <th>is_live_streamer</th>\n",
       "      <th>is_video_author</th>\n",
       "      <th>follow_user_num</th>\n",
       "      <th>fans_user_num</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_daily_watch_time</th>\n",
       "      <th>top_3_categories</th>\n",
       "      <th>cluster</th>\n",
       "      <th>News_Politics</th>\n",
       "      <th>Auto_Tech</th>\n",
       "      <th>Lifestyle</th>\n",
       "      <th>Sports_Fitness</th>\n",
       "      <th>Entertainment</th>\n",
       "      <th>Culture</th>\n",
       "      <th>Others</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>148</td>\n",
       "      <td>2020-07-05 05:27:48.378</td>\n",
       "      <td>0.722103</td>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>8.360719e+12</td>\n",
       "      <td>['Car', 'Pets', 'Real estate家居']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>183</td>\n",
       "      <td>2020-07-05 05:28:00.057</td>\n",
       "      <td>1.907377</td>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>8.360719e+12</td>\n",
       "      <td>['Car', 'Pets', 'Real estate家居']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>3649</td>\n",
       "      <td>2020-07-05 05:29:09.479</td>\n",
       "      <td>2.063311</td>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>8.360719e+12</td>\n",
       "      <td>['Car', 'Pets', 'Real estate家居']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>5262</td>\n",
       "      <td>2020-07-05 05:30:43.285</td>\n",
       "      <td>0.566388</td>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>8.360719e+12</td>\n",
       "      <td>['Car', 'Pets', 'Real estate家居']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>8234</td>\n",
       "      <td>2020-07-05 05:35:43.459</td>\n",
       "      <td>0.418364</td>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>8.360719e+12</td>\n",
       "      <td>['Car', 'Pets', 'Real estate家居']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  video_id                     time  watch_ratio user_active_degree  \\\n",
       "0       14       148  2020-07-05 05:27:48.378     0.722103        full_active   \n",
       "1       14       183  2020-07-05 05:28:00.057     1.907377        full_active   \n",
       "2       14      3649  2020-07-05 05:29:09.479     2.063311        full_active   \n",
       "3       14      5262  2020-07-05 05:30:43.285     0.566388        full_active   \n",
       "4       14      8234  2020-07-05 05:35:43.459     0.418364        full_active   \n",
       "\n",
       "   is_lowactive_period  is_live_streamer  is_video_author  follow_user_num  \\\n",
       "0                    0                 0                1               73   \n",
       "1                    0                 0                1               73   \n",
       "2                    0                 0                1               73   \n",
       "3                    0                 0                1               73   \n",
       "4                    0                 0                1               73   \n",
       "\n",
       "   fans_user_num  ...  avg_daily_watch_time                  top_3_categories  \\\n",
       "0              6  ...          8.360719e+12  ['Car', 'Pets', 'Real estate家居']   \n",
       "1              6  ...          8.360719e+12  ['Car', 'Pets', 'Real estate家居']   \n",
       "2              6  ...          8.360719e+12  ['Car', 'Pets', 'Real estate家居']   \n",
       "3              6  ...          8.360719e+12  ['Car', 'Pets', 'Real estate家居']   \n",
       "4              6  ...          8.360719e+12  ['Car', 'Pets', 'Real estate家居']   \n",
       "\n",
       "   cluster News_Politics Auto_Tech  Lifestyle  Sports_Fitness  Entertainment  \\\n",
       "0        0             0         1          1               0              0   \n",
       "1        0             0         1          1               0              0   \n",
       "2        0             0         1          1               0              0   \n",
       "3        0             0         1          1               0              0   \n",
       "4        0             0         1          1               0              0   \n",
       "\n",
       "   Culture  Others  \n",
       "0        0       1  \n",
       "1        0       1  \n",
       "2        0       1  \n",
       "3        0       1  \n",
       "4        0       1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the current date\n",
    "This is necessary to calculate the number of days since the last interaction as well as the age of the videos, which will be used for the time decay component of our model.\n",
    "We assume it to be the day of the latest interaction in the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current date: 2020-08-03\n"
     ]
    }
   ],
   "source": [
    "# Convert type to datetime\n",
    "train['time'] = pd.to_datetime(train['time'])\n",
    "\n",
    "# Assume current date is the next day of the last date in the training data\n",
    "CURRENT_DATE = train['time'].dt.date.max()\n",
    "\n",
    "# Just the date portion\n",
    "print(f'Current date: {CURRENT_DATE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate age of video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_info = pd.read_csv(root + 'data/item_daily_features.csv', usecols=['video_id', 'upload_dt']).drop_duplicates()\n",
    "\n",
    "video_info['upload_dt'] = pd.to_datetime(video_info['upload_dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/24/w8t485397_n5ygqwgnjns3_80000gn/T/ipykernel_14971/1355271191.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  video_info_filtered['video_age'] = (pd.to_datetime(CURRENT_DATE) - video_info_filtered['upload_dt']).dt.days\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{103: 38,\n",
       " 109: 37,\n",
       " 120: 36,\n",
       " 122: 35,\n",
       " 128: 34,\n",
       " 130: 33,\n",
       " 131: 33,\n",
       " 133: 33,\n",
       " 136: 33,\n",
       " 137: 33,\n",
       " 139: 33,\n",
       " 142: 33,\n",
       " 145: 32,\n",
       " 146: 32,\n",
       " 147: 32,\n",
       " 148: 32,\n",
       " 151: 31,\n",
       " 152: 31,\n",
       " 153: 31,\n",
       " 154: 31,\n",
       " 164: 30,\n",
       " 166: 30,\n",
       " 168: 30,\n",
       " 169: 30,\n",
       " 170: 30,\n",
       " 171: 30,\n",
       " 173: 30,\n",
       " 175: 30,\n",
       " 179: 30,\n",
       " 180: 30,\n",
       " 183: 30,\n",
       " 186: 30,\n",
       " 188: 30,\n",
       " 203: 29,\n",
       " 206: 29,\n",
       " 207: 29,\n",
       " 210: 29,\n",
       " 211: 29,\n",
       " 217: 29,\n",
       " 223: 28,\n",
       " 229: 28,\n",
       " 237: 28,\n",
       " 238: 28,\n",
       " 250: 28,\n",
       " 251: 28,\n",
       " 254: 28,\n",
       " 255: 28,\n",
       " 256: 28,\n",
       " 258: 28,\n",
       " 262: 28,\n",
       " 265: 28,\n",
       " 267: 28,\n",
       " 270: 28,\n",
       " 272: 28,\n",
       " 275: 27,\n",
       " 279: 27,\n",
       " 280: 27,\n",
       " 282: 27,\n",
       " 285: 27,\n",
       " 286: 27,\n",
       " 288: 27,\n",
       " 289: 27,\n",
       " 290: 27,\n",
       " 296: 27,\n",
       " 297: 27,\n",
       " 300: 27,\n",
       " 302: 27,\n",
       " 304: 27,\n",
       " 306: 27,\n",
       " 307: 27,\n",
       " 314: 27,\n",
       " 318: 27,\n",
       " 319: 27,\n",
       " 324: 26,\n",
       " 331: 26,\n",
       " 336: 26,\n",
       " 340: 26,\n",
       " 349: 26,\n",
       " 350: 26,\n",
       " 351: 26,\n",
       " 361: 25,\n",
       " 364: 25,\n",
       " 365: 25,\n",
       " 368: 25,\n",
       " 369: 25,\n",
       " 372: 25,\n",
       " 373: 25,\n",
       " 378: 25,\n",
       " 388: 25,\n",
       " 389: 25,\n",
       " 390: 25,\n",
       " 391: 25,\n",
       " 392: 25,\n",
       " 394: 25,\n",
       " 395: 25,\n",
       " 400: 25,\n",
       " 402: 25,\n",
       " 403: 25,\n",
       " 408: 24,\n",
       " 413: 24,\n",
       " 414: 24,\n",
       " 416: 24,\n",
       " 418: 24,\n",
       " 423: 24,\n",
       " 425: 24,\n",
       " 429: 24,\n",
       " 430: 24,\n",
       " 432: 24,\n",
       " 433: 24,\n",
       " 435: 24,\n",
       " 437: 24,\n",
       " 445: 24,\n",
       " 446: 24,\n",
       " 456: 23,\n",
       " 457: 23,\n",
       " 460: 23,\n",
       " 462: 23,\n",
       " 463: 23,\n",
       " 465: 23,\n",
       " 466: 23,\n",
       " 468: 23,\n",
       " 471: 23,\n",
       " 472: 23,\n",
       " 478: 23,\n",
       " 479: 23,\n",
       " 481: 23,\n",
       " 491: 22,\n",
       " 493: 22,\n",
       " 494: 22,\n",
       " 497: 22,\n",
       " 501: 22,\n",
       " 503: 22,\n",
       " 507: 22,\n",
       " 508: 22,\n",
       " 509: 21,\n",
       " 510: 21,\n",
       " 513: 21,\n",
       " 514: 21,\n",
       " 516: 21,\n",
       " 517: 21,\n",
       " 518: 21,\n",
       " 520: 21,\n",
       " 521: 21,\n",
       " 522: 21,\n",
       " 523: 21,\n",
       " 525: 21,\n",
       " 530: 20,\n",
       " 531: 20,\n",
       " 532: 20,\n",
       " 533: 20,\n",
       " 534: 20,\n",
       " 535: 20,\n",
       " 536: 20,\n",
       " 537: 20,\n",
       " 538: 20,\n",
       " 540: 20,\n",
       " 541: 20,\n",
       " 542: 20,\n",
       " 543: 20,\n",
       " 544: 20,\n",
       " 545: 20,\n",
       " 548: 20,\n",
       " 551: 19,\n",
       " 552: 19,\n",
       " 554: 19,\n",
       " 555: 19,\n",
       " 556: 19,\n",
       " 557: 19,\n",
       " 558: 19,\n",
       " 560: 19,\n",
       " 561: 19,\n",
       " 562: 19,\n",
       " 563: 19,\n",
       " 564: 19,\n",
       " 565: 19,\n",
       " 567: 19,\n",
       " 568: 19,\n",
       " 569: 19,\n",
       " 570: 19,\n",
       " 571: 18,\n",
       " 573: 18,\n",
       " 574: 18,\n",
       " 575: 18,\n",
       " 576: 18,\n",
       " 577: 18,\n",
       " 578: 18,\n",
       " 579: 18,\n",
       " 580: 18,\n",
       " 581: 18,\n",
       " 582: 18,\n",
       " 583: 18,\n",
       " 584: 18,\n",
       " 585: 18,\n",
       " 586: 18,\n",
       " 587: 17,\n",
       " 589: 17,\n",
       " 590: 17,\n",
       " 591: 17,\n",
       " 592: 17,\n",
       " 593: 17,\n",
       " 594: 17,\n",
       " 596: 17,\n",
       " 597: 17,\n",
       " 599: 17,\n",
       " 600: 17,\n",
       " 601: 16,\n",
       " 602: 16,\n",
       " 603: 16,\n",
       " 605: 16,\n",
       " 606: 16,\n",
       " 607: 16,\n",
       " 608: 16,\n",
       " 610: 16,\n",
       " 611: 16,\n",
       " 612: 16,\n",
       " 613: 16,\n",
       " 614: 16,\n",
       " 615: 16,\n",
       " 616: 16,\n",
       " 617: 16,\n",
       " 618: 16,\n",
       " 619: 16,\n",
       " 621: 16,\n",
       " 622: 15,\n",
       " 624: 15,\n",
       " 625: 15,\n",
       " 626: 15,\n",
       " 627: 15,\n",
       " 628: 15,\n",
       " 629: 15,\n",
       " 630: 15,\n",
       " 631: 15,\n",
       " 632: 15,\n",
       " 633: 15,\n",
       " 634: 15,\n",
       " 635: 15,\n",
       " 636: 14,\n",
       " 637: 14,\n",
       " 638: 14,\n",
       " 639: 14,\n",
       " 640: 14,\n",
       " 641: 14,\n",
       " 643: 14,\n",
       " 644: 14,\n",
       " 645: 14,\n",
       " 646: 14,\n",
       " 647: 14,\n",
       " 649: 14,\n",
       " 650: 14,\n",
       " 651: 14,\n",
       " 653: 14,\n",
       " 655: 14,\n",
       " 657: 14,\n",
       " 658: 14,\n",
       " 659: 14,\n",
       " 660: 14,\n",
       " 661: 13,\n",
       " 662: 13,\n",
       " 663: 13,\n",
       " 664: 13,\n",
       " 665: 13,\n",
       " 666: 13,\n",
       " 667: 13,\n",
       " 668: 13,\n",
       " 669: 13,\n",
       " 670: 13,\n",
       " 671: 13,\n",
       " 672: 13,\n",
       " 673: 13,\n",
       " 674: 13,\n",
       " 675: 13,\n",
       " 676: 13,\n",
       " 677: 13,\n",
       " 679: 13,\n",
       " 681: 13,\n",
       " 682: 12,\n",
       " 683: 12,\n",
       " 684: 12,\n",
       " 685: 12,\n",
       " 686: 12,\n",
       " 689: 12,\n",
       " 690: 12,\n",
       " 691: 12,\n",
       " 692: 12,\n",
       " 693: 12,\n",
       " 694: 12,\n",
       " 695: 12,\n",
       " 696: 11,\n",
       " 698: 11,\n",
       " 701: 11,\n",
       " 702: 11,\n",
       " 705: 11,\n",
       " 706: 11,\n",
       " 708: 11,\n",
       " 709: 10,\n",
       " 711: 10,\n",
       " 712: 10,\n",
       " 714: 10,\n",
       " 715: 10,\n",
       " 716: 10,\n",
       " 720: 10,\n",
       " 721: 10,\n",
       " 722: 10,\n",
       " 723: 9,\n",
       " 724: 9,\n",
       " 725: 9,\n",
       " 726: 9,\n",
       " 727: 9,\n",
       " 729: 9,\n",
       " 730: 9,\n",
       " 731: 9,\n",
       " 732: 9,\n",
       " 733: 9,\n",
       " 734: 9,\n",
       " 736: 9,\n",
       " 737: 8,\n",
       " 738: 8,\n",
       " 739: 8,\n",
       " 741: 8,\n",
       " 742: 8,\n",
       " 743: 8,\n",
       " 744: 8,\n",
       " 745: 8,\n",
       " 746: 8,\n",
       " 747: 8,\n",
       " 751: 8,\n",
       " 753: 7,\n",
       " 755: 7,\n",
       " 756: 7,\n",
       " 757: 7,\n",
       " 758: 7,\n",
       " 761: 7,\n",
       " 762: 7,\n",
       " 763: 7,\n",
       " 764: 7,\n",
       " 765: 7,\n",
       " 766: 7,\n",
       " 767: 7,\n",
       " 768: 7,\n",
       " 769: 7,\n",
       " 770: 7,\n",
       " 771: 7,\n",
       " 773: 6,\n",
       " 774: 6,\n",
       " 777: 6,\n",
       " 778: 6,\n",
       " 779: 6,\n",
       " 780: 6,\n",
       " 781: 6,\n",
       " 784: 6,\n",
       " 785: 6,\n",
       " 786: 6,\n",
       " 787: 6,\n",
       " 788: 6,\n",
       " 789: 6,\n",
       " 791: 6,\n",
       " 793: 6,\n",
       " 794: 6,\n",
       " 796: 5,\n",
       " 797: 5,\n",
       " 798: 5,\n",
       " 799: 5,\n",
       " 800: 5,\n",
       " 801: 5,\n",
       " 802: 5,\n",
       " 803: 5,\n",
       " 804: 5,\n",
       " 807: 5,\n",
       " 808: 4,\n",
       " 810: 4,\n",
       " 811: 4,\n",
       " 812: 4,\n",
       " 815: 4,\n",
       " 816: 4,\n",
       " 817: 4,\n",
       " 819: 4,\n",
       " 822: 4,\n",
       " 825: 4,\n",
       " 826: 4,\n",
       " 827: 4,\n",
       " 828: 4,\n",
       " 832: 4,\n",
       " 833: 4,\n",
       " 834: 4,\n",
       " 835: 4,\n",
       " 836: 4,\n",
       " 839: 3,\n",
       " 840: 3,\n",
       " 841: 3,\n",
       " 845: 3,\n",
       " 847: 3,\n",
       " 848: 3,\n",
       " 849: 3,\n",
       " 850: 3,\n",
       " 851: 3,\n",
       " 855: 3,\n",
       " 856: 3,\n",
       " 858: 3,\n",
       " 859: 3,\n",
       " 860: 3,\n",
       " 865: 3,\n",
       " 867: 3,\n",
       " 870: 3,\n",
       " 871: 3,\n",
       " 873: 3,\n",
       " 875: 3,\n",
       " 876: 3,\n",
       " 878: 3,\n",
       " 879: 3,\n",
       " 883: 3,\n",
       " 884: 3,\n",
       " 887: 2,\n",
       " 890: 2,\n",
       " 895: 2,\n",
       " 898: 2,\n",
       " 901: 2,\n",
       " 904: 2,\n",
       " 905: 2,\n",
       " 907: 2,\n",
       " 910: 2,\n",
       " 914: 2,\n",
       " 917: 2,\n",
       " 918: 2,\n",
       " 925: 2,\n",
       " 929: 2,\n",
       " 932: 2,\n",
       " 935: 1,\n",
       " 937: 1,\n",
       " 945: 1,\n",
       " 947: 1,\n",
       " 948: 1,\n",
       " 949: 1,\n",
       " 951: 1,\n",
       " 954: 1,\n",
       " 961: 1,\n",
       " 967: 1,\n",
       " 973: 1,\n",
       " 977: 0,\n",
       " 981: 0,\n",
       " 983: 0,\n",
       " 991: 0,\n",
       " 992: 0,\n",
       " 996: 0,\n",
       " 1002: 0,\n",
       " 1005: 0,\n",
       " 1009: 0,\n",
       " 1017: 0,\n",
       " 1875: 40,\n",
       " 1878: 39,\n",
       " 1879: 39,\n",
       " 1882: 38,\n",
       " 1883: 37,\n",
       " 1886: 36,\n",
       " 1889: 36,\n",
       " 1896: 35,\n",
       " 1898: 35,\n",
       " 1903: 34,\n",
       " 1907: 33,\n",
       " 1909: 33,\n",
       " 1911: 33,\n",
       " 1912: 33,\n",
       " 1915: 33,\n",
       " 1922: 32,\n",
       " 1925: 32,\n",
       " 1926: 32,\n",
       " 1930: 31,\n",
       " 1933: 31,\n",
       " 1936: 31,\n",
       " 1937: 31,\n",
       " 1942: 31,\n",
       " 1943: 31,\n",
       " 1944: 31,\n",
       " 1945: 31,\n",
       " 1951: 30,\n",
       " 1963: 30,\n",
       " 1964: 30,\n",
       " 1965: 30,\n",
       " 1967: 30,\n",
       " 1969: 30,\n",
       " 1973: 30,\n",
       " 1975: 30,\n",
       " 1986: 29,\n",
       " 1988: 29,\n",
       " 1990: 29,\n",
       " 1997: 29,\n",
       " 2000: 29,\n",
       " 2001: 29,\n",
       " 2007: 29,\n",
       " 2008: 29,\n",
       " 2010: 29,\n",
       " 2021: 28,\n",
       " 2023: 28,\n",
       " 2024: 28,\n",
       " 2025: 28,\n",
       " 2029: 28,\n",
       " 2031: 28,\n",
       " 2034: 28,\n",
       " 2038: 28,\n",
       " 2039: 28,\n",
       " 2040: 28,\n",
       " 2045: 28,\n",
       " 2046: 28,\n",
       " 2047: 28,\n",
       " 2049: 28,\n",
       " 2052: 28,\n",
       " 2065: 27,\n",
       " 2074: 27,\n",
       " 2075: 27,\n",
       " 2077: 27,\n",
       " 2080: 27,\n",
       " 2081: 27,\n",
       " 2082: 27,\n",
       " 2084: 27,\n",
       " 2088: 27,\n",
       " 2093: 27,\n",
       " 2099: 27,\n",
       " 2102: 27,\n",
       " 2108: 27,\n",
       " 2113: 26,\n",
       " 2117: 26,\n",
       " 2119: 26,\n",
       " 2121: 26,\n",
       " 2125: 26,\n",
       " 2128: 26,\n",
       " 2130: 26,\n",
       " 2131: 26,\n",
       " 2132: 26,\n",
       " 2133: 26,\n",
       " 2137: 26,\n",
       " 2139: 26,\n",
       " 2144: 26,\n",
       " 2152: 25,\n",
       " 2158: 25,\n",
       " 2159: 25,\n",
       " 2161: 25,\n",
       " 2162: 25,\n",
       " 2167: 25,\n",
       " 2171: 25,\n",
       " 2172: 25,\n",
       " 2175: 25,\n",
       " 2178: 25,\n",
       " 2183: 25,\n",
       " 2194: 24,\n",
       " 2200: 24,\n",
       " 2201: 24,\n",
       " 2210: 24,\n",
       " 2212: 24,\n",
       " 2222: 24,\n",
       " 2223: 23,\n",
       " 2235: 23,\n",
       " 2237: 23,\n",
       " 2238: 23,\n",
       " 2239: 23,\n",
       " 2240: 23,\n",
       " 2242: 23,\n",
       " 2245: 23,\n",
       " 2247: 23,\n",
       " 2254: 22,\n",
       " 2255: 22,\n",
       " 2258: 22,\n",
       " 2259: 22,\n",
       " 2263: 22,\n",
       " 2267: 22,\n",
       " 2268: 22,\n",
       " 2271: 21,\n",
       " 2273: 21,\n",
       " 2274: 21,\n",
       " 2276: 21,\n",
       " 2277: 21,\n",
       " 2279: 21,\n",
       " 2280: 21,\n",
       " 2281: 21,\n",
       " 2283: 21,\n",
       " 2284: 21,\n",
       " 2286: 20,\n",
       " 2288: 20,\n",
       " 2289: 20,\n",
       " 2290: 20,\n",
       " 2292: 20,\n",
       " 2293: 20,\n",
       " 2294: 20,\n",
       " 2295: 20,\n",
       " 2296: 20,\n",
       " 2297: 19,\n",
       " 2298: 19,\n",
       " 2301: 19,\n",
       " 2302: 19,\n",
       " 2303: 19,\n",
       " 2304: 19,\n",
       " 2305: 19,\n",
       " 2306: 19,\n",
       " 2307: 19,\n",
       " 2308: 19,\n",
       " 2310: 19,\n",
       " 2311: 19,\n",
       " 2312: 19,\n",
       " 2314: 18,\n",
       " 2315: 18,\n",
       " 2317: 18,\n",
       " 2318: 18,\n",
       " 2319: 18,\n",
       " 2320: 18,\n",
       " 2321: 18,\n",
       " 2323: 18,\n",
       " 2324: 18,\n",
       " 2326: 17,\n",
       " 2327: 17,\n",
       " 2328: 17,\n",
       " 2329: 17,\n",
       " 2330: 17,\n",
       " 2331: 17,\n",
       " 2332: 17,\n",
       " 2333: 17,\n",
       " 2334: 17,\n",
       " 2335: 17,\n",
       " 2336: 17,\n",
       " 2337: 17,\n",
       " 2338: 17,\n",
       " 2339: 17,\n",
       " 2340: 17,\n",
       " 2341: 17,\n",
       " 2343: 16,\n",
       " 2344: 16,\n",
       " 2345: 16,\n",
       " 2346: 16,\n",
       " 2347: 16,\n",
       " 2348: 16,\n",
       " 2349: 16,\n",
       " 2350: 16,\n",
       " 2351: 16,\n",
       " 2352: 16,\n",
       " 2354: 16,\n",
       " 2355: 16,\n",
       " 2360: 16,\n",
       " 2361: 16,\n",
       " 2363: 16,\n",
       " 2364: 16,\n",
       " 2365: 15,\n",
       " 2367: 15,\n",
       " 2368: 15,\n",
       " 2369: 15,\n",
       " 2371: 15,\n",
       " 2372: 15,\n",
       " 2373: 15,\n",
       " 2374: 15,\n",
       " 2376: 15,\n",
       " 2377: 15,\n",
       " 2378: 15,\n",
       " 2379: 15,\n",
       " 2380: 15,\n",
       " 2381: 15,\n",
       " 2383: 15,\n",
       " 2384: 15,\n",
       " 2387: 14,\n",
       " 2389: 14,\n",
       " 2390: 14,\n",
       " 2391: 14,\n",
       " 2393: 14,\n",
       " 2395: 14,\n",
       " 2396: 14,\n",
       " 2397: 14,\n",
       " 2400: 14,\n",
       " 2402: 13,\n",
       " 2403: 13,\n",
       " 2404: 13,\n",
       " 2405: 13,\n",
       " 2407: 13,\n",
       " 2408: 13,\n",
       " 2409: 13,\n",
       " 2410: 13,\n",
       " 2411: 13,\n",
       " 2412: 13,\n",
       " 2413: 13,\n",
       " 2414: 13,\n",
       " 2418: 12,\n",
       " 2419: 12,\n",
       " 2422: 12,\n",
       " 2423: 12,\n",
       " 2424: 12,\n",
       " 2425: 12,\n",
       " 2426: 12,\n",
       " 2427: 12,\n",
       " 2428: 12,\n",
       " 2430: 12,\n",
       " 2431: 12,\n",
       " 2432: 12,\n",
       " 2434: 12,\n",
       " 2436: 12,\n",
       " 2437: 12,\n",
       " 2439: 11,\n",
       " 2441: 11,\n",
       " 2442: 11,\n",
       " 2443: 11,\n",
       " 2444: 11,\n",
       " 2446: 11,\n",
       " 2447: 11,\n",
       " 2453: 10,\n",
       " 2455: 10,\n",
       " 2456: 10,\n",
       " 2458: 10,\n",
       " 2459: 10,\n",
       " 2460: 10,\n",
       " 2461: 10,\n",
       " 2463: 10,\n",
       " 2469: 9,\n",
       " 2471: 9,\n",
       " 2472: 9,\n",
       " 2473: 9,\n",
       " 2474: 9,\n",
       " 2475: 9,\n",
       " 2476: 9,\n",
       " 2477: 9,\n",
       " 2478: 9,\n",
       " 2479: 9,\n",
       " 2480: 9,\n",
       " 2481: 9,\n",
       " 2482: 9,\n",
       " 2483: 9,\n",
       " 2486: 8,\n",
       " 2489: 8,\n",
       " 2490: 8,\n",
       " 2491: 8,\n",
       " 2492: 8,\n",
       " 2495: 8,\n",
       " 2496: 8,\n",
       " 2497: 8,\n",
       " 2500: 8,\n",
       " 2501: 8,\n",
       " 2502: 8,\n",
       " 2503: 8,\n",
       " 2504: 8,\n",
       " 2505: 8,\n",
       " 2509: 7,\n",
       " 2510: 7,\n",
       " 2514: 7,\n",
       " 2515: 7,\n",
       " 2516: 7,\n",
       " 2517: 7,\n",
       " 2518: 7,\n",
       " 2519: 7,\n",
       " 2521: 7,\n",
       " 2522: 7,\n",
       " 2524: 7,\n",
       " 2526: 7,\n",
       " 2527: 7,\n",
       " 2528: 7,\n",
       " 2529: 7,\n",
       " 2530: 6,\n",
       " 2532: 6,\n",
       " 2533: 6,\n",
       " 2534: 6,\n",
       " 2535: 6,\n",
       " 2536: 6,\n",
       " 2537: 6,\n",
       " 2538: 6,\n",
       " 2540: 6,\n",
       " 2542: 6,\n",
       " 2545: 6,\n",
       " 2546: 5,\n",
       " 2547: 5,\n",
       " 2548: 5,\n",
       " 2551: 5,\n",
       " 2556: 5,\n",
       " 2557: 5,\n",
       " 2558: 5,\n",
       " 2559: 5,\n",
       " 2560: 5,\n",
       " 2561: 5,\n",
       " 2563: 5,\n",
       " 2564: 5,\n",
       " 2565: 5,\n",
       " 2566: 5,\n",
       " 2568: 4,\n",
       " 2569: 4,\n",
       " 2570: 4,\n",
       " 2571: 4,\n",
       " 2572: 4,\n",
       " 2573: 4,\n",
       " 2574: 4,\n",
       " 2575: 4,\n",
       " 2576: 4,\n",
       " 2577: 4,\n",
       " 2578: 4,\n",
       " 2579: 4,\n",
       " 2580: 4,\n",
       " 2582: 4,\n",
       " 2583: 4,\n",
       " 2584: 4,\n",
       " 2586: 4,\n",
       " 2588: 4,\n",
       " 2589: 4,\n",
       " 2590: 4,\n",
       " 2591: 4,\n",
       " 2593: 4,\n",
       " 2594: 4,\n",
       " 2595: 4,\n",
       " 2604: 3,\n",
       " 2606: 3,\n",
       " 2607: 3,\n",
       " 2609: 3,\n",
       " 2610: 3,\n",
       " 2611: 3,\n",
       " 2612: 3,\n",
       " 2613: 3,\n",
       " 2615: 3,\n",
       " 2616: 3,\n",
       " 2618: 3,\n",
       " 2620: 3,\n",
       " 2623: 3,\n",
       " 2627: 3,\n",
       " 2629: 3,\n",
       " 2630: 3,\n",
       " 2631: 3,\n",
       " 2632: 3,\n",
       " 2635: 3,\n",
       " 2638: 3,\n",
       " 2639: 3,\n",
       " 2640: 3,\n",
       " 2645: 2,\n",
       " 2650: 2,\n",
       " 2660: 2,\n",
       " 2661: 2,\n",
       " 2662: 2,\n",
       " 2663: 2,\n",
       " 2666: 2,\n",
       " 2668: 2,\n",
       " 2671: 2,\n",
       " 2672: 2,\n",
       " 2675: 2,\n",
       " 2677: 2,\n",
       " 2678: 2,\n",
       " 2686: 1,\n",
       " 2687: 1,\n",
       " 2690: 1,\n",
       " 2693: 1,\n",
       " 2696: 1,\n",
       " 2698: 1,\n",
       " 2707: 1,\n",
       " 2712: 1,\n",
       " 2713: 1,\n",
       " 2714: 1,\n",
       " 2716: 1,\n",
       " 2723: 0,\n",
       " 2735: 0,\n",
       " 2739: 0,\n",
       " 2743: 0,\n",
       " 2745: 0,\n",
       " 2747: 0,\n",
       " 2750: 0,\n",
       " 2755: 0,\n",
       " 2756: 0,\n",
       " 2759: 0,\n",
       " 3586: 37,\n",
       " 3589: 37,\n",
       " 3590: 37,\n",
       " 3595: 36,\n",
       " 3597: 35,\n",
       " 3599: 35,\n",
       " 3607: 34,\n",
       " 3608: 34,\n",
       " 3610: 34,\n",
       " 3615: 33,\n",
       " 3619: 32,\n",
       " 3622: 32,\n",
       " 3623: 32,\n",
       " 3628: 31,\n",
       " 3630: 31,\n",
       " 3631: 31,\n",
       " 3633: 31,\n",
       " 3634: 31,\n",
       " 3647: 30,\n",
       " 3649: 30,\n",
       " 3650: 30,\n",
       " 3652: 30,\n",
       " 3653: 30,\n",
       " 3654: 30,\n",
       " 3664: 29,\n",
       " 3667: 29,\n",
       " 3669: 29,\n",
       " 3672: 29,\n",
       " 3682: 29,\n",
       " 3684: 29,\n",
       " 3685: 29,\n",
       " 3686: 29,\n",
       " 3694: 28,\n",
       " 3696: 28,\n",
       " 3698: 28,\n",
       " 3699: 28,\n",
       " 3701: 28,\n",
       " 3702: 28,\n",
       " 3703: 28,\n",
       " 3705: 28,\n",
       " 3706: 28,\n",
       " 3709: 28,\n",
       " 3713: 28,\n",
       " 3714: 28,\n",
       " 3719: 28,\n",
       " 3720: 28,\n",
       " 3722: 28,\n",
       " 3723: 28,\n",
       " 3725: 28,\n",
       " 3730: 27,\n",
       " 3734: 27,\n",
       " 3737: 27,\n",
       " 3738: 27,\n",
       " 3739: 27,\n",
       " 3742: 27,\n",
       " 3745: 27,\n",
       " 3747: 27,\n",
       " 3764: 26,\n",
       " 3767: 26,\n",
       " 3770: 26,\n",
       " 3772: 26,\n",
       " 3778: 26,\n",
       " 3783: 26,\n",
       " 3785: 26,\n",
       " 3789: 26,\n",
       " 3790: 26,\n",
       " 3792: 25,\n",
       " 3797: 25,\n",
       " 3809: 25,\n",
       " 3810: 25,\n",
       " 3814: 25,\n",
       " 3818: 25,\n",
       " 3827: 25,\n",
       " 3840: 24,\n",
       " 3846: 24,\n",
       " 3848: 24,\n",
       " 3857: 23,\n",
       " 3859: 23,\n",
       " 3866: 23,\n",
       " 3867: 23,\n",
       " 3868: 23,\n",
       " 3872: 23,\n",
       " 3876: 23,\n",
       " 3881: 23,\n",
       " 3883: 23,\n",
       " 3888: 23,\n",
       " 3891: 23,\n",
       " 3894: 22,\n",
       " 3896: 22,\n",
       " 3904: 22,\n",
       " 3909: 22,\n",
       " 3910: 22,\n",
       " 3912: 22,\n",
       " 3913: 22,\n",
       " 3914: 21,\n",
       " 3916: 21,\n",
       " 3917: 21,\n",
       " 3921: 21,\n",
       " 3922: 21,\n",
       " 3923: 21,\n",
       " 3924: 21,\n",
       " 3925: 21,\n",
       " 3929: 20,\n",
       " 3930: 20,\n",
       " 3931: 20,\n",
       " 3932: 20,\n",
       " 3933: 20,\n",
       " 3934: 20,\n",
       " 3935: 20,\n",
       " 3937: 20,\n",
       " 3938: 20,\n",
       " 3939: 20,\n",
       " 3940: 20,\n",
       " 3942: 19,\n",
       " 3943: 19,\n",
       " 3944: 19,\n",
       " 3945: 19,\n",
       " 3946: 19,\n",
       " 3947: 19,\n",
       " 3948: 19,\n",
       " 3949: 19,\n",
       " 3950: 19,\n",
       " 3951: 19,\n",
       " 3952: 19,\n",
       " 3953: 19,\n",
       " 3954: 19,\n",
       " 3957: 19,\n",
       " 3958: 19,\n",
       " 3959: 19,\n",
       " 3961: 18,\n",
       " 3962: 18,\n",
       " 3963: 18,\n",
       " 3964: 18,\n",
       " 3965: 18,\n",
       " 3966: 18,\n",
       " 3967: 18,\n",
       " 3968: 18,\n",
       " 3969: 18,\n",
       " 3970: 18,\n",
       " 3971: 18,\n",
       " 3972: 18,\n",
       " 3973: 18,\n",
       " 3974: 18,\n",
       " 3975: 18,\n",
       " 3977: 18,\n",
       " 3978: 18,\n",
       " 3979: 17,\n",
       " 3982: 17,\n",
       " ...}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get videos in the training data\n",
    "video_info_filtered = video_info[video_info['video_id'].isin(train['video_id'].unique())]\n",
    "\n",
    "# Calculate video age\n",
    "# video_info_filtered['Current Date'] = CURRENT_DATE\n",
    "video_info_filtered['video_age'] = (pd.to_datetime(CURRENT_DATE) - video_info_filtered['upload_dt']).dt.days\n",
    "\n",
    "# Convert into dictionary\n",
    "video_age_dict = video_info_filtered.set_index('video_id')['video_age'].to_dict()\n",
    "video_age_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate age of the interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/24/w8t485397_n5ygqwgnjns3_80000gn/T/ipykernel_14971/1905797027.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  interaction['interaction_age'] = (pd.to_datetime(CURRENT_DATE) - interaction['time']).dt.days\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(14, 103): 25,\n",
       " (14, 109): 24,\n",
       " (14, 120): 24,\n",
       " (14, 122): 24,\n",
       " (14, 128): 24,\n",
       " (14, 130): 7,\n",
       " (14, 131): 12,\n",
       " (14, 133): 13,\n",
       " (14, 136): 20,\n",
       " (14, 137): 12,\n",
       " (14, 139): 27,\n",
       " (14, 142): 12,\n",
       " (14, 145): 26,\n",
       " (14, 146): 18,\n",
       " (14, 147): 26,\n",
       " (14, 148): 28,\n",
       " (14, 151): 3,\n",
       " (14, 153): 12,\n",
       " (14, 154): 13,\n",
       " (14, 164): 21,\n",
       " (14, 166): 15,\n",
       " (14, 168): 19,\n",
       " (14, 169): 23,\n",
       " (14, 170): 21,\n",
       " (14, 171): 28,\n",
       " (14, 173): 12,\n",
       " (14, 175): 28,\n",
       " (14, 179): 28,\n",
       " (14, 180): 25,\n",
       " (14, 183): 28,\n",
       " (14, 186): 28,\n",
       " (14, 188): 21,\n",
       " (14, 203): 27,\n",
       " (14, 206): 27,\n",
       " (14, 207): 19,\n",
       " (14, 210): 23,\n",
       " (14, 211): 27,\n",
       " (14, 217): 27,\n",
       " (14, 223): 26,\n",
       " (14, 229): 26,\n",
       " (14, 237): 16,\n",
       " (14, 238): 17,\n",
       " (14, 250): 5,\n",
       " (14, 251): 13,\n",
       " (14, 254): 26,\n",
       " (14, 255): 13,\n",
       " (14, 256): 26,\n",
       " (14, 258): 26,\n",
       " (14, 262): 26,\n",
       " (14, 265): 26,\n",
       " (14, 267): 17,\n",
       " (14, 270): 15,\n",
       " (14, 272): 23,\n",
       " (14, 275): 26,\n",
       " (14, 279): 26,\n",
       " (14, 280): 26,\n",
       " (14, 282): 24,\n",
       " (14, 285): 26,\n",
       " (14, 286): 26,\n",
       " (14, 288): 25,\n",
       " (14, 289): 26,\n",
       " (14, 290): 26,\n",
       " (14, 296): 26,\n",
       " (14, 297): 26,\n",
       " (14, 300): 23,\n",
       " (14, 302): 13,\n",
       " (14, 304): 23,\n",
       " (14, 306): 20,\n",
       " (14, 307): 25,\n",
       " (14, 314): 13,\n",
       " (14, 318): 24,\n",
       " (14, 319): 21,\n",
       " (14, 324): 23,\n",
       " (14, 331): 4,\n",
       " (14, 340): 24,\n",
       " (14, 349): 23,\n",
       " (14, 350): 24,\n",
       " (14, 351): 24,\n",
       " (14, 361): 23,\n",
       " (14, 364): 15,\n",
       " (14, 368): 16,\n",
       " (14, 369): 23,\n",
       " (14, 373): 23,\n",
       " (14, 378): 16,\n",
       " (14, 388): 1,\n",
       " (14, 389): 16,\n",
       " (14, 390): 23,\n",
       " (14, 392): 20,\n",
       " (14, 394): 22,\n",
       " (14, 395): 19,\n",
       " (14, 400): 23,\n",
       " (14, 402): 23,\n",
       " (14, 403): 22,\n",
       " (14, 408): 23,\n",
       " (14, 413): 16,\n",
       " (14, 414): 14,\n",
       " (14, 416): 21,\n",
       " (14, 423): 22,\n",
       " (14, 425): 22,\n",
       " (14, 429): 22,\n",
       " (14, 430): 16,\n",
       " (14, 432): 16,\n",
       " (14, 433): 21,\n",
       " (14, 435): 19,\n",
       " (14, 437): 22,\n",
       " (14, 445): 15,\n",
       " (14, 446): 8,\n",
       " (14, 456): 20,\n",
       " (14, 457): 21,\n",
       " (14, 460): 21,\n",
       " (14, 462): 17,\n",
       " (14, 463): 19,\n",
       " (14, 465): 22,\n",
       " (14, 466): 6,\n",
       " (14, 468): 22,\n",
       " (14, 471): 21,\n",
       " (14, 472): 15,\n",
       " (14, 478): 20,\n",
       " (14, 479): 18,\n",
       " (14, 481): 20,\n",
       " (14, 491): 2,\n",
       " (14, 493): 16,\n",
       " (14, 494): 13,\n",
       " (14, 497): 20,\n",
       " (14, 501): 5,\n",
       " (14, 503): 8,\n",
       " (14, 507): 1,\n",
       " (14, 508): 6,\n",
       " (14, 509): 19,\n",
       " (14, 510): 15,\n",
       " (14, 513): 15,\n",
       " (14, 514): 6,\n",
       " (14, 516): 2,\n",
       " (14, 517): 19,\n",
       " (14, 518): 17,\n",
       " (14, 521): 13,\n",
       " (14, 522): 16,\n",
       " (14, 523): 19,\n",
       " (14, 525): 17,\n",
       " (14, 530): 19,\n",
       " (14, 531): 17,\n",
       " (14, 532): 19,\n",
       " (14, 533): 19,\n",
       " (14, 535): 16,\n",
       " (14, 536): 16,\n",
       " (14, 537): 15,\n",
       " (14, 538): 19,\n",
       " (14, 540): 17,\n",
       " (14, 542): 15,\n",
       " (14, 543): 17,\n",
       " (14, 544): 12,\n",
       " (14, 545): 3,\n",
       " (14, 548): 17,\n",
       " (14, 552): 16,\n",
       " (14, 554): 14,\n",
       " (14, 555): 17,\n",
       " (14, 557): 16,\n",
       " (14, 558): 16,\n",
       " (14, 560): 16,\n",
       " (14, 561): 14,\n",
       " (14, 562): 17,\n",
       " (14, 563): 12,\n",
       " (14, 564): 13,\n",
       " (14, 565): 17,\n",
       " (14, 567): 4,\n",
       " (14, 568): 7,\n",
       " (14, 569): 16,\n",
       " (14, 570): 14,\n",
       " (14, 571): 2,\n",
       " (14, 573): 17,\n",
       " (14, 575): 13,\n",
       " (14, 576): 4,\n",
       " (14, 577): 15,\n",
       " (14, 578): 15,\n",
       " (14, 579): 16,\n",
       " (14, 580): 6,\n",
       " (14, 581): 2,\n",
       " (14, 583): 15,\n",
       " (14, 584): 3,\n",
       " (14, 585): 1,\n",
       " (14, 586): 14,\n",
       " (14, 587): 15,\n",
       " (14, 589): 13,\n",
       " (14, 590): 15,\n",
       " (14, 591): 7,\n",
       " (14, 593): 4,\n",
       " (14, 594): 15,\n",
       " (14, 597): 15,\n",
       " (14, 600): 15,\n",
       " (14, 601): 13,\n",
       " (14, 602): 15,\n",
       " (14, 603): 15,\n",
       " (14, 606): 7,\n",
       " (14, 607): 7,\n",
       " (14, 608): 15,\n",
       " (14, 610): 7,\n",
       " (14, 611): 14,\n",
       " (14, 612): 12,\n",
       " (14, 613): 14,\n",
       " (14, 614): 15,\n",
       " (14, 615): 14,\n",
       " (14, 616): 4,\n",
       " (14, 617): 14,\n",
       " (14, 618): 8,\n",
       " (14, 619): 13,\n",
       " (14, 621): 11,\n",
       " (14, 622): 12,\n",
       " (14, 624): 13,\n",
       " (14, 625): 13,\n",
       " (14, 626): 11,\n",
       " (14, 627): 13,\n",
       " (14, 629): 12,\n",
       " (14, 630): 3,\n",
       " (14, 631): 13,\n",
       " (14, 632): 7,\n",
       " (14, 634): 13,\n",
       " (14, 635): 9,\n",
       " (14, 636): 13,\n",
       " (14, 637): 12,\n",
       " (14, 638): 13,\n",
       " (14, 639): 8,\n",
       " (14, 640): 7,\n",
       " (14, 641): 12,\n",
       " (14, 643): 6,\n",
       " (14, 644): 6,\n",
       " (14, 645): 11,\n",
       " (14, 647): 12,\n",
       " (14, 649): 6,\n",
       " (14, 650): 2,\n",
       " (14, 651): 1,\n",
       " (14, 655): 12,\n",
       " (14, 657): 12,\n",
       " (14, 658): 5,\n",
       " (14, 659): 4,\n",
       " (14, 660): 7,\n",
       " (14, 661): -1,\n",
       " (14, 662): 4,\n",
       " (14, 663): 10,\n",
       " (14, 664): 12,\n",
       " (14, 665): 7,\n",
       " (14, 666): 6,\n",
       " (14, 669): 10,\n",
       " (14, 670): 4,\n",
       " (14, 672): 12,\n",
       " (14, 673): 12,\n",
       " (14, 674): 4,\n",
       " (14, 675): 9,\n",
       " (14, 677): 7,\n",
       " (14, 679): 2,\n",
       " (14, 681): 4,\n",
       " (14, 684): 8,\n",
       " (14, 685): 6,\n",
       " (14, 686): 9,\n",
       " (14, 689): 5,\n",
       " (14, 691): 0,\n",
       " (14, 692): 8,\n",
       " (14, 693): 5,\n",
       " (14, 696): 5,\n",
       " (14, 698): 7,\n",
       " (14, 701): 7,\n",
       " (14, 702): 6,\n",
       " (14, 705): 6,\n",
       " (14, 706): 9,\n",
       " (14, 709): 3,\n",
       " (14, 711): 4,\n",
       " (14, 712): 3,\n",
       " (14, 714): 5,\n",
       " (14, 715): 4,\n",
       " (14, 716): 8,\n",
       " (14, 721): 6,\n",
       " (14, 722): 6,\n",
       " (14, 723): 4,\n",
       " (14, 725): 4,\n",
       " (14, 726): 5,\n",
       " (14, 727): 0,\n",
       " (14, 729): 6,\n",
       " (14, 730): 7,\n",
       " (14, 731): 8,\n",
       " (14, 732): 3,\n",
       " (14, 733): 3,\n",
       " (14, 734): 2,\n",
       " (14, 736): 7,\n",
       " (14, 737): 3,\n",
       " (14, 738): 6,\n",
       " (14, 739): 0,\n",
       " (14, 741): 2,\n",
       " (14, 742): 6,\n",
       " (14, 743): 1,\n",
       " (14, 744): 1,\n",
       " (14, 745): 4,\n",
       " (14, 746): 4,\n",
       " (14, 747): 6,\n",
       " (14, 753): 5,\n",
       " (14, 756): 3,\n",
       " (14, 757): 4,\n",
       " (14, 758): 4,\n",
       " (14, 761): 2,\n",
       " (14, 762): 3,\n",
       " (14, 763): 5,\n",
       " (14, 764): 3,\n",
       " (14, 765): 3,\n",
       " (14, 766): 3,\n",
       " (14, 768): 4,\n",
       " (14, 769): 3,\n",
       " (14, 773): 4,\n",
       " (14, 774): 1,\n",
       " (14, 777): 3,\n",
       " (14, 778): 3,\n",
       " (14, 779): 5,\n",
       " (14, 781): 3,\n",
       " (14, 785): 5,\n",
       " (14, 787): 3,\n",
       " (14, 788): 2,\n",
       " (14, 789): -1,\n",
       " (14, 791): 3,\n",
       " (14, 793): 4,\n",
       " (14, 794): 3,\n",
       " (14, 796): 0,\n",
       " (14, 797): 4,\n",
       " (14, 798): -1,\n",
       " (14, 800): 2,\n",
       " (14, 803): 2,\n",
       " (14, 808): 1,\n",
       " (14, 810): 3,\n",
       " (14, 811): 3,\n",
       " (14, 817): -1,\n",
       " (14, 827): 2,\n",
       " (14, 828): 2,\n",
       " (14, 832): 2,\n",
       " (14, 833): 2,\n",
       " (14, 836): 2,\n",
       " (14, 848): 0,\n",
       " (14, 851): 1,\n",
       " (14, 856): 1,\n",
       " (14, 865): 1,\n",
       " (14, 873): 2,\n",
       " (14, 878): 0,\n",
       " (14, 901): -1,\n",
       " (14, 914): 0,\n",
       " (14, 917): -1,\n",
       " (14, 945): -1,\n",
       " (14, 1005): -1,\n",
       " (14, 1875): 13,\n",
       " (14, 1878): 24,\n",
       " (14, 1879): 17,\n",
       " (14, 1882): 17,\n",
       " (14, 1883): 19,\n",
       " (14, 1886): 7,\n",
       " (14, 1889): 24,\n",
       " (14, 1896): 24,\n",
       " (14, 1898): 27,\n",
       " (14, 1903): 26,\n",
       " (14, 1907): 13,\n",
       " (14, 1909): 13,\n",
       " (14, 1911): 12,\n",
       " (14, 1912): 12,\n",
       " (14, 1915): 12,\n",
       " (14, 1922): 24,\n",
       " (14, 1925): 19,\n",
       " (14, 1926): 13,\n",
       " (14, 1930): 12,\n",
       " (14, 1936): 6,\n",
       " (14, 1937): 6,\n",
       " (14, 1942): 13,\n",
       " (14, 1943): 26,\n",
       " (14, 1944): 7,\n",
       " (14, 1945): 6,\n",
       " (14, 1951): 28,\n",
       " (14, 1963): 28,\n",
       " (14, 1964): 8,\n",
       " (14, 1965): 8,\n",
       " (14, 1967): 13,\n",
       " (14, 1969): 24,\n",
       " (14, 1973): 28,\n",
       " (14, 1975): 19,\n",
       " (14, 1986): 26,\n",
       " (14, 1988): 27,\n",
       " (14, 1990): 0,\n",
       " (14, 1997): 8,\n",
       " (14, 2000): 27,\n",
       " (14, 2001): 21,\n",
       " (14, 2007): 27,\n",
       " (14, 2008): 27,\n",
       " (14, 2010): 17,\n",
       " (14, 2021): 2,\n",
       " (14, 2023): 22,\n",
       " (14, 2024): 26,\n",
       " (14, 2025): 15,\n",
       " (14, 2029): 26,\n",
       " (14, 2034): 13,\n",
       " (14, 2038): 15,\n",
       " (14, 2039): 8,\n",
       " (14, 2040): 26,\n",
       " (14, 2045): 13,\n",
       " (14, 2046): 2,\n",
       " (14, 2047): 15,\n",
       " (14, 2049): 12,\n",
       " (14, 2052): 26,\n",
       " (14, 2074): 26,\n",
       " (14, 2075): 26,\n",
       " (14, 2077): 26,\n",
       " (14, 2080): 13,\n",
       " (14, 2081): 26,\n",
       " (14, 2082): 26,\n",
       " (14, 2084): 26,\n",
       " (14, 2088): 23,\n",
       " (14, 2093): 26,\n",
       " (14, 2099): 13,\n",
       " (14, 2108): 23,\n",
       " (14, 2113): 25,\n",
       " (14, 2119): 5,\n",
       " (14, 2121): 24,\n",
       " (14, 2125): 24,\n",
       " (14, 2128): 15,\n",
       " (14, 2130): 7,\n",
       " (14, 2131): 24,\n",
       " (14, 2132): 14,\n",
       " (14, 2133): 15,\n",
       " (14, 2137): 24,\n",
       " (14, 2139): 24,\n",
       " (14, 2144): 20,\n",
       " (14, 2158): 16,\n",
       " (14, 2159): 17,\n",
       " (14, 2161): 19,\n",
       " (14, 2162): 16,\n",
       " (14, 2167): 5,\n",
       " (14, 2171): 21,\n",
       " (14, 2172): 5,\n",
       " (14, 2175): 7,\n",
       " (14, 2178): 15,\n",
       " (14, 2183): 20,\n",
       " (14, 2194): 23,\n",
       " (14, 2200): 0,\n",
       " (14, 2201): 22,\n",
       " (14, 2210): 8,\n",
       " (14, 2212): 22,\n",
       " (14, 2222): 0,\n",
       " (14, 2223): 22,\n",
       " (14, 2235): 17,\n",
       " (14, 2238): 7,\n",
       " (14, 2239): 6,\n",
       " (14, 2240): 12,\n",
       " (14, 2242): 2,\n",
       " (14, 2245): 16,\n",
       " (14, 2247): 21,\n",
       " (14, 2254): 18,\n",
       " (14, 2255): 20,\n",
       " (14, 2258): 20,\n",
       " (14, 2259): 16,\n",
       " (14, 2263): 20,\n",
       " (14, 2267): 20,\n",
       " (14, 2268): 13,\n",
       " (14, 2271): 10,\n",
       " (14, 2273): 20,\n",
       " (14, 2274): 17,\n",
       " (14, 2276): 20,\n",
       " (14, 2277): 17,\n",
       " (14, 2279): 17,\n",
       " (14, 2280): 13,\n",
       " (14, 2281): 1,\n",
       " (14, 2283): 4,\n",
       " (14, 2286): 16,\n",
       " (14, 2289): 13,\n",
       " (14, 2290): 19,\n",
       " (14, 2292): 17,\n",
       " (14, 2293): 14,\n",
       " (14, 2294): 18,\n",
       " (14, 2295): 15,\n",
       " (14, 2296): 14,\n",
       " (14, 2297): 7,\n",
       " (14, 2298): 14,\n",
       " (14, 2301): 4,\n",
       " (14, 2302): 17,\n",
       " (14, 2303): 16,\n",
       " (14, 2304): 16,\n",
       " (14, 2305): 17,\n",
       " (14, 2306): 17,\n",
       " (14, 2307): 16,\n",
       " (14, 2308): 8,\n",
       " (14, 2310): 17,\n",
       " (14, 2311): 12,\n",
       " (14, 2312): 15,\n",
       " (14, 2314): 17,\n",
       " (14, 2315): 16,\n",
       " (14, 2317): 16,\n",
       " (14, 2318): 1,\n",
       " (14, 2319): 15,\n",
       " (14, 2320): 13,\n",
       " (14, 2323): 15,\n",
       " (14, 2324): 3,\n",
       " (14, 2326): 14,\n",
       " (14, 2327): 15,\n",
       " (14, 2328): 7,\n",
       " (14, 2329): 6,\n",
       " (14, 2330): 15,\n",
       " (14, 2331): 15,\n",
       " (14, 2332): 15,\n",
       " (14, 2333): 15,\n",
       " (14, 2334): 4,\n",
       " (14, 2335): 7,\n",
       " (14, 2337): 15,\n",
       " (14, 2338): 13,\n",
       " (14, 2339): 12,\n",
       " (14, 2340): 8,\n",
       " (14, 2341): 1,\n",
       " (14, 2343): 7,\n",
       " (14, 2344): 4,\n",
       " (14, 2345): 15,\n",
       " (14, 2346): 14,\n",
       " (14, 2347): 10,\n",
       " (14, 2348): 15,\n",
       " (14, 2349): 4,\n",
       " (14, 2350): 15,\n",
       " (14, 2351): 12,\n",
       " (14, 2352): 1,\n",
       " (14, 2354): 15,\n",
       " (14, 2355): 2,\n",
       " (14, 2360): 4,\n",
       " (14, 2361): 14,\n",
       " (14, 2363): 14,\n",
       " (14, 2364): 13,\n",
       " (14, 2367): 7,\n",
       " (14, 2368): 4,\n",
       " (14, 2369): 13,\n",
       " (14, 2371): 13,\n",
       " (14, 2372): 13,\n",
       " (14, 2374): 2,\n",
       " (14, 2376): 6,\n",
       " (14, 2377): 12,\n",
       " (14, 2378): 5,\n",
       " (14, 2380): 13,\n",
       " (14, 2381): 13,\n",
       " (14, 2383): 12,\n",
       " (14, 2384): 10,\n",
       " (14, 2389): 6,\n",
       " (14, 2390): 12,\n",
       " (14, 2391): 3,\n",
       " (14, 2393): 12,\n",
       " (14, 2395): 13,\n",
       " (14, 2396): 5,\n",
       " (14, 2397): 6,\n",
       " (14, 2400): 7,\n",
       " (14, 2402): 10,\n",
       " (14, 2403): 3,\n",
       " (14, 2404): 4,\n",
       " (14, 2405): 12,\n",
       " (14, 2409): 4,\n",
       " (14, 2410): 8,\n",
       " (14, 2411): 7,\n",
       " (14, 2412): 5,\n",
       " (14, 2413): 4,\n",
       " (14, 2414): 4,\n",
       " (14, 2418): 9,\n",
       " (14, 2419): 7,\n",
       " (14, 2424): 3,\n",
       " (14, 2425): 10,\n",
       " (14, 2427): 0,\n",
       " (14, 2428): 2,\n",
       " (14, 2430): 9,\n",
       " (14, 2431): 5,\n",
       " (14, 2432): 6,\n",
       " (14, 2436): 4,\n",
       " (14, 2437): 7,\n",
       " (14, 2439): 8,\n",
       " (14, 2441): 6,\n",
       " (14, 2444): 8,\n",
       " (14, 2446): 4,\n",
       " (14, 2447): 4,\n",
       " (14, 2453): 8,\n",
       " (14, 2455): 6,\n",
       " (14, 2458): 4,\n",
       " (14, 2459): 1,\n",
       " (14, 2461): 4,\n",
       " (14, 2463): 6,\n",
       " (14, 2469): 7,\n",
       " (14, 2471): 6,\n",
       " (14, 2473): 5,\n",
       " (14, 2475): 8,\n",
       " (14, 2476): 0,\n",
       " (14, 2477): 5,\n",
       " (14, 2478): 6,\n",
       " (14, 2479): 4,\n",
       " (14, 2480): 2,\n",
       " (14, 2482): 6,\n",
       " (14, 2483): 7,\n",
       " (14, 2486): 3,\n",
       " (14, 2489): 6,\n",
       " (14, 2490): 6,\n",
       " (14, 2491): 5,\n",
       " (14, 2492): 6,\n",
       " (14, 2495): 4,\n",
       " (14, 2497): 5,\n",
       " (14, 2500): 6,\n",
       " (14, 2501): 6,\n",
       " (14, 2502): 6,\n",
       " (14, 2503): 5,\n",
       " (14, 2504): 4,\n",
       " (14, 2505): 0,\n",
       " (14, 2509): 4,\n",
       " (14, 2515): 4,\n",
       " (14, 2516): 5,\n",
       " (14, 2517): 5,\n",
       " (14, 2519): 3,\n",
       " (14, 2521): 4,\n",
       " (14, 2526): 5,\n",
       " (14, 2527): 4,\n",
       " (14, 2528): 5,\n",
       " (14, 2530): 5,\n",
       " (14, 2533): 3,\n",
       " (14, 2534): 2,\n",
       " (14, 2535): 5,\n",
       " (14, 2536): 2,\n",
       " (14, 2540): 4,\n",
       " (14, 2542): 4,\n",
       " (14, 2545): 1,\n",
       " (14, 2547): 2,\n",
       " (14, 2551): 3,\n",
       " (14, 2556): 3,\n",
       " (14, 2559): 3,\n",
       " (14, 2561): 3,\n",
       " (14, 2563): 3,\n",
       " (14, 2569): 2,\n",
       " (14, 2571): 3,\n",
       " (14, 2572): 2,\n",
       " (14, 2573): 3,\n",
       " (14, 2576): 3,\n",
       " (14, 2577): 1,\n",
       " (14, 2579): 2,\n",
       " (14, 2586): 2,\n",
       " (14, 2590): 1,\n",
       " (14, 2594): 1,\n",
       " (14, 2595): 2,\n",
       " (14, 2606): 2,\n",
       " (14, 2611): 2,\n",
       " (14, 2613): 2,\n",
       " (14, 2623): 2,\n",
       " (14, 2645): 1,\n",
       " (14, 2675): 0,\n",
       " (14, 2698): -1,\n",
       " (14, 3586): 26,\n",
       " (14, 3589): 24,\n",
       " (14, 3590): 23,\n",
       " (14, 3595): 24,\n",
       " (14, 3597): 24,\n",
       " (14, 3599): 23,\n",
       " (14, 3607): 21,\n",
       " (14, 3608): 7,\n",
       " (14, 3610): 17,\n",
       " (14, 3615): 13,\n",
       " (14, 3619): 7,\n",
       " (14, 3622): 3,\n",
       " (14, 3623): 7,\n",
       " (14, 3628): 23,\n",
       " (14, 3630): 26,\n",
       " (14, 3631): 4,\n",
       " (14, 3634): 28,\n",
       " (14, 3647): 28,\n",
       " (14, 3649): 28,\n",
       " (14, 3650): 26,\n",
       " (14, 3652): 13,\n",
       " (14, 3653): 13,\n",
       " (14, 3654): 27,\n",
       " (14, 3664): 22,\n",
       " (14, 3667): 19,\n",
       " (14, 3669): 27,\n",
       " (14, 3672): 27,\n",
       " (14, 3682): 13,\n",
       " (14, 3684): 27,\n",
       " (14, 3685): 13,\n",
       " (14, 3686): 13,\n",
       " (14, 3694): 26,\n",
       " (14, 3696): 16,\n",
       " (14, 3698): 26,\n",
       " (14, 3699): 26,\n",
       " (14, 3701): 3,\n",
       " (14, 3702): 26,\n",
       " (14, 3703): 15,\n",
       " (14, 3705): 12,\n",
       " (14, 3706): 26,\n",
       " (14, 3709): 17,\n",
       " (14, 3713): 19,\n",
       " (14, 3714): 24,\n",
       " (14, 3719): 26,\n",
       " (14, 3720): 15,\n",
       " (14, 3722): 26,\n",
       " (14, 3723): 21,\n",
       " (14, 3730): 24,\n",
       " (14, 3734): 25,\n",
       " (14, 3737): 26,\n",
       " (14, 3738): 19,\n",
       " (14, 3739): 13,\n",
       " (14, 3742): 17,\n",
       " (14, 3745): 21,\n",
       " (14, 3747): 25,\n",
       " (14, 3764): 24,\n",
       " (14, 3767): 23,\n",
       " (14, 3770): 24,\n",
       " (14, 3772): 24,\n",
       " (14, 3778): 24,\n",
       " (14, 3783): 20,\n",
       " (14, 3785): 23,\n",
       " (14, 3789): 14,\n",
       " (14, 3790): 15,\n",
       " (14, 3792): 20,\n",
       " (14, 3797): 23,\n",
       " (14, 3809): 21,\n",
       " (14, 3810): 5,\n",
       " (14, 3814): 0,\n",
       " (14, 3818): 23,\n",
       " (14, 3827): 1,\n",
       " (14, 3840): 15,\n",
       " (14, 3846): 21,\n",
       " (14, 3848): 22,\n",
       " (14, 3859): 21,\n",
       " (14, 3866): 22,\n",
       " (14, 3867): 16,\n",
       " (14, 3868): 21,\n",
       " (14, 3872): 21,\n",
       " (14, 3876): 16,\n",
       " (14, 3881): 17,\n",
       " (14, 3883): 15,\n",
       " (14, 3888): 17,\n",
       " (14, 3891): 19,\n",
       " (14, 3894): 20,\n",
       " (14, 3896): 13,\n",
       " (14, 3904): 15,\n",
       " (14, 3909): 19,\n",
       " (14, 3910): 20,\n",
       " (14, 3912): 20,\n",
       " (14, 3913): 20,\n",
       " (14, 3914): 15,\n",
       " (14, 3916): 4,\n",
       " (14, 3917): 1,\n",
       " (14, 3921): 16,\n",
       " (14, 3922): 15,\n",
       " (14, 3923): 19,\n",
       " (14, 3924): 15,\n",
       " (14, 3925): 12,\n",
       " (14, 3929): 5,\n",
       " (14, 3930): 16,\n",
       " (14, 3931): 19,\n",
       " (14, 3932): 16,\n",
       " (14, 3933): 18,\n",
       " (14, 3934): 7,\n",
       " (14, 3935): 4,\n",
       " (14, 3937): 17,\n",
       " (14, 3938): 15,\n",
       " (14, 3939): 16,\n",
       " (14, 3940): 3,\n",
       " (14, 3942): 16,\n",
       " (14, 3943): 15,\n",
       " (14, 3944): 6,\n",
       " (14, 3945): 17,\n",
       " (14, 3946): 14,\n",
       " (14, 3947): 16,\n",
       " (14, 3948): 17,\n",
       " (14, 3949): 17,\n",
       " (14, 3950): 17,\n",
       " (14, 3951): 17,\n",
       " (14, 3952): 16,\n",
       " (14, 3953): 16,\n",
       " (14, 3954): 15,\n",
       " (14, 3957): 4,\n",
       " (14, 3958): 16,\n",
       " (14, 3959): 16,\n",
       " (14, 3961): 13,\n",
       " (14, 3962): 14,\n",
       " (14, 3963): 17,\n",
       " (14, 3964): 13,\n",
       " (14, 3965): 17,\n",
       " (14, 3966): 4,\n",
       " (14, 3967): 16,\n",
       " (14, 3968): 15,\n",
       " (14, 3969): 15,\n",
       " (14, 3971): 14,\n",
       " (14, 3972): 15,\n",
       " (14, 3973): 15,\n",
       " (14, 3974): 17,\n",
       " (14, 3975): 15,\n",
       " (14, 3977): 15,\n",
       " (14, 3979): 1,\n",
       " (14, 3982): 8,\n",
       " (14, 3984): 15,\n",
       " (14, 3986): 14,\n",
       " (14, 3987): 15,\n",
       " (14, 3989): 15,\n",
       " (14, 3990): 15,\n",
       " (14, 3994): 14,\n",
       " (14, 3995): 14,\n",
       " (14, 3996): 15,\n",
       " (14, 3998): 14,\n",
       " (14, 3999): 7,\n",
       " (14, 4000): 14,\n",
       " (14, 4001): 12,\n",
       " (14, 4004): 12,\n",
       " (14, 4005): 7,\n",
       " (14, 4006): 15,\n",
       " (14, 4008): 13,\n",
       " (14, 4009): 6,\n",
       " (14, 4010): 12,\n",
       " (14, 4012): 3,\n",
       " (14, 4013): 4,\n",
       " (14, 4014): 13,\n",
       " (14, 4016): 13,\n",
       " (14, 4017): 13,\n",
       " (14, 4019): 4,\n",
       " (14, 4020): 1,\n",
       " (14, 4022): 7,\n",
       " (14, 4023): 3,\n",
       " (14, 4024): 7,\n",
       " (14, 4025): 12,\n",
       " (14, 4026): 7,\n",
       " (14, 4027): 6,\n",
       " (14, 4028): 13,\n",
       " (14, 4029): 4,\n",
       " (14, 4031): 12,\n",
       " (14, 4032): 8,\n",
       " (14, 4033): 11,\n",
       " (14, 4034): 4,\n",
       " (14, 4035): 8,\n",
       " (14, 4036): 8,\n",
       " (14, 4037): 7,\n",
       " (14, 4038): 6,\n",
       " (14, 4039): 11,\n",
       " (14, 4040): 8,\n",
       " (14, 4041): 9,\n",
       " (14, 4043): 6,\n",
       " (14, 4044): 10,\n",
       " (14, 4045): 1,\n",
       " (14, 4046): 6,\n",
       " (14, 4048): 6,\n",
       " (14, 4049): 6,\n",
       " (14, 4050): 11,\n",
       " (14, 4051): 2,\n",
       " (14, 4052): 4,\n",
       " (14, 4053): 5,\n",
       " (14, 4054): 5,\n",
       " (14, 4055): 4,\n",
       " (14, 4059): 2,\n",
       " (14, 4060): 7,\n",
       " (14, 4061): 3,\n",
       " (14, 4062): 7,\n",
       " (14, 4063): 8,\n",
       " (14, 4065): 5,\n",
       " (14, 4066): 7,\n",
       " (14, 4068): 7,\n",
       " (14, 4074): 4,\n",
       " (14, 4075): 4,\n",
       " (14, 4076): 8,\n",
       " (14, 4078): 4,\n",
       " (14, 4080): 5,\n",
       " (14, 4081): 5,\n",
       " (14, 4082): 3,\n",
       " (14, 4083): 9,\n",
       " (14, 4084): 4,\n",
       " (14, 4086): 8,\n",
       " (14, 4088): 2,\n",
       " (14, 4090): 6,\n",
       " (14, 4092): 4,\n",
       " (14, 4093): 4,\n",
       " (14, 4094): 5,\n",
       " (14, 4099): 8,\n",
       " (14, 4100): 3,\n",
       " (14, 4101): 8,\n",
       " (14, 4103): 4,\n",
       " (14, 4105): 3,\n",
       " (14, 4106): 4,\n",
       " (14, 4107): 4,\n",
       " (14, 4114): 4,\n",
       " (14, 4118): 4,\n",
       " (14, 4119): 3,\n",
       " (14, 4120): 6,\n",
       " (14, 4121): 4,\n",
       " (14, 4123): 6,\n",
       " (14, 4124): 4,\n",
       " (14, 4125): 3,\n",
       " (14, 4127): 6,\n",
       " (14, 4129): 4,\n",
       " (14, 4131): 5,\n",
       " (14, 4133): 5,\n",
       " (14, 4136): 5,\n",
       " (14, 4137): 5,\n",
       " (14, 4142): 4,\n",
       " (14, 4145): 3,\n",
       " (14, 4146): 4,\n",
       " (14, 4148): 4,\n",
       " (14, 4149): 5,\n",
       " (14, 4153): 5,\n",
       " (14, 4156): 5,\n",
       " (14, 4157): 3,\n",
       " (14, 4158): 3,\n",
       " (14, 4161): 3,\n",
       " (14, 4162): 5,\n",
       " (14, 4163): 4,\n",
       " (14, 4165): 3,\n",
       " (14, 4166): 4,\n",
       " (14, 4168): 2,\n",
       " (14, 4172): 2,\n",
       " (14, 4176): 3,\n",
       " (14, 4177): 3,\n",
       " (14, 4179): 3,\n",
       " (14, 4188): 1,\n",
       " (14, 4189): 2,\n",
       " (14, 4195): -1,\n",
       " (14, 4211): 0,\n",
       " (14, 4213): 2,\n",
       " (14, 4226): 2,\n",
       " (14, 4230): 2,\n",
       " (14, 4250): 1,\n",
       " (14, 4276): 0,\n",
       " (14, 4285): -1,\n",
       " (14, 4321): -1,\n",
       " (14, 4335): -1,\n",
       " (14, 5205): 12,\n",
       " (14, 5206): 12,\n",
       " (14, 5210): 15,\n",
       " (14, 5211): 22,\n",
       " (14, 5212): 20,\n",
       " (14, 5215): 10,\n",
       " (14, 5216): 16,\n",
       " (14, 5219): 22,\n",
       " (14, 5222): 24,\n",
       " (14, 5225): 13,\n",
       " (14, 5226): 13,\n",
       " (14, 5228): 25,\n",
       " (14, 5231): 13,\n",
       " (14, 5232): 8,\n",
       " (14, 5237): 26,\n",
       " (14, 5241): 28,\n",
       " (14, 5249): 4,\n",
       " (14, 5251): 25,\n",
       " (14, 5252): 26,\n",
       " (14, 5253): 23,\n",
       " (14, 5254): 17,\n",
       " (14, 5257): 12,\n",
       " (14, 5260): 13,\n",
       " (14, 5261): 27,\n",
       " (14, 5262): 28,\n",
       " (14, 5265): 25,\n",
       " (14, 5266): 28,\n",
       " (14, 5273): 23,\n",
       " (14, 5274): 13,\n",
       " (14, 5276): 17,\n",
       " (14, 5278): 13,\n",
       " (14, 5285): 22,\n",
       " (14, 5287): 21,\n",
       " (14, 5290): 25,\n",
       " (14, 5291): 16,\n",
       " (14, 5297): 26,\n",
       " (14, 5302): 7,\n",
       " (14, 5315): 26,\n",
       " (14, 5326): 26,\n",
       " (14, 5328): 26,\n",
       " (14, 5330): 20,\n",
       " (14, 5331): 26,\n",
       " (14, 5339): 26,\n",
       " (14, 5352): 16,\n",
       " (14, 5353): 26,\n",
       " (14, 5364): 22,\n",
       " (14, 5365): 26,\n",
       " (14, 5366): 16,\n",
       " (14, 5367): 26,\n",
       " (14, 5374): 25,\n",
       " (14, 5375): 25,\n",
       " (14, 5381): 26,\n",
       " (14, 5383): 13,\n",
       " (14, 5387): 22,\n",
       " (14, 5401): 20,\n",
       " (14, 5402): 24,\n",
       " (14, 5404): 24,\n",
       " (14, 5409): 23,\n",
       " (14, 5410): 23,\n",
       " (14, 5412): 23,\n",
       " (14, 5413): 24,\n",
       " (14, 5416): 24,\n",
       " (14, 5420): 19,\n",
       " (14, 5421): 24,\n",
       " (14, 5422): 23,\n",
       " (14, 5433): 19,\n",
       " (14, 5436): 0,\n",
       " (14, 5439): 17,\n",
       " (14, 5442): 23,\n",
       " (14, 5443): 23,\n",
       " (14, 5446): 23,\n",
       " (14, 5448): 23,\n",
       " (14, 5450): 20,\n",
       " (14, 5453): 21,\n",
       " (14, 5460): 17,\n",
       " (14, 5464): 20,\n",
       " (14, 5466): 7,\n",
       " (14, 5468): 6,\n",
       " (14, 5477): 21,\n",
       " (14, 5485): 17,\n",
       " (14, 5490): 7,\n",
       " (14, 5491): 17,\n",
       " (14, 5496): 20,\n",
       " (14, 5501): 14,\n",
       " (14, 5502): 20,\n",
       " (14, 5504): 16,\n",
       " (14, 5510): 20,\n",
       " ...}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get interaction age as dictionary\n",
    "interaction = train[['user_id', 'video_id', 'time']]\n",
    "interaction['interaction_age'] = (pd.to_datetime(CURRENT_DATE) - interaction['time']).dt.days\n",
    "\n",
    "interaction_age_dict = interaction.groupby(['user_id', 'video_id'])['interaction_age'].max().to_dict()\n",
    "interaction_age_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'video_id', 'time', 'watch_ratio', 'user_active_degree',\n",
       "       'is_lowactive_period', 'is_live_streamer', 'is_video_author',\n",
       "       'follow_user_num', 'fans_user_num', 'friend_user_num', 'register_days',\n",
       "       'author_id', 'video_type', 'video_tag_name', 'video_duration',\n",
       "       'show_cnt', 'play_cnt', 'play_duration', 'like_cnt', 'comment_cnt',\n",
       "       'share_cnt', 'follow_cnt', 'collect_cnt', 'manual_cover_text',\n",
       "       'caption', 'topic_tag', 'first_level_category_name',\n",
       "       'second_level_category_name', 'third_level_category_name',\n",
       "       'english_caption', 'english_first_level_category_name',\n",
       "       'english_second_level_category_name',\n",
       "       'english_third_level_category_name', 'english_topic_tag', 'is_new_user',\n",
       "       'total_connections', 'is_content_creator', 'hour', 'day_of_week',\n",
       "       'watch_frequency', 'is_weekend_interaction', 'is_weekend',\n",
       "       'time_period', 'count_afternoon_views', 'count_evening_views',\n",
       "       'count_midnight_views', 'count_morning_views', 'avg_daily_watch_time',\n",
       "       'top_3_categories', 'cluster', 'News_Politics', 'Auto_Tech',\n",
       "       'Lifestyle', 'Sports_Fitness', 'Entertainment', 'Culture', 'Others'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segregate by \n",
    "# 'cluster'\n",
    "\n",
    "# One hot encode\n",
    "# 'user_active_degree', 'time_period'\n",
    "\n",
    "# Remove\n",
    "# 'author_id', 'video_type' (all 'NORMAL'), \n",
    "# 'manual_cover_text', 'video_tag_name'\n",
    "# 'caption', 'topic_tag', 'first_level_category_name',\n",
    "# 'second_level_category_name', 'third_level_category_name',\n",
    "# 'english_caption', 'english_first_level_category_name',\n",
    "# 'english_second_level_category_name',\n",
    "# 'english_third_level_category_name', 'english_topic_tag'\n",
    "# 'top_3_categories' (use the OHE categories)\n",
    "# 'play_duration' (as it is highly correlated to target variable 'watch_ratio' (watch_ratio = play_duration / video_duration))\n",
    "\n",
    "# To incorporate (time decay)\n",
    "# 'time', 'video_age'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for feeding into Neural Network portion of NCF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode 'user_active_degree', 'time_period'\n",
    "train_processed = pd.get_dummies(train, columns=['user_active_degree', 'time_period'])\n",
    "\n",
    "# Remove the column for user_active_degree = UNKNOWN\n",
    "train_processed = train_processed.drop(columns=['user_active_degree_UNKNOWN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed = train_processed.drop(columns=['author_id', 'video_type', \n",
    "                                    'video_tag_name', \n",
    "                                    'manual_cover_text', 'caption', 'topic_tag', \n",
    "                                    'first_level_category_name', 'second_level_category_name', 'third_level_category_name',\n",
    "                                    'english_caption', 'english_first_level_category_name',\n",
    "                                    'english_second_level_category_name',\n",
    "                                    'english_third_level_category_name', 'english_topic_tag',\n",
    "                                    'top_3_categories',\n",
    "                                    'play_duration'\n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'video_id', 'time', 'watch_ratio', 'is_lowactive_period',\n",
       "       'is_live_streamer', 'is_video_author', 'follow_user_num',\n",
       "       'fans_user_num', 'friend_user_num', 'register_days', 'video_duration',\n",
       "       'show_cnt', 'play_cnt', 'like_cnt', 'comment_cnt', 'share_cnt',\n",
       "       'follow_cnt', 'collect_cnt', 'is_new_user', 'total_connections',\n",
       "       'is_content_creator', 'hour', 'day_of_week', 'watch_frequency',\n",
       "       'is_weekend_interaction', 'is_weekend', 'count_afternoon_views',\n",
       "       'count_evening_views', 'count_midnight_views', 'count_morning_views',\n",
       "       'avg_daily_watch_time', 'cluster', 'News_Politics', 'Auto_Tech',\n",
       "       'Lifestyle', 'Sports_Fitness', 'Entertainment', 'Culture', 'Others',\n",
       "       'user_active_degree_full_active', 'user_active_degree_high_active',\n",
       "       'user_active_degree_middle_active', 'time_period_afternoon',\n",
       "       'time_period_evening', 'time_period_midnight', 'time_period_morning'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_processed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>follow_user_num</th>\n",
       "      <th>fans_user_num</th>\n",
       "      <th>friend_user_num</th>\n",
       "      <th>register_days</th>\n",
       "      <th>video_duration</th>\n",
       "      <th>show_cnt</th>\n",
       "      <th>play_cnt</th>\n",
       "      <th>like_cnt</th>\n",
       "      <th>comment_cnt</th>\n",
       "      <th>share_cnt</th>\n",
       "      <th>follow_cnt</th>\n",
       "      <th>collect_cnt</th>\n",
       "      <th>count_afternoon_views</th>\n",
       "      <th>count_evening_views</th>\n",
       "      <th>count_midnight_views</th>\n",
       "      <th>count_morning_views</th>\n",
       "      <th>avg_daily_watch_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.381411e+01</td>\n",
       "      <td>3.872561e+00</td>\n",
       "      <td>1.331606e+00</td>\n",
       "      <td>2.653341e+02</td>\n",
       "      <td>1.164791e+04</td>\n",
       "      <td>6.959049e+06</td>\n",
       "      <td>7.052437e+06</td>\n",
       "      <td>2.044780e+05</td>\n",
       "      <td>8.935899e+03</td>\n",
       "      <td>3.805251e+03</td>\n",
       "      <td>2.093272e+04</td>\n",
       "      <td>2.858760e+02</td>\n",
       "      <td>4.658341e+02</td>\n",
       "      <td>2.809108e+02</td>\n",
       "      <td>4.579366e+02</td>\n",
       "      <td>6.100598e+02</td>\n",
       "      <td>8.062631e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.418902e+02</td>\n",
       "      <td>9.716679e+00</td>\n",
       "      <td>4.924868e+00</td>\n",
       "      <td>2.640708e+02</td>\n",
       "      <td>1.344116e+04</td>\n",
       "      <td>9.275605e+06</td>\n",
       "      <td>9.511481e+06</td>\n",
       "      <td>3.209431e+05</td>\n",
       "      <td>2.111983e+04</td>\n",
       "      <td>1.269530e+04</td>\n",
       "      <td>6.331006e+04</td>\n",
       "      <td>1.337505e+03</td>\n",
       "      <td>2.844922e+02</td>\n",
       "      <td>2.385123e+02</td>\n",
       "      <td>4.339834e+02</td>\n",
       "      <td>3.305712e+02</td>\n",
       "      <td>7.068827e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>3.066000e+03</td>\n",
       "      <td>6.440000e+02</td>\n",
       "      <td>3.310000e+02</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.632392e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.190000e+02</td>\n",
       "      <td>7.333000e+03</td>\n",
       "      <td>8.329130e+05</td>\n",
       "      <td>7.629220e+05</td>\n",
       "      <td>1.552800e+04</td>\n",
       "      <td>3.450000e+02</td>\n",
       "      <td>6.400000e+01</td>\n",
       "      <td>1.002000e+03</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>2.490000e+02</td>\n",
       "      <td>8.400000e+01</td>\n",
       "      <td>5.300000e+01</td>\n",
       "      <td>3.740000e+02</td>\n",
       "      <td>7.686325e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.500000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+02</td>\n",
       "      <td>9.383000e+03</td>\n",
       "      <td>3.127692e+06</td>\n",
       "      <td>3.071419e+06</td>\n",
       "      <td>7.359000e+04</td>\n",
       "      <td>2.171000e+03</td>\n",
       "      <td>4.140000e+02</td>\n",
       "      <td>4.968000e+03</td>\n",
       "      <td>2.800000e+01</td>\n",
       "      <td>4.440000e+02</td>\n",
       "      <td>2.250000e+02</td>\n",
       "      <td>3.560000e+02</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>8.158000e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.300000e+01</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.020000e+02</td>\n",
       "      <td>1.150000e+04</td>\n",
       "      <td>9.372330e+06</td>\n",
       "      <td>9.544620e+06</td>\n",
       "      <td>2.512090e+05</td>\n",
       "      <td>8.918000e+03</td>\n",
       "      <td>2.275000e+03</td>\n",
       "      <td>1.797800e+04</td>\n",
       "      <td>1.330000e+02</td>\n",
       "      <td>6.560000e+02</td>\n",
       "      <td>4.190000e+02</td>\n",
       "      <td>7.480000e+02</td>\n",
       "      <td>8.060000e+02</td>\n",
       "      <td>8.518700e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.811000e+03</td>\n",
       "      <td>2.510000e+02</td>\n",
       "      <td>7.100000e+01</td>\n",
       "      <td>2.002000e+03</td>\n",
       "      <td>2.945200e+05</td>\n",
       "      <td>6.525508e+07</td>\n",
       "      <td>6.479578e+07</td>\n",
       "      <td>2.762854e+06</td>\n",
       "      <td>3.383650e+05</td>\n",
       "      <td>2.061050e+05</td>\n",
       "      <td>1.215372e+06</td>\n",
       "      <td>2.919700e+04</td>\n",
       "      <td>1.477000e+03</td>\n",
       "      <td>1.435000e+03</td>\n",
       "      <td>1.852000e+03</td>\n",
       "      <td>1.727000e+03</td>\n",
       "      <td>1.277244e+13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       follow_user_num  fans_user_num  friend_user_num  register_days  \\\n",
       "count     2.552082e+06   2.552082e+06     2.552082e+06   2.552082e+06   \n",
       "mean      5.381411e+01   3.872561e+00     1.331606e+00   2.653341e+02   \n",
       "std       1.418902e+02   9.716679e+00     4.924868e+00   2.640708e+02   \n",
       "min       0.000000e+00   0.000000e+00     0.000000e+00   8.000000e+00   \n",
       "25%       7.000000e+00   0.000000e+00     0.000000e+00   1.190000e+02   \n",
       "50%       1.500000e+01   1.000000e+00     0.000000e+00   2.000000e+02   \n",
       "75%       4.300000e+01   4.000000e+00     1.000000e+00   3.020000e+02   \n",
       "max       1.811000e+03   2.510000e+02     7.100000e+01   2.002000e+03   \n",
       "\n",
       "       video_duration      show_cnt      play_cnt      like_cnt   comment_cnt  \\\n",
       "count    2.552082e+06  2.552082e+06  2.552082e+06  2.552082e+06  2.552082e+06   \n",
       "mean     1.164791e+04  6.959049e+06  7.052437e+06  2.044780e+05  8.935899e+03   \n",
       "std      1.344116e+04  9.275605e+06  9.511481e+06  3.209431e+05  2.111983e+04   \n",
       "min      3.066000e+03  6.440000e+02  3.310000e+02  2.000000e+00  0.000000e+00   \n",
       "25%      7.333000e+03  8.329130e+05  7.629220e+05  1.552800e+04  3.450000e+02   \n",
       "50%      9.383000e+03  3.127692e+06  3.071419e+06  7.359000e+04  2.171000e+03   \n",
       "75%      1.150000e+04  9.372330e+06  9.544620e+06  2.512090e+05  8.918000e+03   \n",
       "max      2.945200e+05  6.525508e+07  6.479578e+07  2.762854e+06  3.383650e+05   \n",
       "\n",
       "          share_cnt    follow_cnt   collect_cnt  count_afternoon_views  \\\n",
       "count  2.552082e+06  2.552082e+06  2.552082e+06           2.552082e+06   \n",
       "mean   3.805251e+03  2.093272e+04  2.858760e+02           4.658341e+02   \n",
       "std    1.269530e+04  6.331006e+04  1.337505e+03           2.844922e+02   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00           0.000000e+00   \n",
       "25%    6.400000e+01  1.002000e+03  5.000000e+00           2.490000e+02   \n",
       "50%    4.140000e+02  4.968000e+03  2.800000e+01           4.440000e+02   \n",
       "75%    2.275000e+03  1.797800e+04  1.330000e+02           6.560000e+02   \n",
       "max    2.061050e+05  1.215372e+06  2.919700e+04           1.477000e+03   \n",
       "\n",
       "       count_evening_views  count_midnight_views  count_morning_views  \\\n",
       "count         2.552082e+06          2.552082e+06         2.552082e+06   \n",
       "mean          2.809108e+02          4.579366e+02         6.100598e+02   \n",
       "std           2.385123e+02          4.339834e+02         3.305712e+02   \n",
       "min           0.000000e+00          0.000000e+00         0.000000e+00   \n",
       "25%           8.400000e+01          5.300000e+01         3.740000e+02   \n",
       "50%           2.250000e+02          3.560000e+02         5.690000e+02   \n",
       "75%           4.190000e+02          7.480000e+02         8.060000e+02   \n",
       "max           1.435000e+03          1.852000e+03         1.727000e+03   \n",
       "\n",
       "       avg_daily_watch_time  \n",
       "count          2.552082e+06  \n",
       "mean           8.062631e+12  \n",
       "std            7.068827e+11  \n",
       "min            4.632392e+12  \n",
       "25%            7.686325e+12  \n",
       "50%            8.158000e+12  \n",
       "75%            8.518700e+12  \n",
       "max            1.277244e+13  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see that the values are all on different scales. For example, follow_user_num is in the tens-thousands while like_cnt can range form millions to billions. This will affect the training of the model, therefore scaling is needed\n",
    "\n",
    "train_processed[['follow_user_num',\n",
    "       'fans_user_num', 'friend_user_num', 'register_days', 'video_duration',\n",
    "       'show_cnt', 'play_cnt', 'like_cnt', 'comment_cnt',\n",
    "       'share_cnt', 'follow_cnt', 'collect_cnt', 'count_afternoon_views', 'count_evening_views', 'count_midnight_views',\n",
    "       'count_morning_views', 'avg_daily_watch_time']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "columns_to_scale = ['follow_user_num',\n",
    "       'fans_user_num', 'friend_user_num', 'register_days', 'video_duration',\n",
    "       'show_cnt', 'play_cnt', \n",
    "       'like_cnt', 'comment_cnt',\n",
    "       'share_cnt', 'follow_cnt', 'collect_cnt', \n",
    "       'total_connections',\n",
    "       'watch_frequency', \n",
    "       'count_afternoon_views', 'count_evening_views', 'count_midnight_views',\n",
    "       'count_morning_views', \n",
    "       'avg_daily_watch_time', \n",
    "       ]\n",
    "\n",
    "train_processed[columns_to_scale] = scaler.fit_transform(train_processed[columns_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>follow_user_num</th>\n",
       "      <th>fans_user_num</th>\n",
       "      <th>friend_user_num</th>\n",
       "      <th>register_days</th>\n",
       "      <th>video_duration</th>\n",
       "      <th>show_cnt</th>\n",
       "      <th>play_cnt</th>\n",
       "      <th>like_cnt</th>\n",
       "      <th>comment_cnt</th>\n",
       "      <th>share_cnt</th>\n",
       "      <th>follow_cnt</th>\n",
       "      <th>collect_cnt</th>\n",
       "      <th>count_afternoon_views</th>\n",
       "      <th>count_evening_views</th>\n",
       "      <th>count_midnight_views</th>\n",
       "      <th>count_morning_views</th>\n",
       "      <th>avg_daily_watch_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.875563e-18</td>\n",
       "      <td>-3.162816e-17</td>\n",
       "      <td>1.933327e-17</td>\n",
       "      <td>-6.521637e-17</td>\n",
       "      <td>-2.266133e-16</td>\n",
       "      <td>4.790998e-17</td>\n",
       "      <td>8.174320e-18</td>\n",
       "      <td>-2.525798e-17</td>\n",
       "      <td>-1.905485e-17</td>\n",
       "      <td>-1.854256e-18</td>\n",
       "      <td>-9.449469e-18</td>\n",
       "      <td>1.789107e-17</td>\n",
       "      <td>-5.862346e-17</td>\n",
       "      <td>-3.456824e-17</td>\n",
       "      <td>2.940082e-17</td>\n",
       "      <td>1.021901e-16</td>\n",
       "      <td>-2.135391e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.792660e-01</td>\n",
       "      <td>-3.985478e-01</td>\n",
       "      <td>-2.703841e-01</td>\n",
       "      <td>-9.744891e-01</td>\n",
       "      <td>-6.384799e-01</td>\n",
       "      <td>-7.501836e-01</td>\n",
       "      <td>-7.414311e-01</td>\n",
       "      <td>-6.371098e-01</td>\n",
       "      <td>-4.231048e-01</td>\n",
       "      <td>-2.997370e-01</td>\n",
       "      <td>-3.306382e-01</td>\n",
       "      <td>-2.137384e-01</td>\n",
       "      <td>-1.637424e+00</td>\n",
       "      <td>-1.177763e+00</td>\n",
       "      <td>-1.055194e+00</td>\n",
       "      <td>-1.845472e+00</td>\n",
       "      <td>-4.852628e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-3.299320e-01</td>\n",
       "      <td>-3.985478e-01</td>\n",
       "      <td>-2.703841e-01</td>\n",
       "      <td>-5.541473e-01</td>\n",
       "      <td>-3.210220e-01</td>\n",
       "      <td>-6.604569e-01</td>\n",
       "      <td>-6.612552e-01</td>\n",
       "      <td>-5.887337e-01</td>\n",
       "      <td>-4.067695e-01</td>\n",
       "      <td>-2.946957e-01</td>\n",
       "      <td>-3.148114e-01</td>\n",
       "      <td>-2.100001e-01</td>\n",
       "      <td>-7.621797e-01</td>\n",
       "      <td>-8.255794e-01</td>\n",
       "      <td>-9.330694e-01</td>\n",
       "      <td>-7.140968e-01</td>\n",
       "      <td>-5.323456e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-2.735504e-01</td>\n",
       "      <td>-2.956320e-01</td>\n",
       "      <td>-2.703841e-01</td>\n",
       "      <td>-2.474113e-01</td>\n",
       "      <td>-1.685054e-01</td>\n",
       "      <td>-4.130575e-01</td>\n",
       "      <td>-4.185488e-01</td>\n",
       "      <td>-4.078230e-01</td>\n",
       "      <td>-3.203104e-01</td>\n",
       "      <td>-2.671265e-01</td>\n",
       "      <td>-2.521673e-01</td>\n",
       "      <td>-1.928039e-01</td>\n",
       "      <td>-7.674775e-02</td>\n",
       "      <td>-2.344147e-01</td>\n",
       "      <td>-2.348860e-01</td>\n",
       "      <td>-1.242086e-01</td>\n",
       "      <td>1.349154e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-7.621467e-02</td>\n",
       "      <td>1.311554e-02</td>\n",
       "      <td>-6.733294e-02</td>\n",
       "      <td>1.388488e-01</td>\n",
       "      <td>-1.100400e-02</td>\n",
       "      <td>2.601751e-01</td>\n",
       "      <td>2.620184e-01</td>\n",
       "      <td>1.456054e-01</td>\n",
       "      <td>-8.475032e-04</td>\n",
       "      <td>-1.205368e-01</td>\n",
       "      <td>-4.667066e-02</td>\n",
       "      <td>-1.142995e-01</td>\n",
       "      <td>6.684398e-01</td>\n",
       "      <td>5.789608e-01</td>\n",
       "      <td>6.683745e-01</td>\n",
       "      <td>5.927325e-01</td>\n",
       "      <td>6.451838e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.238413e+01</td>\n",
       "      <td>2.543333e+01</td>\n",
       "      <td>1.414625e+01</td>\n",
       "      <td>6.576517e+00</td>\n",
       "      <td>2.104522e+01</td>\n",
       "      <td>6.284877e+00</td>\n",
       "      <td>6.070911e+00</td>\n",
       "      <td>7.971434e+00</td>\n",
       "      <td>1.559810e+01</td>\n",
       "      <td>1.593501e+01</td>\n",
       "      <td>1.886651e+01</td>\n",
       "      <td>2.161572e+01</td>\n",
       "      <td>3.554284e+00</td>\n",
       "      <td>4.838701e+00</td>\n",
       "      <td>3.212251e+00</td>\n",
       "      <td>3.378820e+00</td>\n",
       "      <td>6.662791e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       follow_user_num  fans_user_num  friend_user_num  register_days  \\\n",
       "count     2.552082e+06   2.552082e+06     2.552082e+06   2.552082e+06   \n",
       "mean      3.875563e-18  -3.162816e-17     1.933327e-17  -6.521637e-17   \n",
       "std       1.000000e+00   1.000000e+00     1.000000e+00   1.000000e+00   \n",
       "min      -3.792660e-01  -3.985478e-01    -2.703841e-01  -9.744891e-01   \n",
       "25%      -3.299320e-01  -3.985478e-01    -2.703841e-01  -5.541473e-01   \n",
       "50%      -2.735504e-01  -2.956320e-01    -2.703841e-01  -2.474113e-01   \n",
       "75%      -7.621467e-02   1.311554e-02    -6.733294e-02   1.388488e-01   \n",
       "max       1.238413e+01   2.543333e+01     1.414625e+01   6.576517e+00   \n",
       "\n",
       "       video_duration      show_cnt      play_cnt      like_cnt   comment_cnt  \\\n",
       "count    2.552082e+06  2.552082e+06  2.552082e+06  2.552082e+06  2.552082e+06   \n",
       "mean    -2.266133e-16  4.790998e-17  8.174320e-18 -2.525798e-17 -1.905485e-17   \n",
       "std      1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min     -6.384799e-01 -7.501836e-01 -7.414311e-01 -6.371098e-01 -4.231048e-01   \n",
       "25%     -3.210220e-01 -6.604569e-01 -6.612552e-01 -5.887337e-01 -4.067695e-01   \n",
       "50%     -1.685054e-01 -4.130575e-01 -4.185488e-01 -4.078230e-01 -3.203104e-01   \n",
       "75%     -1.100400e-02  2.601751e-01  2.620184e-01  1.456054e-01 -8.475032e-04   \n",
       "max      2.104522e+01  6.284877e+00  6.070911e+00  7.971434e+00  1.559810e+01   \n",
       "\n",
       "          share_cnt    follow_cnt   collect_cnt  count_afternoon_views  \\\n",
       "count  2.552082e+06  2.552082e+06  2.552082e+06           2.552082e+06   \n",
       "mean  -1.854256e-18 -9.449469e-18  1.789107e-17          -5.862346e-17   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00           1.000000e+00   \n",
       "min   -2.997370e-01 -3.306382e-01 -2.137384e-01          -1.637424e+00   \n",
       "25%   -2.946957e-01 -3.148114e-01 -2.100001e-01          -7.621797e-01   \n",
       "50%   -2.671265e-01 -2.521673e-01 -1.928039e-01          -7.674775e-02   \n",
       "75%   -1.205368e-01 -4.667066e-02 -1.142995e-01           6.684398e-01   \n",
       "max    1.593501e+01  1.886651e+01  2.161572e+01           3.554284e+00   \n",
       "\n",
       "       count_evening_views  count_midnight_views  count_morning_views  \\\n",
       "count         2.552082e+06          2.552082e+06         2.552082e+06   \n",
       "mean         -3.456824e-17          2.940082e-17         1.021901e-16   \n",
       "std           1.000000e+00          1.000000e+00         1.000000e+00   \n",
       "min          -1.177763e+00         -1.055194e+00        -1.845472e+00   \n",
       "25%          -8.255794e-01         -9.330694e-01        -7.140968e-01   \n",
       "50%          -2.344147e-01         -2.348860e-01        -1.242086e-01   \n",
       "75%           5.789608e-01          6.683745e-01         5.927325e-01   \n",
       "max           4.838701e+00          3.212251e+00         3.378820e+00   \n",
       "\n",
       "       avg_daily_watch_time  \n",
       "count          2.552082e+06  \n",
       "mean          -2.135391e-15  \n",
       "std            1.000000e+00  \n",
       "min           -4.852628e+00  \n",
       "25%           -5.323456e-01  \n",
       "50%            1.349154e-01  \n",
       "75%            6.451838e-01  \n",
       "max            6.662791e+00  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_processed[['follow_user_num',\n",
    "       'fans_user_num', 'friend_user_num', 'register_days', 'video_duration',\n",
    "       'show_cnt', 'play_cnt', 'like_cnt', 'comment_cnt',\n",
    "       'share_cnt', 'follow_cnt', 'collect_cnt', 'count_afternoon_views', 'count_evening_views', 'count_midnight_views',\n",
    "       'count_morning_views', 'avg_daily_watch_time']].describe()\n",
    "\n",
    "# We now see that the mean of all the columns is (close to) 0 and the standard deviation is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KuaiShouDataset(Dataset):\n",
    "    def __init__(self, data, user_id_col, video_id_col, user_feature_cols, video_feature_cols, watch_ratio_col, interaction_age_dict, video_age_dict):\n",
    "        self.user_feature_cols = user_feature_cols\n",
    "        self.video_feature_cols = video_feature_cols\n",
    "\n",
    "        # Initialise and fit LabelEncoders\n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.video_encoder = LabelEncoder()\n",
    "        \n",
    "        self.user_indices = torch.tensor(self.user_encoder.fit_transform(data[user_id_col]), dtype=torch.long)\n",
    "        self.video_indices = torch.tensor(self.video_encoder.fit_transform(data[video_id_col]), dtype=torch.long)\n",
    "\n",
    "        # Convert to correct numpy dtype before creating tensors\n",
    "        user_features = data[user_feature_cols].values.astype(np.float32)\n",
    "        video_features = data[video_feature_cols].values.astype(np.float32)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        self.user_features = torch.tensor(user_features, dtype=torch.float32)\n",
    "        self.video_features = torch.tensor(video_features, dtype=torch.float32)\n",
    "        self.watch_ratios = torch.tensor(data[watch_ratio_col].values, dtype=torch.float32)\n",
    "\n",
    "        # Time related features\n",
    "        self.interaction_age_dict = interaction_age_dict\n",
    "        self.video_age_dict = video_age_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.user_indices[idx], self.video_indices[idx], self.user_features[idx], self.video_features[idx], self.watch_ratios[idx]\n",
    "\n",
    "    def inverse_transform_user_ids(self, encoded_user_idx):\n",
    "        \"\"\"Decode encoded user indices to original user_ids.\"\"\"\n",
    "        return self.user_encoder.inverse_transform(encoded_user_idx)\n",
    "    \n",
    "    def inverse_transform_video_ids(self, encoded_video_idx):\n",
    "        \"\"\"Decode encoded video indices to original video_ids.\"\"\"\n",
    "        return self.video_encoder.inverse_transform(encoded_video_idx)\n",
    "    \n",
    "    def get_interaction_age(self, user_idx, video_idx):\n",
    "        \"\"\"Get days since interaction.\"\"\"\n",
    "        user_ids = self.inverse_transform_user_ids(user_idx)\n",
    "        video_ids = self.inverse_transform_video_ids(video_idx)\n",
    "\n",
    "        ages = []\n",
    "        for i in range(len(user_ids)):\n",
    "            ages.append(self.interaction_age_dict[(user_ids[i], video_ids[i])])\n",
    "        return torch.tensor(ages, dtype=torch.float32)\n",
    "    \n",
    "    def get_video_age(self, video_idx):\n",
    "        \"\"\"Get video age.\"\"\"\n",
    "        video_ids = self.inverse_transform_video_ids(video_idx)\n",
    "\n",
    "        ages = []\n",
    "        for i in range(len(video_idx)):\n",
    "            ages.append(self.video_age_dict[video_ids[i]])\n",
    "        return torch.tensor(ages, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Infused Neural Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_videos, embedding_dim, num_user_features, num_video_features, dropout, alpha, beta):\n",
    "        super(NCF, self).__init__()\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "        # GMF Components for embeddings\n",
    "        self.user_embeddings_gmf = nn.Embedding(num_users, embedding_dim)\n",
    "        self.video_embeddings_gmf = nn.Embedding(num_videos, embedding_dim)\n",
    "\n",
    "        # MLP Components for embeddings\n",
    "        self.user_embeddings_mlp = nn.Embedding(num_users, embedding_dim)\n",
    "        self.video_embeddings_mlp = nn.Embedding(num_videos, embedding_dim)\n",
    "\n",
    "        # MLP layers for user and video embeddings\n",
    "        self.fc1_mlp = nn.Linear(2 * embedding_dim, 128)\n",
    "        self.fc2_mlp = nn.Linear(128, 64)\n",
    "\n",
    "        # MLP layers for user and video features\n",
    "        # self.user_features_fc = nn.Linear(num_user_features, embedding_dim)\n",
    "        # self.video_features_fc = nn.Linear(num_video_features, embedding_dim)\n",
    "        self.user_video_features_fc = nn.Linear(num_user_features + num_video_features, 64)\n",
    "\n",
    "        # Final layers combining GMF, MLP for embeddings, and additional features\n",
    "        # self.fc1_combined = nn.Linear(embedding_dim + 64 + 2 * embedding_dim, 128)\n",
    "        self.fc1_combined = nn.Linear(embedding_dim + 64 + 64, 128)\n",
    "        self.fc2_combined = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, user_idx, video_idx, user_features, video_features, interaction_days, video_age_days):\n",
    "        ####### GMF Embedding branch #######\n",
    "        user_emb_gmf = self.user_embeddings_gmf(user_idx)\n",
    "        video_emb_gmf = self.video_embeddings_gmf(video_idx)\n",
    "        gmf_output = user_emb_gmf * video_emb_gmf                                   # dimension: (batch_size, embedding_dim)\n",
    "\n",
    "        ####### MLP Embedding branch #######\n",
    "        user_emb_mlp = self.user_embeddings_mlp(user_idx)\n",
    "        video_emb_mlp = self.video_embeddings_mlp(video_idx)\n",
    "        mlp_input = torch.cat([user_emb_mlp, video_emb_mlp], dim=-1)                # dimension: (batch_size, 2 * embedding_dim)\n",
    "\n",
    "        # First fully connected layer with BatchNorm and ReLU\n",
    "        mlp_output = self.fc1_mlp(mlp_input)\n",
    "        if self.training:\n",
    "            mlp_output = nn.BatchNorm1d(128)(mlp_output)\n",
    "        mlp_output = torch.relu(mlp_output)\n",
    "        mlp_output = nn.Dropout(self.dropout)(mlp_output)\n",
    "\n",
    "        # Second fully connected layer with BatchNorm and ReLU\n",
    "        mlp_output = self.fc2_mlp(mlp_output)                                       # dimension: (batch_size, 64)\n",
    "        if self.training:\n",
    "            mlp_output = nn.BatchNorm1d(64)(mlp_output)\n",
    "        mlp_output = torch.relu(mlp_output)\n",
    "        mlp_output = nn.Dropout(self.dropout)(mlp_output)\n",
    "\n",
    "        ####### MLP Feature processing branch #######\n",
    "        # user_features_processed = self.user_features_fc(user_features)              # dimension: (batch_size, embedding_dim)\n",
    "        # user_features_processed = torch.relu(user_features_processed)\n",
    "        # user_features_processed = nn.Dropout(self.dropout)(user_features_processed)\n",
    "\n",
    "        # video_features_processed = self.video_features_fc(video_features)           # dimension: (batch_size, embedding_dim)\n",
    "        # video_features_processed = torch.relu(video_features_processed)\n",
    "        # video_features_processed = nn.Dropout(self.dropout)(video_features_processed)\n",
    "        user_video_features = torch.cat([user_features, video_features], dim=-1)\n",
    "        user_video_features_processed = self.user_video_features_fc(user_video_features)  # dimension: (batch_size, 64)\n",
    "        user_video_features_processed = torch.relu(user_video_features_processed)\n",
    "        user_video_features_processed = nn.Dropout(self.dropout)(user_video_features_processed)\n",
    "\n",
    "        ####### Combine GMF, MLP, and additional features #######\n",
    "        combined_input = torch.cat([gmf_output, mlp_output, user_video_features_processed], dim=-1)\n",
    "        combined_output = self.fc1_combined(combined_input)\n",
    "        # if self.training:\n",
    "        #     combined_output = nn.BatchNorm1d(128)(combined_output)\n",
    "        combined_output = torch.relu(combined_output)\n",
    "        combined_output = nn.Dropout(self.dropout)(combined_output)\n",
    "\n",
    "        combined_output = self.fc2_combined(combined_output)\n",
    "        combined_output = torch.relu(combined_output).squeeze()\n",
    "\n",
    "        ######## Apply decay factors based on interaction time and video age #######\n",
    "        decay_weight = self.calculate_exponential_weight(interaction_days, video_age_days)\n",
    "        final_output = combined_output * decay_weight\n",
    "\n",
    "        return final_output\n",
    "    \n",
    "    def calculate_exponential_weight(self, interaction_days, video_age_days):\n",
    "        \"\"\"\n",
    "        Returns the decay weight based on the defined decay constant and time since the transaction was made.\n",
    "        \"\"\"\n",
    "        decay_interaction = torch.exp(-self.alpha * interaction_days)\n",
    "        decay_video_age = torch.exp(-self.beta * video_age_days)\n",
    "        \n",
    "        return decay_interaction * decay_video_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KuaiShou_NCF_RecSys:\n",
    "    def __init__(self, dataset: KuaiShouDataset, model: nn.Module, embedding_dim: int, dropout: float, alpha: float, beta: float):\n",
    "        self.dataset = dataset\n",
    "        self.num_users = len(dataset.user_encoder.classes_)\n",
    "        self.num_videos = len(dataset.video_encoder.classes_)\n",
    "        self.num_user_features = len(dataset.user_feature_cols)\n",
    "        self.num_video_features = len(dataset.video_feature_cols)\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Move model to GPU if available\n",
    "        \n",
    "        # Initialise the model\n",
    "        self.model: nn.Module = model(self.num_users, self.num_videos, embedding_dim, self.num_user_features, self.num_video_features, dropout, alpha, beta)\n",
    "\n",
    "    def train(self, batch_size, num_epochs, lr, criterion, optimizer):\n",
    "        # Initialise the DataLoader\n",
    "        train_loader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        print(f\"Model moved to {self.device}\")\n",
    "\n",
    "        # Optimizer and loss function\n",
    "        optimizer = optimizer(self.model.parameters(), lr=lr)\n",
    "        criterion = criterion\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            \n",
    "            for user_idx, video_idx, user_features, video_features, watch_ratio in train_loader:\n",
    "                user_idx, video_idx, user_features, video_features, watch_ratio = user_idx.to(self.device), video_idx.to(self.device), user_features.to(self.device), video_features.to(self.device), watch_ratio.to(self.device)\n",
    "                \n",
    "                # Get the interaction days and video age\n",
    "                interaction_days = self.dataset.get_interaction_age(user_idx, video_idx)\n",
    "                video_age = self.dataset.get_video_age(video_idx)\n",
    "                \n",
    "                # Forward pass\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(user_idx, video_idx, user_features, video_features, interaction_days, video_age)\n",
    "                loss = criterion(outputs, watch_ratio)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Accumulate loss for reporting\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            # Print loss for each epoch\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    def predict(self, batch_size=512):\n",
    "        \"\"\"\n",
    "        Generates a dataframe with predicted watch ratios for each user-video pair in batches.\n",
    "        \"\"\"\n",
    "        self.model.eval()  # Set model to evaluation mode\n",
    "        all_predictions = []\n",
    "\n",
    "        for start_user_idx in range(0, self.num_users, batch_size):\n",
    "            end_user_idx = min(start_user_idx + batch_size, self.num_users)\n",
    "            user_indices = torch.arange(start_user_idx, end_user_idx, dtype=torch.long).to(self.device)\n",
    "\n",
    "            # Gather user features in batch\n",
    "            user_features_batch = self.dataset.user_features[user_indices].to(self.device)\n",
    "            \n",
    "            for video_idx in range(self.num_videos):\n",
    "                video_tensor = torch.tensor([video_idx], dtype=torch.long).to(self.device)\n",
    "                video_feature_tensor = self.dataset.video_features[video_idx].unsqueeze(0).to(self.device)\n",
    "                video_age = self.dataset.get_video_age(video_tensor).to(self.device)\n",
    "                \n",
    "                # Repeat video data for the entire user batch\n",
    "                video_tensor_batch = video_tensor.expand(len(user_indices))\n",
    "                video_feature_batch = video_feature_tensor.expand(len(user_indices), -1)\n",
    "                video_age_batch = video_age.expand(len(user_indices))\n",
    "\n",
    "                # Set interaction days to 0 for prediction\n",
    "                interaction_days = torch.zeros(len(user_indices), dtype=torch.float32).to(self.device)\n",
    "\n",
    "                # Predict in batch\n",
    "                with torch.no_grad():\n",
    "                    predicted_watch_ratios = self.model(\n",
    "                        user_indices, video_tensor_batch, user_features_batch,\n",
    "                        video_feature_batch, interaction_days, video_age_batch\n",
    "                    )\n",
    "\n",
    "                # Collect predictions\n",
    "                for user_idx, watch_ratio in zip(user_indices.tolist(), predicted_watch_ratios.tolist()):\n",
    "                    all_predictions.append((user_idx, video_idx, watch_ratio))\n",
    "\n",
    "        # Convert to DataFrame at once\n",
    "        watch_ratio_df = pd.DataFrame(all_predictions, columns=['user_id', 'video_id', 'watch_ratio'])\n",
    "\n",
    "        # Inverse transform user and video indices\n",
    "        watch_ratio_df['user_id'] = self.dataset.inverse_transform_user_ids(watch_ratio_df['user_id'].astype(int))\n",
    "        watch_ratio_df['video_id'] = self.dataset.inverse_transform_video_ids(watch_ratio_df['video_id'].astype(int))\n",
    "\n",
    "        return watch_ratio_df\n",
    "\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        \"\"\"\n",
    "        Returns the model parameters.\n",
    "        \"\"\"\n",
    "        return self.model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Training Data to the Model and Generating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns for user and video features in the user-item interaction data\n",
    "user_cols = ['is_lowactive_period',\n",
    "             'is_live_streamer', 'is_video_author', 'follow_user_num',\n",
    "             'fans_user_num', 'friend_user_num', 'register_days', 'is_new_user',\n",
    "             'total_connections', 'is_content_creator', 'hour', 'day_of_week',\n",
    "             'watch_frequency', 'is_weekend_interaction', 'is_weekend',\n",
    "             'count_afternoon_views', 'count_evening_views', 'count_midnight_views', 'count_morning_views', \n",
    "             'avg_daily_watch_time', \n",
    "             'user_active_degree_full_active', 'user_active_degree_high_active', 'user_active_degree_middle_active', \n",
    "             'time_period_afternoon', 'time_period_evening', 'time_period_midnight', 'time_period_morning'\n",
    "            ]\n",
    "video_cols = ['video_duration', 'show_cnt', 'play_cnt', \n",
    "              'like_cnt', 'comment_cnt', 'share_cnt', 'follow_cnt', 'collect_cnt', \n",
    "              'News_Politics', 'Auto_Tech', 'Lifestyle', 'Sports_Fitness', 'Entertainment', 'Culture', 'Others',\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(hyperparameters: dict, train_data: pd.DataFrame, **kwargs):\n",
    "    cluster = kwargs.get('cluster', None)\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    BATCH_SIZE = hyperparameters['batch_size']\n",
    "    NUM_EPOCHS = hyperparameters['num_epochs']\n",
    "    LEARNING_RATE = hyperparameters['lr']\n",
    "    EMBEDDING_DIM = hyperparameters['embedding_dim']\n",
    "    DROPOUT = hyperparameters['dropout']\n",
    "    ALPHA = hyperparameters['alpha']\n",
    "    BETA = hyperparameters['beta']\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimiser = optim.Adam\n",
    "\n",
    "    print(f\"----- Training {'' if cluster == None else f'for cluster {cluster} '}-----\")\n",
    "\n",
    "    # Create the dataset\n",
    "    dataset_train = KuaiShouDataset(train_data, 'user_id', 'video_id', user_cols, video_cols, 'watch_ratio', interaction_age_dict, video_age_dict)\n",
    "\n",
    "    # Initialise the NCF model\n",
    "    print(\"Initialising...\")\n",
    "    ncf_rec_sys = KuaiShou_NCF_RecSys(dataset_train, NCF, EMBEDDING_DIM, DROPOUT, ALPHA, BETA)\n",
    "\n",
    "    # Train on data\n",
    "    ncf_rec_sys.train(BATCH_SIZE, NUM_EPOCHS, LEARNING_RATE, criterion, optimiser)\n",
    "\n",
    "    # Generate predictions\n",
    "    print(\"Generating predictions...\")\n",
    "    predictions_df = ncf_rec_sys.predict()\n",
    "    \n",
    "    print(\"Complete!\")\n",
    "    return cluster, predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example: Fitting to Cluster 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4294\n",
      "Epoch [2/10], Loss: 0.3764\n",
      "Epoch [3/10], Loss: 0.3639\n",
      "Epoch [4/10], Loss: 0.3581\n",
      "Epoch [5/10], Loss: 0.3541\n",
      "Epoch [6/10], Loss: 0.3520\n",
      "Epoch [7/10], Loss: 0.3497\n",
      "Epoch [8/10], Loss: 0.3471\n",
      "Epoch [9/10], Loss: 0.3447\n",
      "Epoch [10/10], Loss: 0.3418\n",
      "Generating predictions...\n",
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'batch_size': 512,\n",
    "    'num_epochs': 10,\n",
    "    'lr': 0.001,\n",
    "    'embedding_dim': 64,\n",
    "    'dropout': 0.3,\n",
    "    'alpha': 0.01,  # Decay constant for days since interaction (days)\n",
    "    'beta': 0.01    # Decay constant for video age (days)\n",
    "}\n",
    "\n",
    "cluster = 0\n",
    "train_cluster = train_processed[train_processed['cluster'] == cluster]\n",
    "\n",
    "cluster, cluster_0_predictions = train_and_predict(params, train_cluster, **{'cluster': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>watch_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>103</td>\n",
       "      <td>1.548594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>103</td>\n",
       "      <td>1.339207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>103</td>\n",
       "      <td>0.704573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51</td>\n",
       "      <td>103</td>\n",
       "      <td>1.320418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>103</td>\n",
       "      <td>1.833451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634835</th>\n",
       "      <td>7086</td>\n",
       "      <td>10130</td>\n",
       "      <td>1.105983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634836</th>\n",
       "      <td>7116</td>\n",
       "      <td>10130</td>\n",
       "      <td>1.049223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634837</th>\n",
       "      <td>7132</td>\n",
       "      <td>10130</td>\n",
       "      <td>1.121811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634838</th>\n",
       "      <td>7141</td>\n",
       "      <td>10130</td>\n",
       "      <td>1.258493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634839</th>\n",
       "      <td>7162</td>\n",
       "      <td>10130</td>\n",
       "      <td>0.922992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>634840 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  video_id  watch_ratio\n",
       "0            14       103     1.548594\n",
       "1            21       103     1.339207\n",
       "2            24       103     0.704573\n",
       "3            51       103     1.320418\n",
       "4            64       103     1.833451\n",
       "...         ...       ...          ...\n",
       "634835     7086     10130     1.105983\n",
       "634836     7116     10130     1.049223\n",
       "634837     7132     10130     1.121811\n",
       "634838     7141     10130     1.258493\n",
       "634839     7162     10130     0.922992\n",
       "\n",
       "[634840 rows x 3 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_0_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there a way to parallelise this?\n",
    "def train_by_cluster_and_without(params: dict, train_data: pd.DataFrame, train_by_cluster: bool = True, train_without_clustering: bool = False):\n",
    "    param_str = '_'.join([f'{key}{val}' for key, val in params.items()])\n",
    "\n",
    "    # Train for each cluster\n",
    "    if train_by_cluster:\n",
    "        cluster_predictions = {}\n",
    "        for cluster in sorted(train_data['cluster'].unique()):\n",
    "            train_cluster = train_data[train_data['cluster'] == cluster]\n",
    "\n",
    "            cluster, predictions_df = train_and_predict(params, train_cluster, **{'cluster': cluster})\n",
    "            cluster_predictions[cluster] = predictions_df\n",
    "\n",
    "            # Include parameters in the output file name\n",
    "            output_file = root + f'results/cluster{cluster}_{param_str}.csv'\n",
    "            predictions_df.to_csv(output_file, index=False)\n",
    "\n",
    "            print(f'Predictions for cluster {cluster} saved to {output_file}')\n",
    "        \n",
    "        # Save combined predictions to a single file\n",
    "        watch_ratio_predictions_df = pd.DataFrame()\n",
    "        for cluster, df in cluster_predictions.items():\n",
    "            cluster_predictions_df = df\n",
    "            cluster_predictions_df['cluster'] = cluster\n",
    "            \n",
    "            watch_ratio_predictions_df = pd.concat([watch_ratio_predictions_df, cluster_predictions_df])\n",
    "        \n",
    "        # Include parameters in the output file name\n",
    "        output_file = root + f'results/w_clustering_{param_str}.csv'\n",
    "        watch_ratio_predictions_df.to_csv(output_file, index=False)\n",
    "        print(f'Predictions with segmentation saved to {output_file}')\n",
    "    \n",
    "    # Train without clustering\n",
    "    if train_without_clustering:\n",
    "        _, predictions_df = train_and_predict(params, train_data)\n",
    "\n",
    "        # Include parameters in the output file name\n",
    "        output_file = root + f'results/wo_clustering_{param_str}.csv'\n",
    "        predictions_df.to_csv(output_file, index=False)\n",
    "        print(f'Predictions without segmentation saved to {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4294\n",
      "Epoch [2/10], Loss: 0.3764\n",
      "Epoch [3/10], Loss: 0.3639\n",
      "Epoch [4/10], Loss: 0.3581\n",
      "Epoch [5/10], Loss: 0.3541\n",
      "Epoch [6/10], Loss: 0.3520\n",
      "Epoch [7/10], Loss: 0.3497\n",
      "Epoch [8/10], Loss: 0.3471\n",
      "Epoch [9/10], Loss: 0.3447\n",
      "Epoch [10/10], Loss: 0.3418\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.3_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.2275\n",
      "Epoch [2/10], Loss: 0.1863\n",
      "Epoch [3/10], Loss: 0.1792\n",
      "Epoch [4/10], Loss: 0.1766\n",
      "Epoch [5/10], Loss: 0.1750\n",
      "Epoch [6/10], Loss: 0.1737\n",
      "Epoch [7/10], Loss: 0.1727\n",
      "Epoch [8/10], Loss: 0.1713\n",
      "Epoch [9/10], Loss: 0.1700\n",
      "Epoch [10/10], Loss: 0.1688\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.3_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.2831\n",
      "Epoch [2/10], Loss: 0.2398\n",
      "Epoch [3/10], Loss: 0.2309\n",
      "Epoch [4/10], Loss: 0.2270\n",
      "Epoch [5/10], Loss: 0.2253\n",
      "Epoch [6/10], Loss: 0.2238\n",
      "Epoch [7/10], Loss: 0.2226\n",
      "Epoch [8/10], Loss: 0.2212\n",
      "Epoch [9/10], Loss: 0.2196\n",
      "Epoch [10/10], Loss: 0.2179\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.3_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.2831\n",
      "Epoch [2/10], Loss: 0.2402\n",
      "Epoch [3/10], Loss: 0.2321\n",
      "Epoch [4/10], Loss: 0.2286\n",
      "Epoch [5/10], Loss: 0.2268\n",
      "Epoch [6/10], Loss: 0.2251\n",
      "Epoch [7/10], Loss: 0.2238\n",
      "Epoch [8/10], Loss: 0.2223\n",
      "Epoch [9/10], Loss: 0.2209\n",
      "Epoch [10/10], Loss: 0.2188\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.3_alpha0.01_beta0.01.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.3_alpha0.01_beta0.01.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.2599\n",
      "Epoch [2/10], Loss: 0.2373\n",
      "Epoch [3/10], Loss: 0.2349\n",
      "Epoch [4/10], Loss: 0.2333\n",
      "Epoch [5/10], Loss: 0.2321\n",
      "Epoch [6/10], Loss: 0.2309\n",
      "Epoch [7/10], Loss: 0.2296\n",
      "Epoch [8/10], Loss: 0.2281\n",
      "Epoch [9/10], Loss: 0.2263\n",
      "Epoch [10/10], Loss: 0.2244\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.3_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.5165\n",
      "Epoch [2/10], Loss: 0.3876\n",
      "Epoch [3/10], Loss: 0.3731\n",
      "Epoch [4/10], Loss: 0.3670\n",
      "Epoch [5/10], Loss: 0.3625\n",
      "Epoch [6/10], Loss: 0.3598\n",
      "Epoch [7/10], Loss: 0.3568\n",
      "Epoch [8/10], Loss: 0.3542\n",
      "Epoch [9/10], Loss: 0.3518\n",
      "Epoch [10/10], Loss: 0.3489\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.3_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.2820\n",
      "Epoch [2/10], Loss: 0.1959\n",
      "Epoch [3/10], Loss: 0.1880\n",
      "Epoch [4/10], Loss: 0.1842\n",
      "Epoch [5/10], Loss: 0.1819\n",
      "Epoch [6/10], Loss: 0.1801\n",
      "Epoch [7/10], Loss: 0.1787\n",
      "Epoch [8/10], Loss: 0.1771\n",
      "Epoch [9/10], Loss: 0.1757\n",
      "Epoch [10/10], Loss: 0.1746\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.3_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3465\n",
      "Epoch [2/10], Loss: 0.2492\n",
      "Epoch [3/10], Loss: 0.2398\n",
      "Epoch [4/10], Loss: 0.2358\n",
      "Epoch [5/10], Loss: 0.2332\n",
      "Epoch [6/10], Loss: 0.2307\n",
      "Epoch [7/10], Loss: 0.2292\n",
      "Epoch [8/10], Loss: 0.2273\n",
      "Epoch [9/10], Loss: 0.2255\n",
      "Epoch [10/10], Loss: 0.2239\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.3_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3407\n",
      "Epoch [2/10], Loss: 0.2502\n",
      "Epoch [3/10], Loss: 0.2414\n",
      "Epoch [4/10], Loss: 0.2370\n",
      "Epoch [5/10], Loss: 0.2342\n",
      "Epoch [6/10], Loss: 0.2317\n",
      "Epoch [7/10], Loss: 0.2300\n",
      "Epoch [8/10], Loss: 0.2283\n",
      "Epoch [9/10], Loss: 0.2266\n",
      "Epoch [10/10], Loss: 0.2247\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.3_alpha0.01_beta0.05.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.3_alpha0.01_beta0.05.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.2817\n",
      "Epoch [2/10], Loss: 0.2432\n",
      "Epoch [3/10], Loss: 0.2392\n",
      "Epoch [4/10], Loss: 0.2371\n",
      "Epoch [5/10], Loss: 0.2360\n",
      "Epoch [6/10], Loss: 0.2348\n",
      "Epoch [7/10], Loss: 0.2334\n",
      "Epoch [8/10], Loss: 0.2319\n",
      "Epoch [9/10], Loss: 0.2302\n",
      "Epoch [10/10], Loss: 0.2286\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.3_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.5253\n",
      "Epoch [2/10], Loss: 0.4291\n",
      "Epoch [3/10], Loss: 0.4139\n",
      "Epoch [4/10], Loss: 0.4075\n",
      "Epoch [5/10], Loss: 0.4032\n",
      "Epoch [6/10], Loss: 0.4004\n",
      "Epoch [7/10], Loss: 0.3974\n",
      "Epoch [8/10], Loss: 0.3947\n",
      "Epoch [9/10], Loss: 0.3920\n",
      "Epoch [10/10], Loss: 0.3888\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.3_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3024\n",
      "Epoch [2/10], Loss: 0.2338\n",
      "Epoch [3/10], Loss: 0.2259\n",
      "Epoch [4/10], Loss: 0.2222\n",
      "Epoch [5/10], Loss: 0.2201\n",
      "Epoch [6/10], Loss: 0.2181\n",
      "Epoch [7/10], Loss: 0.2169\n",
      "Epoch [8/10], Loss: 0.2152\n",
      "Epoch [9/10], Loss: 0.2133\n",
      "Epoch [10/10], Loss: 0.2120\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.3_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3619\n",
      "Epoch [2/10], Loss: 0.2861\n",
      "Epoch [3/10], Loss: 0.2764\n",
      "Epoch [4/10], Loss: 0.2723\n",
      "Epoch [5/10], Loss: 0.2698\n",
      "Epoch [6/10], Loss: 0.2674\n",
      "Epoch [7/10], Loss: 0.2662\n",
      "Epoch [8/10], Loss: 0.2639\n",
      "Epoch [9/10], Loss: 0.2619\n",
      "Epoch [10/10], Loss: 0.2600\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.3_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3591\n",
      "Epoch [2/10], Loss: 0.2866\n",
      "Epoch [3/10], Loss: 0.2776\n",
      "Epoch [4/10], Loss: 0.2733\n",
      "Epoch [5/10], Loss: 0.2708\n",
      "Epoch [6/10], Loss: 0.2683\n",
      "Epoch [7/10], Loss: 0.2669\n",
      "Epoch [8/10], Loss: 0.2648\n",
      "Epoch [9/10], Loss: 0.2628\n",
      "Epoch [10/10], Loss: 0.2602\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.3_alpha0.05_beta0.01.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.3_alpha0.05_beta0.01.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3153\n",
      "Epoch [2/10], Loss: 0.2828\n",
      "Epoch [3/10], Loss: 0.2790\n",
      "Epoch [4/10], Loss: 0.2768\n",
      "Epoch [5/10], Loss: 0.2756\n",
      "Epoch [6/10], Loss: 0.2740\n",
      "Epoch [7/10], Loss: 0.2724\n",
      "Epoch [8/10], Loss: 0.2703\n",
      "Epoch [9/10], Loss: 0.2685\n",
      "Epoch [10/10], Loss: 0.2661\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.3_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.6141\n",
      "Epoch [2/10], Loss: 0.4417\n",
      "Epoch [3/10], Loss: 0.4244\n",
      "Epoch [4/10], Loss: 0.4174\n",
      "Epoch [5/10], Loss: 0.4122\n",
      "Epoch [6/10], Loss: 0.4092\n",
      "Epoch [7/10], Loss: 0.4053\n",
      "Epoch [8/10], Loss: 0.4028\n",
      "Epoch [9/10], Loss: 0.4005\n",
      "Epoch [10/10], Loss: 0.3972\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.3_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3623\n",
      "Epoch [2/10], Loss: 0.2444\n",
      "Epoch [3/10], Loss: 0.2351\n",
      "Epoch [4/10], Loss: 0.2304\n",
      "Epoch [5/10], Loss: 0.2274\n",
      "Epoch [6/10], Loss: 0.2251\n",
      "Epoch [7/10], Loss: 0.2235\n",
      "Epoch [8/10], Loss: 0.2215\n",
      "Epoch [9/10], Loss: 0.2197\n",
      "Epoch [10/10], Loss: 0.2183\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.3_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4266\n",
      "Epoch [2/10], Loss: 0.2966\n",
      "Epoch [3/10], Loss: 0.2860\n",
      "Epoch [4/10], Loss: 0.2805\n",
      "Epoch [5/10], Loss: 0.2773\n",
      "Epoch [6/10], Loss: 0.2742\n",
      "Epoch [7/10], Loss: 0.2723\n",
      "Epoch [8/10], Loss: 0.2701\n",
      "Epoch [9/10], Loss: 0.2676\n",
      "Epoch [10/10], Loss: 0.2657\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.3_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4220\n",
      "Epoch [2/10], Loss: 0.2981\n",
      "Epoch [3/10], Loss: 0.2871\n",
      "Epoch [4/10], Loss: 0.2815\n",
      "Epoch [5/10], Loss: 0.2786\n",
      "Epoch [6/10], Loss: 0.2752\n",
      "Epoch [7/10], Loss: 0.2732\n",
      "Epoch [8/10], Loss: 0.2711\n",
      "Epoch [9/10], Loss: 0.2689\n",
      "Epoch [10/10], Loss: 0.2667\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.3_alpha0.05_beta0.05.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.3_alpha0.05_beta0.05.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3389\n",
      "Epoch [2/10], Loss: 0.2889\n",
      "Epoch [3/10], Loss: 0.2839\n",
      "Epoch [4/10], Loss: 0.2816\n",
      "Epoch [5/10], Loss: 0.2802\n",
      "Epoch [6/10], Loss: 0.2786\n",
      "Epoch [7/10], Loss: 0.2769\n",
      "Epoch [8/10], Loss: 0.2751\n",
      "Epoch [9/10], Loss: 0.2733\n",
      "Epoch [10/10], Loss: 0.2713\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.3_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4519\n",
      "Epoch [2/10], Loss: 0.3896\n",
      "Epoch [3/10], Loss: 0.3726\n",
      "Epoch [4/10], Loss: 0.3643\n",
      "Epoch [5/10], Loss: 0.3605\n",
      "Epoch [6/10], Loss: 0.3585\n",
      "Epoch [7/10], Loss: 0.3567\n",
      "Epoch [8/10], Loss: 0.3548\n",
      "Epoch [9/10], Loss: 0.3534\n",
      "Epoch [10/10], Loss: 0.3516\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.5_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.2472\n",
      "Epoch [2/10], Loss: 0.1939\n",
      "Epoch [3/10], Loss: 0.1841\n",
      "Epoch [4/10], Loss: 0.1810\n",
      "Epoch [5/10], Loss: 0.1796\n",
      "Epoch [6/10], Loss: 0.1784\n",
      "Epoch [7/10], Loss: 0.1778\n",
      "Epoch [8/10], Loss: 0.1766\n",
      "Epoch [9/10], Loss: 0.1756\n",
      "Epoch [10/10], Loss: 0.1750\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.5_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3045\n",
      "Epoch [2/10], Loss: 0.2498\n",
      "Epoch [3/10], Loss: 0.2364\n",
      "Epoch [4/10], Loss: 0.2320\n",
      "Epoch [5/10], Loss: 0.2300\n",
      "Epoch [6/10], Loss: 0.2289\n",
      "Epoch [7/10], Loss: 0.2279\n",
      "Epoch [8/10], Loss: 0.2270\n",
      "Epoch [9/10], Loss: 0.2259\n",
      "Epoch [10/10], Loss: 0.2250\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.5_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3032\n",
      "Epoch [2/10], Loss: 0.2492\n",
      "Epoch [3/10], Loss: 0.2382\n",
      "Epoch [4/10], Loss: 0.2338\n",
      "Epoch [5/10], Loss: 0.2323\n",
      "Epoch [6/10], Loss: 0.2310\n",
      "Epoch [7/10], Loss: 0.2300\n",
      "Epoch [8/10], Loss: 0.2288\n",
      "Epoch [9/10], Loss: 0.2277\n",
      "Epoch [10/10], Loss: 0.2265\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.5_alpha0.01_beta0.01.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.5_alpha0.01_beta0.01.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.2705\n",
      "Epoch [2/10], Loss: 0.2413\n",
      "Epoch [3/10], Loss: 0.2390\n",
      "Epoch [4/10], Loss: 0.2376\n",
      "Epoch [5/10], Loss: 0.2368\n",
      "Epoch [6/10], Loss: 0.2358\n",
      "Epoch [7/10], Loss: 0.2350\n",
      "Epoch [8/10], Loss: 0.2340\n",
      "Epoch [9/10], Loss: 0.2329\n",
      "Epoch [10/10], Loss: 0.2317\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.5_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.5604\n",
      "Epoch [2/10], Loss: 0.4053\n",
      "Epoch [3/10], Loss: 0.3838\n",
      "Epoch [4/10], Loss: 0.3762\n",
      "Epoch [5/10], Loss: 0.3717\n",
      "Epoch [6/10], Loss: 0.3695\n",
      "Epoch [7/10], Loss: 0.3666\n",
      "Epoch [8/10], Loss: 0.3651\n",
      "Epoch [9/10], Loss: 0.3636\n",
      "Epoch [10/10], Loss: 0.3618\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.5_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3156\n",
      "Epoch [2/10], Loss: 0.2076\n",
      "Epoch [3/10], Loss: 0.1967\n",
      "Epoch [4/10], Loss: 0.1920\n",
      "Epoch [5/10], Loss: 0.1895\n",
      "Epoch [6/10], Loss: 0.1879\n",
      "Epoch [7/10], Loss: 0.1866\n",
      "Epoch [8/10], Loss: 0.1855\n",
      "Epoch [9/10], Loss: 0.1841\n",
      "Epoch [10/10], Loss: 0.1837\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.5_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3842\n",
      "Epoch [2/10], Loss: 0.2620\n",
      "Epoch [3/10], Loss: 0.2491\n",
      "Epoch [4/10], Loss: 0.2442\n",
      "Epoch [5/10], Loss: 0.2409\n",
      "Epoch [6/10], Loss: 0.2388\n",
      "Epoch [7/10], Loss: 0.2372\n",
      "Epoch [8/10], Loss: 0.2362\n",
      "Epoch [9/10], Loss: 0.2348\n",
      "Epoch [10/10], Loss: 0.2336\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.5_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3749\n",
      "Epoch [2/10], Loss: 0.2624\n",
      "Epoch [3/10], Loss: 0.2505\n",
      "Epoch [4/10], Loss: 0.2444\n",
      "Epoch [5/10], Loss: 0.2416\n",
      "Epoch [6/10], Loss: 0.2396\n",
      "Epoch [7/10], Loss: 0.2385\n",
      "Epoch [8/10], Loss: 0.2371\n",
      "Epoch [9/10], Loss: 0.2358\n",
      "Epoch [10/10], Loss: 0.2347\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.5_alpha0.01_beta0.05.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.5_alpha0.01_beta0.05.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.2987\n",
      "Epoch [2/10], Loss: 0.2509\n",
      "Epoch [3/10], Loss: 0.2472\n",
      "Epoch [4/10], Loss: 0.2451\n",
      "Epoch [5/10], Loss: 0.2441\n",
      "Epoch [6/10], Loss: 0.2431\n",
      "Epoch [7/10], Loss: 0.2419\n",
      "Epoch [8/10], Loss: 0.2408\n",
      "Epoch [9/10], Loss: 0.2397\n",
      "Epoch [10/10], Loss: 0.2383\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.5_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.5607\n",
      "Epoch [2/10], Loss: 0.4462\n",
      "Epoch [3/10], Loss: 0.4237\n",
      "Epoch [4/10], Loss: 0.4162\n",
      "Epoch [5/10], Loss: 0.4125\n",
      "Epoch [6/10], Loss: 0.4103\n",
      "Epoch [7/10], Loss: 0.4073\n",
      "Epoch [8/10], Loss: 0.4056\n",
      "Epoch [9/10], Loss: 0.4041\n",
      "Epoch [10/10], Loss: 0.4017\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.5_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3310\n",
      "Epoch [2/10], Loss: 0.2434\n",
      "Epoch [3/10], Loss: 0.2329\n",
      "Epoch [4/10], Loss: 0.2293\n",
      "Epoch [5/10], Loss: 0.2270\n",
      "Epoch [6/10], Loss: 0.2252\n",
      "Epoch [7/10], Loss: 0.2244\n",
      "Epoch [8/10], Loss: 0.2230\n",
      "Epoch [9/10], Loss: 0.2215\n",
      "Epoch [10/10], Loss: 0.2211\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.5_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3938\n",
      "Epoch [2/10], Loss: 0.2976\n",
      "Epoch [3/10], Loss: 0.2839\n",
      "Epoch [4/10], Loss: 0.2796\n",
      "Epoch [5/10], Loss: 0.2768\n",
      "Epoch [6/10], Loss: 0.2752\n",
      "Epoch [7/10], Loss: 0.2739\n",
      "Epoch [8/10], Loss: 0.2723\n",
      "Epoch [9/10], Loss: 0.2711\n",
      "Epoch [10/10], Loss: 0.2697\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.5_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3877\n",
      "Epoch [2/10], Loss: 0.2968\n",
      "Epoch [3/10], Loss: 0.2854\n",
      "Epoch [4/10], Loss: 0.2802\n",
      "Epoch [5/10], Loss: 0.2781\n",
      "Epoch [6/10], Loss: 0.2761\n",
      "Epoch [7/10], Loss: 0.2750\n",
      "Epoch [8/10], Loss: 0.2735\n",
      "Epoch [9/10], Loss: 0.2720\n",
      "Epoch [10/10], Loss: 0.2706\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.5_alpha0.05_beta0.01.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.5_alpha0.05_beta0.01.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3297\n",
      "Epoch [2/10], Loss: 0.2891\n",
      "Epoch [3/10], Loss: 0.2858\n",
      "Epoch [4/10], Loss: 0.2839\n",
      "Epoch [5/10], Loss: 0.2827\n",
      "Epoch [6/10], Loss: 0.2814\n",
      "Epoch [7/10], Loss: 0.2801\n",
      "Epoch [8/10], Loss: 0.2787\n",
      "Epoch [9/10], Loss: 0.2774\n",
      "Epoch [10/10], Loss: 0.2757\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.5_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.6664\n",
      "Epoch [2/10], Loss: 0.4664\n",
      "Epoch [3/10], Loss: 0.4417\n",
      "Epoch [4/10], Loss: 0.4316\n",
      "Epoch [5/10], Loss: 0.4264\n",
      "Epoch [6/10], Loss: 0.4229\n",
      "Epoch [7/10], Loss: 0.4193\n",
      "Epoch [8/10], Loss: 0.4174\n",
      "Epoch [9/10], Loss: 0.4161\n",
      "Epoch [10/10], Loss: 0.4139\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.5_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4021\n",
      "Epoch [2/10], Loss: 0.2612\n",
      "Epoch [3/10], Loss: 0.2467\n",
      "Epoch [4/10], Loss: 0.2398\n",
      "Epoch [5/10], Loss: 0.2363\n",
      "Epoch [6/10], Loss: 0.2344\n",
      "Epoch [7/10], Loss: 0.2335\n",
      "Epoch [8/10], Loss: 0.2316\n",
      "Epoch [9/10], Loss: 0.2301\n",
      "Epoch [10/10], Loss: 0.2298\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.5_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4698\n",
      "Epoch [2/10], Loss: 0.3149\n",
      "Epoch [3/10], Loss: 0.2999\n",
      "Epoch [4/10], Loss: 0.2926\n",
      "Epoch [5/10], Loss: 0.2882\n",
      "Epoch [6/10], Loss: 0.2854\n",
      "Epoch [7/10], Loss: 0.2834\n",
      "Epoch [8/10], Loss: 0.2814\n",
      "Epoch [9/10], Loss: 0.2796\n",
      "Epoch [10/10], Loss: 0.2783\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.5_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4625\n",
      "Epoch [2/10], Loss: 0.3149\n",
      "Epoch [3/10], Loss: 0.3000\n",
      "Epoch [4/10], Loss: 0.2923\n",
      "Epoch [5/10], Loss: 0.2885\n",
      "Epoch [6/10], Loss: 0.2858\n",
      "Epoch [7/10], Loss: 0.2842\n",
      "Epoch [8/10], Loss: 0.2825\n",
      "Epoch [9/10], Loss: 0.2808\n",
      "Epoch [10/10], Loss: 0.2796\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.5_alpha0.05_beta0.05.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.5_alpha0.05_beta0.05.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3618\n",
      "Epoch [2/10], Loss: 0.3012\n",
      "Epoch [3/10], Loss: 0.2961\n",
      "Epoch [4/10], Loss: 0.2931\n",
      "Epoch [5/10], Loss: 0.2914\n",
      "Epoch [6/10], Loss: 0.2894\n",
      "Epoch [7/10], Loss: 0.2877\n",
      "Epoch [8/10], Loss: 0.2861\n",
      "Epoch [9/10], Loss: 0.2846\n",
      "Epoch [10/10], Loss: 0.2831\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim64_dropout0.5_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4193\n",
      "Epoch [2/10], Loss: 0.3700\n",
      "Epoch [3/10], Loss: 0.3600\n",
      "Epoch [4/10], Loss: 0.3546\n",
      "Epoch [5/10], Loss: 0.3509\n",
      "Epoch [6/10], Loss: 0.3482\n",
      "Epoch [7/10], Loss: 0.3441\n",
      "Epoch [8/10], Loss: 0.3402\n",
      "Epoch [9/10], Loss: 0.3355\n",
      "Epoch [10/10], Loss: 0.3306\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.3_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.2196\n",
      "Epoch [2/10], Loss: 0.1831\n",
      "Epoch [3/10], Loss: 0.1780\n",
      "Epoch [4/10], Loss: 0.1754\n",
      "Epoch [5/10], Loss: 0.1738\n",
      "Epoch [6/10], Loss: 0.1724\n",
      "Epoch [7/10], Loss: 0.1706\n",
      "Epoch [8/10], Loss: 0.1694\n",
      "Epoch [9/10], Loss: 0.1676\n",
      "Epoch [10/10], Loss: 0.1653\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.3_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.2738\n",
      "Epoch [2/10], Loss: 0.2350\n",
      "Epoch [3/10], Loss: 0.2288\n",
      "Epoch [4/10], Loss: 0.2257\n",
      "Epoch [5/10], Loss: 0.2234\n",
      "Epoch [6/10], Loss: 0.2214\n",
      "Epoch [7/10], Loss: 0.2193\n",
      "Epoch [8/10], Loss: 0.2168\n",
      "Epoch [9/10], Loss: 0.2138\n",
      "Epoch [10/10], Loss: 0.2105\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.3_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.2737\n",
      "Epoch [2/10], Loss: 0.2358\n",
      "Epoch [3/10], Loss: 0.2303\n",
      "Epoch [4/10], Loss: 0.2273\n",
      "Epoch [5/10], Loss: 0.2252\n",
      "Epoch [6/10], Loss: 0.2230\n",
      "Epoch [7/10], Loss: 0.2213\n",
      "Epoch [8/10], Loss: 0.2187\n",
      "Epoch [9/10], Loss: 0.2158\n",
      "Epoch [10/10], Loss: 0.2126\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.3_alpha0.01_beta0.01.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.3_alpha0.01_beta0.01.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.2551\n",
      "Epoch [2/10], Loss: 0.2369\n",
      "Epoch [3/10], Loss: 0.2344\n",
      "Epoch [4/10], Loss: 0.2329\n",
      "Epoch [5/10], Loss: 0.2316\n",
      "Epoch [6/10], Loss: 0.2299\n",
      "Epoch [7/10], Loss: 0.2278\n",
      "Epoch [8/10], Loss: 0.2250\n",
      "Epoch [9/10], Loss: 0.2219\n",
      "Epoch [10/10], Loss: 0.2182\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.3_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4806\n",
      "Epoch [2/10], Loss: 0.3808\n",
      "Epoch [3/10], Loss: 0.3699\n",
      "Epoch [4/10], Loss: 0.3638\n",
      "Epoch [5/10], Loss: 0.3592\n",
      "Epoch [6/10], Loss: 0.3556\n",
      "Epoch [7/10], Loss: 0.3513\n",
      "Epoch [8/10], Loss: 0.3473\n",
      "Epoch [9/10], Loss: 0.3429\n",
      "Epoch [10/10], Loss: 0.3387\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.3_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.2595\n",
      "Epoch [2/10], Loss: 0.1928\n",
      "Epoch [3/10], Loss: 0.1862\n",
      "Epoch [4/10], Loss: 0.1823\n",
      "Epoch [5/10], Loss: 0.1800\n",
      "Epoch [6/10], Loss: 0.1780\n",
      "Epoch [7/10], Loss: 0.1759\n",
      "Epoch [8/10], Loss: 0.1745\n",
      "Epoch [9/10], Loss: 0.1728\n",
      "Epoch [10/10], Loss: 0.1706\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.3_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3191\n",
      "Epoch [2/10], Loss: 0.2452\n",
      "Epoch [3/10], Loss: 0.2380\n",
      "Epoch [4/10], Loss: 0.2338\n",
      "Epoch [5/10], Loss: 0.2306\n",
      "Epoch [6/10], Loss: 0.2282\n",
      "Epoch [7/10], Loss: 0.2256\n",
      "Epoch [8/10], Loss: 0.2228\n",
      "Epoch [9/10], Loss: 0.2205\n",
      "Epoch [10/10], Loss: 0.2171\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.3_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3166\n",
      "Epoch [2/10], Loss: 0.2459\n",
      "Epoch [3/10], Loss: 0.2388\n",
      "Epoch [4/10], Loss: 0.2346\n",
      "Epoch [5/10], Loss: 0.2318\n",
      "Epoch [6/10], Loss: 0.2295\n",
      "Epoch [7/10], Loss: 0.2273\n",
      "Epoch [8/10], Loss: 0.2250\n",
      "Epoch [9/10], Loss: 0.2225\n",
      "Epoch [10/10], Loss: 0.2198\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.3_alpha0.01_beta0.05.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.3_alpha0.01_beta0.05.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.2734\n",
      "Epoch [2/10], Loss: 0.2430\n",
      "Epoch [3/10], Loss: 0.2395\n",
      "Epoch [4/10], Loss: 0.2374\n",
      "Epoch [5/10], Loss: 0.2359\n",
      "Epoch [6/10], Loss: 0.2341\n",
      "Epoch [7/10], Loss: 0.2320\n",
      "Epoch [8/10], Loss: 0.2294\n",
      "Epoch [9/10], Loss: 0.2264\n",
      "Epoch [10/10], Loss: 0.2230\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.3_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.5006\n",
      "Epoch [2/10], Loss: 0.4218\n",
      "Epoch [3/10], Loss: 0.4106\n",
      "Epoch [4/10], Loss: 0.4046\n",
      "Epoch [5/10], Loss: 0.4003\n",
      "Epoch [6/10], Loss: 0.3964\n",
      "Epoch [7/10], Loss: 0.3918\n",
      "Epoch [8/10], Loss: 0.3878\n",
      "Epoch [9/10], Loss: 0.3824\n",
      "Epoch [10/10], Loss: 0.3777\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.3_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.2863\n",
      "Epoch [2/10], Loss: 0.2305\n",
      "Epoch [3/10], Loss: 0.2242\n",
      "Epoch [4/10], Loss: 0.2209\n",
      "Epoch [5/10], Loss: 0.2184\n",
      "Epoch [6/10], Loss: 0.2162\n",
      "Epoch [7/10], Loss: 0.2140\n",
      "Epoch [8/10], Loss: 0.2122\n",
      "Epoch [9/10], Loss: 0.2100\n",
      "Epoch [10/10], Loss: 0.2074\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.3_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3417\n",
      "Epoch [2/10], Loss: 0.2815\n",
      "Epoch [3/10], Loss: 0.2744\n",
      "Epoch [4/10], Loss: 0.2703\n",
      "Epoch [5/10], Loss: 0.2674\n",
      "Epoch [6/10], Loss: 0.2643\n",
      "Epoch [7/10], Loss: 0.2618\n",
      "Epoch [8/10], Loss: 0.2588\n",
      "Epoch [9/10], Loss: 0.2555\n",
      "Epoch [10/10], Loss: 0.2519\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.3_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3406\n",
      "Epoch [2/10], Loss: 0.2821\n",
      "Epoch [3/10], Loss: 0.2750\n",
      "Epoch [4/10], Loss: 0.2707\n",
      "Epoch [5/10], Loss: 0.2682\n",
      "Epoch [6/10], Loss: 0.2654\n",
      "Epoch [7/10], Loss: 0.2627\n",
      "Epoch [8/10], Loss: 0.2600\n",
      "Epoch [9/10], Loss: 0.2567\n",
      "Epoch [10/10], Loss: 0.2530\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.3_alpha0.05_beta0.01.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.3_alpha0.05_beta0.01.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3085\n",
      "Epoch [2/10], Loss: 0.2824\n",
      "Epoch [3/10], Loss: 0.2789\n",
      "Epoch [4/10], Loss: 0.2769\n",
      "Epoch [5/10], Loss: 0.2749\n",
      "Epoch [6/10], Loss: 0.2726\n",
      "Epoch [7/10], Loss: 0.2702\n",
      "Epoch [8/10], Loss: 0.2667\n",
      "Epoch [9/10], Loss: 0.2628\n",
      "Epoch [10/10], Loss: 0.2585\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.3_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.5684\n",
      "Epoch [2/10], Loss: 0.4346\n",
      "Epoch [3/10], Loss: 0.4219\n",
      "Epoch [4/10], Loss: 0.4145\n",
      "Epoch [5/10], Loss: 0.4097\n",
      "Epoch [6/10], Loss: 0.4053\n",
      "Epoch [7/10], Loss: 0.4007\n",
      "Epoch [8/10], Loss: 0.3964\n",
      "Epoch [9/10], Loss: 0.3916\n",
      "Epoch [10/10], Loss: 0.3871\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.3_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3311\n",
      "Epoch [2/10], Loss: 0.2408\n",
      "Epoch [3/10], Loss: 0.2327\n",
      "Epoch [4/10], Loss: 0.2279\n",
      "Epoch [5/10], Loss: 0.2251\n",
      "Epoch [6/10], Loss: 0.2222\n",
      "Epoch [7/10], Loss: 0.2201\n",
      "Epoch [8/10], Loss: 0.2183\n",
      "Epoch [9/10], Loss: 0.2160\n",
      "Epoch [10/10], Loss: 0.2135\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.3_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3942\n",
      "Epoch [2/10], Loss: 0.2928\n",
      "Epoch [3/10], Loss: 0.2840\n",
      "Epoch [4/10], Loss: 0.2784\n",
      "Epoch [5/10], Loss: 0.2747\n",
      "Epoch [6/10], Loss: 0.2712\n",
      "Epoch [7/10], Loss: 0.2682\n",
      "Epoch [8/10], Loss: 0.2649\n",
      "Epoch [9/10], Loss: 0.2622\n",
      "Epoch [10/10], Loss: 0.2583\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.3_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3884\n",
      "Epoch [2/10], Loss: 0.2913\n",
      "Epoch [3/10], Loss: 0.2822\n",
      "Epoch [4/10], Loss: 0.2771\n",
      "Epoch [5/10], Loss: 0.2743\n",
      "Epoch [6/10], Loss: 0.2710\n",
      "Epoch [7/10], Loss: 0.2686\n",
      "Epoch [8/10], Loss: 0.2662\n",
      "Epoch [9/10], Loss: 0.2632\n",
      "Epoch [10/10], Loss: 0.2599\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.3_alpha0.05_beta0.05.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.3_alpha0.05_beta0.05.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3290\n",
      "Epoch [2/10], Loss: 0.2890\n",
      "Epoch [3/10], Loss: 0.2842\n",
      "Epoch [4/10], Loss: 0.2817\n",
      "Epoch [5/10], Loss: 0.2798\n",
      "Epoch [6/10], Loss: 0.2773\n",
      "Epoch [7/10], Loss: 0.2750\n",
      "Epoch [8/10], Loss: 0.2718\n",
      "Epoch [9/10], Loss: 0.2686\n",
      "Epoch [10/10], Loss: 0.2648\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.3_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4434\n",
      "Epoch [2/10], Loss: 0.3806\n",
      "Epoch [3/10], Loss: 0.3671\n",
      "Epoch [4/10], Loss: 0.3614\n",
      "Epoch [5/10], Loss: 0.3587\n",
      "Epoch [6/10], Loss: 0.3568\n",
      "Epoch [7/10], Loss: 0.3543\n",
      "Epoch [8/10], Loss: 0.3524\n",
      "Epoch [9/10], Loss: 0.3499\n",
      "Epoch [10/10], Loss: 0.3467\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.5_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.2383\n",
      "Epoch [2/10], Loss: 0.1889\n",
      "Epoch [3/10], Loss: 0.1831\n",
      "Epoch [4/10], Loss: 0.1805\n",
      "Epoch [5/10], Loss: 0.1791\n",
      "Epoch [6/10], Loss: 0.1779\n",
      "Epoch [7/10], Loss: 0.1769\n",
      "Epoch [8/10], Loss: 0.1760\n",
      "Epoch [9/10], Loss: 0.1751\n",
      "Epoch [10/10], Loss: 0.1734\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.5_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.2930\n",
      "Epoch [2/10], Loss: 0.2420\n",
      "Epoch [3/10], Loss: 0.2340\n",
      "Epoch [4/10], Loss: 0.2314\n",
      "Epoch [5/10], Loss: 0.2296\n",
      "Epoch [6/10], Loss: 0.2279\n",
      "Epoch [7/10], Loss: 0.2266\n",
      "Epoch [8/10], Loss: 0.2252\n",
      "Epoch [9/10], Loss: 0.2233\n",
      "Epoch [10/10], Loss: 0.2216\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.5_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.2921\n",
      "Epoch [2/10], Loss: 0.2422\n",
      "Epoch [3/10], Loss: 0.2356\n",
      "Epoch [4/10], Loss: 0.2330\n",
      "Epoch [5/10], Loss: 0.2315\n",
      "Epoch [6/10], Loss: 0.2299\n",
      "Epoch [7/10], Loss: 0.2286\n",
      "Epoch [8/10], Loss: 0.2273\n",
      "Epoch [9/10], Loss: 0.2257\n",
      "Epoch [10/10], Loss: 0.2235\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.5_alpha0.01_beta0.01.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.5_alpha0.01_beta0.01.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.2640\n",
      "Epoch [2/10], Loss: 0.2412\n",
      "Epoch [3/10], Loss: 0.2390\n",
      "Epoch [4/10], Loss: 0.2376\n",
      "Epoch [5/10], Loss: 0.2366\n",
      "Epoch [6/10], Loss: 0.2354\n",
      "Epoch [7/10], Loss: 0.2342\n",
      "Epoch [8/10], Loss: 0.2326\n",
      "Epoch [9/10], Loss: 0.2305\n",
      "Epoch [10/10], Loss: 0.2281\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.5_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.5203\n",
      "Epoch [2/10], Loss: 0.3966\n",
      "Epoch [3/10], Loss: 0.3824\n",
      "Epoch [4/10], Loss: 0.3757\n",
      "Epoch [5/10], Loss: 0.3718\n",
      "Epoch [6/10], Loss: 0.3689\n",
      "Epoch [7/10], Loss: 0.3656\n",
      "Epoch [8/10], Loss: 0.3635\n",
      "Epoch [9/10], Loss: 0.3607\n",
      "Epoch [10/10], Loss: 0.3573\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.5_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.2887\n",
      "Epoch [2/10], Loss: 0.2041\n",
      "Epoch [3/10], Loss: 0.1957\n",
      "Epoch [4/10], Loss: 0.1912\n",
      "Epoch [5/10], Loss: 0.1890\n",
      "Epoch [6/10], Loss: 0.1873\n",
      "Epoch [7/10], Loss: 0.1860\n",
      "Epoch [8/10], Loss: 0.1847\n",
      "Epoch [9/10], Loss: 0.1837\n",
      "Epoch [10/10], Loss: 0.1816\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.5_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3512\n",
      "Epoch [2/10], Loss: 0.2580\n",
      "Epoch [3/10], Loss: 0.2485\n",
      "Epoch [4/10], Loss: 0.2438\n",
      "Epoch [5/10], Loss: 0.2409\n",
      "Epoch [6/10], Loss: 0.2381\n",
      "Epoch [7/10], Loss: 0.2362\n",
      "Epoch [8/10], Loss: 0.2344\n",
      "Epoch [9/10], Loss: 0.2324\n",
      "Epoch [10/10], Loss: 0.2306\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.5_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3462\n",
      "Epoch [2/10], Loss: 0.2575\n",
      "Epoch [3/10], Loss: 0.2488\n",
      "Epoch [4/10], Loss: 0.2444\n",
      "Epoch [5/10], Loss: 0.2417\n",
      "Epoch [6/10], Loss: 0.2394\n",
      "Epoch [7/10], Loss: 0.2379\n",
      "Epoch [8/10], Loss: 0.2363\n",
      "Epoch [9/10], Loss: 0.2347\n",
      "Epoch [10/10], Loss: 0.2325\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.5_alpha0.01_beta0.05.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.5_alpha0.01_beta0.05.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.2888\n",
      "Epoch [2/10], Loss: 0.2517\n",
      "Epoch [3/10], Loss: 0.2476\n",
      "Epoch [4/10], Loss: 0.2454\n",
      "Epoch [5/10], Loss: 0.2442\n",
      "Epoch [6/10], Loss: 0.2431\n",
      "Epoch [7/10], Loss: 0.2419\n",
      "Epoch [8/10], Loss: 0.2401\n",
      "Epoch [9/10], Loss: 0.2378\n",
      "Epoch [10/10], Loss: 0.2353\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.5_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.5339\n",
      "Epoch [2/10], Loss: 0.4360\n",
      "Epoch [3/10], Loss: 0.4209\n",
      "Epoch [4/10], Loss: 0.4148\n",
      "Epoch [5/10], Loss: 0.4116\n",
      "Epoch [6/10], Loss: 0.4087\n",
      "Epoch [7/10], Loss: 0.4054\n",
      "Epoch [8/10], Loss: 0.4030\n",
      "Epoch [9/10], Loss: 0.3997\n",
      "Epoch [10/10], Loss: 0.3958\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.5_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3109\n",
      "Epoch [2/10], Loss: 0.2395\n",
      "Epoch [3/10], Loss: 0.2319\n",
      "Epoch [4/10], Loss: 0.2285\n",
      "Epoch [5/10], Loss: 0.2262\n",
      "Epoch [6/10], Loss: 0.2248\n",
      "Epoch [7/10], Loss: 0.2231\n",
      "Epoch [8/10], Loss: 0.2219\n",
      "Epoch [9/10], Loss: 0.2207\n",
      "Epoch [10/10], Loss: 0.2184\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.5_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3679\n",
      "Epoch [2/10], Loss: 0.2916\n",
      "Epoch [3/10], Loss: 0.2827\n",
      "Epoch [4/10], Loss: 0.2785\n",
      "Epoch [5/10], Loss: 0.2761\n",
      "Epoch [6/10], Loss: 0.2737\n",
      "Epoch [7/10], Loss: 0.2719\n",
      "Epoch [8/10], Loss: 0.2699\n",
      "Epoch [9/10], Loss: 0.2677\n",
      "Epoch [10/10], Loss: 0.2654\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.5_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3652\n",
      "Epoch [2/10], Loss: 0.2909\n",
      "Epoch [3/10], Loss: 0.2829\n",
      "Epoch [4/10], Loss: 0.2791\n",
      "Epoch [5/10], Loss: 0.2771\n",
      "Epoch [6/10], Loss: 0.2750\n",
      "Epoch [7/10], Loss: 0.2731\n",
      "Epoch [8/10], Loss: 0.2717\n",
      "Epoch [9/10], Loss: 0.2692\n",
      "Epoch [10/10], Loss: 0.2665\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.5_alpha0.05_beta0.01.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.5_alpha0.05_beta0.01.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3211\n",
      "Epoch [2/10], Loss: 0.2892\n",
      "Epoch [3/10], Loss: 0.2856\n",
      "Epoch [4/10], Loss: 0.2838\n",
      "Epoch [5/10], Loss: 0.2823\n",
      "Epoch [6/10], Loss: 0.2807\n",
      "Epoch [7/10], Loss: 0.2791\n",
      "Epoch [8/10], Loss: 0.2765\n",
      "Epoch [9/10], Loss: 0.2736\n",
      "Epoch [10/10], Loss: 0.2702\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.5_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.6150\n",
      "Epoch [2/10], Loss: 0.4550\n",
      "Epoch [3/10], Loss: 0.4367\n",
      "Epoch [4/10], Loss: 0.4288\n",
      "Epoch [5/10], Loss: 0.4247\n",
      "Epoch [6/10], Loss: 0.4208\n",
      "Epoch [7/10], Loss: 0.4167\n",
      "Epoch [8/10], Loss: 0.4144\n",
      "Epoch [9/10], Loss: 0.4112\n",
      "Epoch [10/10], Loss: 0.4075\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.5_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3669\n",
      "Epoch [2/10], Loss: 0.2579\n",
      "Epoch [3/10], Loss: 0.2477\n",
      "Epoch [4/10], Loss: 0.2425\n",
      "Epoch [5/10], Loss: 0.2387\n",
      "Epoch [6/10], Loss: 0.2363\n",
      "Epoch [7/10], Loss: 0.2339\n",
      "Epoch [8/10], Loss: 0.2319\n",
      "Epoch [9/10], Loss: 0.2304\n",
      "Epoch [10/10], Loss: 0.2276\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.5_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4330\n",
      "Epoch [2/10], Loss: 0.3092\n",
      "Epoch [3/10], Loss: 0.2978\n",
      "Epoch [4/10], Loss: 0.2917\n",
      "Epoch [5/10], Loss: 0.2881\n",
      "Epoch [6/10], Loss: 0.2848\n",
      "Epoch [7/10], Loss: 0.2822\n",
      "Epoch [8/10], Loss: 0.2798\n",
      "Epoch [9/10], Loss: 0.2777\n",
      "Epoch [10/10], Loss: 0.2748\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.5_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4248\n",
      "Epoch [2/10], Loss: 0.3093\n",
      "Epoch [3/10], Loss: 0.2987\n",
      "Epoch [4/10], Loss: 0.2921\n",
      "Epoch [5/10], Loss: 0.2888\n",
      "Epoch [6/10], Loss: 0.2860\n",
      "Epoch [7/10], Loss: 0.2838\n",
      "Epoch [8/10], Loss: 0.2815\n",
      "Epoch [9/10], Loss: 0.2797\n",
      "Epoch [10/10], Loss: 0.2767\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.5_alpha0.05_beta0.05.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.5_alpha0.05_beta0.05.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3498\n",
      "Epoch [2/10], Loss: 0.3025\n",
      "Epoch [3/10], Loss: 0.2965\n",
      "Epoch [4/10], Loss: 0.2926\n",
      "Epoch [5/10], Loss: 0.2905\n",
      "Epoch [6/10], Loss: 0.2887\n",
      "Epoch [7/10], Loss: 0.2872\n",
      "Epoch [8/10], Loss: 0.2847\n",
      "Epoch [9/10], Loss: 0.2821\n",
      "Epoch [10/10], Loss: 0.2792\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.001_embedding_dim128_dropout0.5_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.5285\n",
      "Epoch [2/10], Loss: 0.4583\n",
      "Epoch [3/10], Loss: 0.4342\n",
      "Epoch [4/10], Loss: 0.4157\n",
      "Epoch [5/10], Loss: 0.4026\n",
      "Epoch [6/10], Loss: 0.3949\n",
      "Epoch [7/10], Loss: 0.3890\n",
      "Epoch [8/10], Loss: 0.3853\n",
      "Epoch [9/10], Loss: 0.3814\n",
      "Epoch [10/10], Loss: 0.3784\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.3_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3177\n",
      "Epoch [2/10], Loss: 0.2531\n",
      "Epoch [3/10], Loss: 0.2259\n",
      "Epoch [4/10], Loss: 0.2123\n",
      "Epoch [5/10], Loss: 0.2047\n",
      "Epoch [6/10], Loss: 0.1991\n",
      "Epoch [7/10], Loss: 0.1950\n",
      "Epoch [8/10], Loss: 0.1916\n",
      "Epoch [9/10], Loss: 0.1887\n",
      "Epoch [10/10], Loss: 0.1860\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.3_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3720\n",
      "Epoch [2/10], Loss: 0.3057\n",
      "Epoch [3/10], Loss: 0.2806\n",
      "Epoch [4/10], Loss: 0.2672\n",
      "Epoch [5/10], Loss: 0.2592\n",
      "Epoch [6/10], Loss: 0.2535\n",
      "Epoch [7/10], Loss: 0.2496\n",
      "Epoch [8/10], Loss: 0.2459\n",
      "Epoch [9/10], Loss: 0.2427\n",
      "Epoch [10/10], Loss: 0.2405\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.3_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3725\n",
      "Epoch [2/10], Loss: 0.3084\n",
      "Epoch [3/10], Loss: 0.2817\n",
      "Epoch [4/10], Loss: 0.2673\n",
      "Epoch [5/10], Loss: 0.2593\n",
      "Epoch [6/10], Loss: 0.2538\n",
      "Epoch [7/10], Loss: 0.2498\n",
      "Epoch [8/10], Loss: 0.2461\n",
      "Epoch [9/10], Loss: 0.2432\n",
      "Epoch [10/10], Loss: 0.2408\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.3_alpha0.01_beta0.01.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.3_alpha0.01_beta0.01.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3266\n",
      "Epoch [2/10], Loss: 0.2708\n",
      "Epoch [3/10], Loss: 0.2561\n",
      "Epoch [4/10], Loss: 0.2477\n",
      "Epoch [5/10], Loss: 0.2429\n",
      "Epoch [6/10], Loss: 0.2399\n",
      "Epoch [7/10], Loss: 0.2379\n",
      "Epoch [8/10], Loss: 0.2364\n",
      "Epoch [9/10], Loss: 0.2354\n",
      "Epoch [10/10], Loss: 0.2347\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.3_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.6821\n",
      "Epoch [2/10], Loss: 0.6047\n",
      "Epoch [3/10], Loss: 0.5636\n",
      "Epoch [4/10], Loss: 0.5256\n",
      "Epoch [5/10], Loss: 0.4938\n",
      "Epoch [6/10], Loss: 0.4679\n",
      "Epoch [7/10], Loss: 0.4476\n",
      "Epoch [8/10], Loss: 0.4334\n",
      "Epoch [9/10], Loss: 0.4201\n",
      "Epoch [10/10], Loss: 0.4103\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.3_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4642\n",
      "Epoch [2/10], Loss: 0.3753\n",
      "Epoch [3/10], Loss: 0.3132\n",
      "Epoch [4/10], Loss: 0.2724\n",
      "Epoch [5/10], Loss: 0.2467\n",
      "Epoch [6/10], Loss: 0.2299\n",
      "Epoch [7/10], Loss: 0.2186\n",
      "Epoch [8/10], Loss: 0.2108\n",
      "Epoch [9/10], Loss: 0.2047\n",
      "Epoch [10/10], Loss: 0.1999\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.3_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.5124\n",
      "Epoch [2/10], Loss: 0.4345\n",
      "Epoch [3/10], Loss: 0.3860\n",
      "Epoch [4/10], Loss: 0.3479\n",
      "Epoch [5/10], Loss: 0.3194\n",
      "Epoch [6/10], Loss: 0.2984\n",
      "Epoch [7/10], Loss: 0.2842\n",
      "Epoch [8/10], Loss: 0.2732\n",
      "Epoch [9/10], Loss: 0.2647\n",
      "Epoch [10/10], Loss: 0.2589\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.3_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.5128\n",
      "Epoch [2/10], Loss: 0.4308\n",
      "Epoch [3/10], Loss: 0.3755\n",
      "Epoch [4/10], Loss: 0.3346\n",
      "Epoch [5/10], Loss: 0.3075\n",
      "Epoch [6/10], Loss: 0.2895\n",
      "Epoch [7/10], Loss: 0.2772\n",
      "Epoch [8/10], Loss: 0.2681\n",
      "Epoch [9/10], Loss: 0.2613\n",
      "Epoch [10/10], Loss: 0.2563\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.3_alpha0.01_beta0.05.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.3_alpha0.01_beta0.05.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4379\n",
      "Epoch [2/10], Loss: 0.3098\n",
      "Epoch [3/10], Loss: 0.2740\n",
      "Epoch [4/10], Loss: 0.2591\n",
      "Epoch [5/10], Loss: 0.2518\n",
      "Epoch [6/10], Loss: 0.2477\n",
      "Epoch [7/10], Loss: 0.2450\n",
      "Epoch [8/10], Loss: 0.2430\n",
      "Epoch [9/10], Loss: 0.2416\n",
      "Epoch [10/10], Loss: 0.2406\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.3_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.6559\n",
      "Epoch [2/10], Loss: 0.5829\n",
      "Epoch [3/10], Loss: 0.5494\n",
      "Epoch [4/10], Loss: 0.5219\n",
      "Epoch [5/10], Loss: 0.5003\n",
      "Epoch [6/10], Loss: 0.4832\n",
      "Epoch [7/10], Loss: 0.4699\n",
      "Epoch [8/10], Loss: 0.4602\n",
      "Epoch [9/10], Loss: 0.4507\n",
      "Epoch [10/10], Loss: 0.4438\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.3_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4408\n",
      "Epoch [2/10], Loss: 0.3637\n",
      "Epoch [3/10], Loss: 0.3187\n",
      "Epoch [4/10], Loss: 0.2904\n",
      "Epoch [5/10], Loss: 0.2725\n",
      "Epoch [6/10], Loss: 0.2605\n",
      "Epoch [7/10], Loss: 0.2519\n",
      "Epoch [8/10], Loss: 0.2457\n",
      "Epoch [9/10], Loss: 0.2408\n",
      "Epoch [10/10], Loss: 0.2365\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.3_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4883\n",
      "Epoch [2/10], Loss: 0.4165\n",
      "Epoch [3/10], Loss: 0.3805\n",
      "Epoch [4/10], Loss: 0.3553\n",
      "Epoch [5/10], Loss: 0.3364\n",
      "Epoch [6/10], Loss: 0.3223\n",
      "Epoch [7/10], Loss: 0.3125\n",
      "Epoch [8/10], Loss: 0.3042\n",
      "Epoch [9/10], Loss: 0.2975\n",
      "Epoch [10/10], Loss: 0.2928\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.3_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4903\n",
      "Epoch [2/10], Loss: 0.4179\n",
      "Epoch [3/10], Loss: 0.3768\n",
      "Epoch [4/10], Loss: 0.3488\n",
      "Epoch [5/10], Loss: 0.3301\n",
      "Epoch [6/10], Loss: 0.3172\n",
      "Epoch [7/10], Loss: 0.3083\n",
      "Epoch [8/10], Loss: 0.3008\n",
      "Epoch [9/10], Loss: 0.2957\n",
      "Epoch [10/10], Loss: 0.2912\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.3_alpha0.05_beta0.01.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.3_alpha0.05_beta0.01.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4315\n",
      "Epoch [2/10], Loss: 0.3382\n",
      "Epoch [3/10], Loss: 0.3104\n",
      "Epoch [4/10], Loss: 0.2973\n",
      "Epoch [5/10], Loss: 0.2906\n",
      "Epoch [6/10], Loss: 0.2865\n",
      "Epoch [7/10], Loss: 0.2838\n",
      "Epoch [8/10], Loss: 0.2820\n",
      "Epoch [9/10], Loss: 0.2806\n",
      "Epoch [10/10], Loss: 0.2795\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.3_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.8142\n",
      "Epoch [2/10], Loss: 0.7416\n",
      "Epoch [3/10], Loss: 0.6922\n",
      "Epoch [4/10], Loss: 0.6430\n",
      "Epoch [5/10], Loss: 0.5992\n",
      "Epoch [6/10], Loss: 0.5617\n",
      "Epoch [7/10], Loss: 0.5318\n",
      "Epoch [8/10], Loss: 0.5098\n",
      "Epoch [9/10], Loss: 0.4913\n",
      "Epoch [10/10], Loss: 0.4770\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.3_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.5934\n",
      "Epoch [2/10], Loss: 0.5006\n",
      "Epoch [3/10], Loss: 0.4201\n",
      "Epoch [4/10], Loss: 0.3623\n",
      "Epoch [5/10], Loss: 0.3233\n",
      "Epoch [6/10], Loss: 0.2974\n",
      "Epoch [7/10], Loss: 0.2794\n",
      "Epoch [8/10], Loss: 0.2677\n",
      "Epoch [9/10], Loss: 0.2589\n",
      "Epoch [10/10], Loss: 0.2518\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.3_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.6337\n",
      "Epoch [2/10], Loss: 0.5552\n",
      "Epoch [3/10], Loss: 0.4952\n",
      "Epoch [4/10], Loss: 0.4435\n",
      "Epoch [5/10], Loss: 0.4022\n",
      "Epoch [6/10], Loss: 0.3709\n",
      "Epoch [7/10], Loss: 0.3491\n",
      "Epoch [8/10], Loss: 0.3328\n",
      "Epoch [9/10], Loss: 0.3201\n",
      "Epoch [10/10], Loss: 0.3115\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.3_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.6381\n",
      "Epoch [2/10], Loss: 0.5528\n",
      "Epoch [3/10], Loss: 0.4843\n",
      "Epoch [4/10], Loss: 0.4267\n",
      "Epoch [5/10], Loss: 0.3853\n",
      "Epoch [6/10], Loss: 0.3577\n",
      "Epoch [7/10], Loss: 0.3388\n",
      "Epoch [8/10], Loss: 0.3255\n",
      "Epoch [9/10], Loss: 0.3158\n",
      "Epoch [10/10], Loss: 0.3081\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.3_alpha0.05_beta0.05.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.3_alpha0.05_beta0.05.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.5517\n",
      "Epoch [2/10], Loss: 0.3798\n",
      "Epoch [3/10], Loss: 0.3289\n",
      "Epoch [4/10], Loss: 0.3099\n",
      "Epoch [5/10], Loss: 0.3009\n",
      "Epoch [6/10], Loss: 0.2954\n",
      "Epoch [7/10], Loss: 0.2918\n",
      "Epoch [8/10], Loss: 0.2893\n",
      "Epoch [9/10], Loss: 0.2875\n",
      "Epoch [10/10], Loss: 0.2861\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.3_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.5609\n",
      "Epoch [2/10], Loss: 0.4879\n",
      "Epoch [3/10], Loss: 0.4652\n",
      "Epoch [4/10], Loss: 0.4450\n",
      "Epoch [5/10], Loss: 0.4286\n",
      "Epoch [6/10], Loss: 0.4166\n",
      "Epoch [7/10], Loss: 0.4078\n",
      "Epoch [8/10], Loss: 0.4021\n",
      "Epoch [9/10], Loss: 0.3974\n",
      "Epoch [10/10], Loss: 0.3930\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.5_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3498\n",
      "Epoch [2/10], Loss: 0.2849\n",
      "Epoch [3/10], Loss: 0.2520\n",
      "Epoch [4/10], Loss: 0.2326\n",
      "Epoch [5/10], Loss: 0.2218\n",
      "Epoch [6/10], Loss: 0.2140\n",
      "Epoch [7/10], Loss: 0.2085\n",
      "Epoch [8/10], Loss: 0.2032\n",
      "Epoch [9/10], Loss: 0.1990\n",
      "Epoch [10/10], Loss: 0.1953\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.5_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4051\n",
      "Epoch [2/10], Loss: 0.3377\n",
      "Epoch [3/10], Loss: 0.3065\n",
      "Epoch [4/10], Loss: 0.2882\n",
      "Epoch [5/10], Loss: 0.2766\n",
      "Epoch [6/10], Loss: 0.2690\n",
      "Epoch [7/10], Loss: 0.2635\n",
      "Epoch [8/10], Loss: 0.2591\n",
      "Epoch [9/10], Loss: 0.2548\n",
      "Epoch [10/10], Loss: 0.2518\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.5_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4059\n",
      "Epoch [2/10], Loss: 0.3383\n",
      "Epoch [3/10], Loss: 0.3080\n",
      "Epoch [4/10], Loss: 0.2880\n",
      "Epoch [5/10], Loss: 0.2763\n",
      "Epoch [6/10], Loss: 0.2691\n",
      "Epoch [7/10], Loss: 0.2638\n",
      "Epoch [8/10], Loss: 0.2590\n",
      "Epoch [9/10], Loss: 0.2546\n",
      "Epoch [10/10], Loss: 0.2516\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.5_alpha0.01_beta0.01.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.5_alpha0.01_beta0.01.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3531\n",
      "Epoch [2/10], Loss: 0.2870\n",
      "Epoch [3/10], Loss: 0.2682\n",
      "Epoch [4/10], Loss: 0.2565\n",
      "Epoch [5/10], Loss: 0.2492\n",
      "Epoch [6/10], Loss: 0.2448\n",
      "Epoch [7/10], Loss: 0.2420\n",
      "Epoch [8/10], Loss: 0.2404\n",
      "Epoch [9/10], Loss: 0.2392\n",
      "Epoch [10/10], Loss: 0.2384\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.5_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.7089\n",
      "Epoch [2/10], Loss: 0.6411\n",
      "Epoch [3/10], Loss: 0.6118\n",
      "Epoch [4/10], Loss: 0.5809\n",
      "Epoch [5/10], Loss: 0.5517\n",
      "Epoch [6/10], Loss: 0.5230\n",
      "Epoch [7/10], Loss: 0.4977\n",
      "Epoch [8/10], Loss: 0.4781\n",
      "Epoch [9/10], Loss: 0.4601\n",
      "Epoch [10/10], Loss: 0.4446\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.5_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4937\n",
      "Epoch [2/10], Loss: 0.4254\n",
      "Epoch [3/10], Loss: 0.3728\n",
      "Epoch [4/10], Loss: 0.3260\n",
      "Epoch [5/10], Loss: 0.2900\n",
      "Epoch [6/10], Loss: 0.2636\n",
      "Epoch [7/10], Loss: 0.2454\n",
      "Epoch [8/10], Loss: 0.2321\n",
      "Epoch [9/10], Loss: 0.2219\n",
      "Epoch [10/10], Loss: 0.2145\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.5_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.5412\n",
      "Epoch [2/10], Loss: 0.4762\n",
      "Epoch [3/10], Loss: 0.4363\n",
      "Epoch [4/10], Loss: 0.4016\n",
      "Epoch [5/10], Loss: 0.3687\n",
      "Epoch [6/10], Loss: 0.3411\n",
      "Epoch [7/10], Loss: 0.3196\n",
      "Epoch [8/10], Loss: 0.3030\n",
      "Epoch [9/10], Loss: 0.2895\n",
      "Epoch [10/10], Loss: 0.2796\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.5_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.5428\n",
      "Epoch [2/10], Loss: 0.4747\n",
      "Epoch [3/10], Loss: 0.4300\n",
      "Epoch [4/10], Loss: 0.3876\n",
      "Epoch [5/10], Loss: 0.3534\n",
      "Epoch [6/10], Loss: 0.3272\n",
      "Epoch [7/10], Loss: 0.3084\n",
      "Epoch [8/10], Loss: 0.2938\n",
      "Epoch [9/10], Loss: 0.2823\n",
      "Epoch [10/10], Loss: 0.2740\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.5_alpha0.01_beta0.05.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.5_alpha0.01_beta0.05.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4815\n",
      "Epoch [2/10], Loss: 0.3467\n",
      "Epoch [3/10], Loss: 0.2937\n",
      "Epoch [4/10], Loss: 0.2721\n",
      "Epoch [5/10], Loss: 0.2621\n",
      "Epoch [6/10], Loss: 0.2565\n",
      "Epoch [7/10], Loss: 0.2529\n",
      "Epoch [8/10], Loss: 0.2504\n",
      "Epoch [9/10], Loss: 0.2489\n",
      "Epoch [10/10], Loss: 0.2478\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.5_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.6841\n",
      "Epoch [2/10], Loss: 0.6169\n",
      "Epoch [3/10], Loss: 0.5897\n",
      "Epoch [4/10], Loss: 0.5638\n",
      "Epoch [5/10], Loss: 0.5419\n",
      "Epoch [6/10], Loss: 0.5226\n",
      "Epoch [7/10], Loss: 0.5064\n",
      "Epoch [8/10], Loss: 0.4940\n",
      "Epoch [9/10], Loss: 0.4825\n",
      "Epoch [10/10], Loss: 0.4719\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.5_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4708\n",
      "Epoch [2/10], Loss: 0.4054\n",
      "Epoch [3/10], Loss: 0.3628\n",
      "Epoch [4/10], Loss: 0.3304\n",
      "Epoch [5/10], Loss: 0.3066\n",
      "Epoch [6/10], Loss: 0.2884\n",
      "Epoch [7/10], Loss: 0.2753\n",
      "Epoch [8/10], Loss: 0.2648\n",
      "Epoch [9/10], Loss: 0.2566\n",
      "Epoch [10/10], Loss: 0.2499\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.5_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.5183\n",
      "Epoch [2/10], Loss: 0.4538\n",
      "Epoch [3/10], Loss: 0.4182\n",
      "Epoch [4/10], Loss: 0.3929\n",
      "Epoch [5/10], Loss: 0.3724\n",
      "Epoch [6/10], Loss: 0.3553\n",
      "Epoch [7/10], Loss: 0.3414\n",
      "Epoch [8/10], Loss: 0.3295\n",
      "Epoch [9/10], Loss: 0.3194\n",
      "Epoch [10/10], Loss: 0.3120\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.5_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.5210\n",
      "Epoch [2/10], Loss: 0.4542\n",
      "Epoch [3/10], Loss: 0.4172\n",
      "Epoch [4/10], Loss: 0.3873\n",
      "Epoch [5/10], Loss: 0.3650\n",
      "Epoch [6/10], Loss: 0.3476\n",
      "Epoch [7/10], Loss: 0.3342\n",
      "Epoch [8/10], Loss: 0.3232\n",
      "Epoch [9/10], Loss: 0.3143\n",
      "Epoch [10/10], Loss: 0.3074\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.5_alpha0.05_beta0.01.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.5_alpha0.05_beta0.01.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4666\n",
      "Epoch [2/10], Loss: 0.3673\n",
      "Epoch [3/10], Loss: 0.3281\n",
      "Epoch [4/10], Loss: 0.3089\n",
      "Epoch [5/10], Loss: 0.2992\n",
      "Epoch [6/10], Loss: 0.2935\n",
      "Epoch [7/10], Loss: 0.2900\n",
      "Epoch [8/10], Loss: 0.2879\n",
      "Epoch [9/10], Loss: 0.2864\n",
      "Epoch [10/10], Loss: 0.2853\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.5_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.8372\n",
      "Epoch [2/10], Loss: 0.7785\n",
      "Epoch [3/10], Loss: 0.7464\n",
      "Epoch [4/10], Loss: 0.7088\n",
      "Epoch [5/10], Loss: 0.6706\n",
      "Epoch [6/10], Loss: 0.6313\n",
      "Epoch [7/10], Loss: 0.5961\n",
      "Epoch [8/10], Loss: 0.5674\n",
      "Epoch [9/10], Loss: 0.5425\n",
      "Epoch [10/10], Loss: 0.5210\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.5_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.6195\n",
      "Epoch [2/10], Loss: 0.5565\n",
      "Epoch [3/10], Loss: 0.4924\n",
      "Epoch [4/10], Loss: 0.4291\n",
      "Epoch [5/10], Loss: 0.3790\n",
      "Epoch [6/10], Loss: 0.3413\n",
      "Epoch [7/10], Loss: 0.3142\n",
      "Epoch [8/10], Loss: 0.2949\n",
      "Epoch [9/10], Loss: 0.2809\n",
      "Epoch [10/10], Loss: 0.2706\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.5_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.6594\n",
      "Epoch [2/10], Loss: 0.5995\n",
      "Epoch [3/10], Loss: 0.5535\n",
      "Epoch [4/10], Loss: 0.5079\n",
      "Epoch [5/10], Loss: 0.4629\n",
      "Epoch [6/10], Loss: 0.4239\n",
      "Epoch [7/10], Loss: 0.3935\n",
      "Epoch [8/10], Loss: 0.3699\n",
      "Epoch [9/10], Loss: 0.3512\n",
      "Epoch [10/10], Loss: 0.3373\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.5_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.6656\n",
      "Epoch [2/10], Loss: 0.6000\n",
      "Epoch [3/10], Loss: 0.5477\n",
      "Epoch [4/10], Loss: 0.4928\n",
      "Epoch [5/10], Loss: 0.4445\n",
      "Epoch [6/10], Loss: 0.4065\n",
      "Epoch [7/10], Loss: 0.3788\n",
      "Epoch [8/10], Loss: 0.3578\n",
      "Epoch [9/10], Loss: 0.3423\n",
      "Epoch [10/10], Loss: 0.3307\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.5_alpha0.05_beta0.05.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.5_alpha0.05_beta0.05.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.6021\n",
      "Epoch [2/10], Loss: 0.4266\n",
      "Epoch [3/10], Loss: 0.3536\n",
      "Epoch [4/10], Loss: 0.3274\n",
      "Epoch [5/10], Loss: 0.3152\n",
      "Epoch [6/10], Loss: 0.3077\n",
      "Epoch [7/10], Loss: 0.3027\n",
      "Epoch [8/10], Loss: 0.2993\n",
      "Epoch [9/10], Loss: 0.2972\n",
      "Epoch [10/10], Loss: 0.2956\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim64_dropout0.5_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.5123\n",
      "Epoch [2/10], Loss: 0.4462\n",
      "Epoch [3/10], Loss: 0.4193\n",
      "Epoch [4/10], Loss: 0.4019\n",
      "Epoch [5/10], Loss: 0.3902\n",
      "Epoch [6/10], Loss: 0.3828\n",
      "Epoch [7/10], Loss: 0.3772\n",
      "Epoch [8/10], Loss: 0.3729\n",
      "Epoch [9/10], Loss: 0.3692\n",
      "Epoch [10/10], Loss: 0.3668\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.3_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3082\n",
      "Epoch [2/10], Loss: 0.2390\n",
      "Epoch [3/10], Loss: 0.2139\n",
      "Epoch [4/10], Loss: 0.2019\n",
      "Epoch [5/10], Loss: 0.1951\n",
      "Epoch [6/10], Loss: 0.1906\n",
      "Epoch [7/10], Loss: 0.1867\n",
      "Epoch [8/10], Loss: 0.1844\n",
      "Epoch [9/10], Loss: 0.1825\n",
      "Epoch [10/10], Loss: 0.1808\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.3_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3667\n",
      "Epoch [2/10], Loss: 0.2934\n",
      "Epoch [3/10], Loss: 0.2683\n",
      "Epoch [4/10], Loss: 0.2553\n",
      "Epoch [5/10], Loss: 0.2479\n",
      "Epoch [6/10], Loss: 0.2427\n",
      "Epoch [7/10], Loss: 0.2391\n",
      "Epoch [8/10], Loss: 0.2360\n",
      "Epoch [9/10], Loss: 0.2337\n",
      "Epoch [10/10], Loss: 0.2319\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.3_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3596\n",
      "Epoch [2/10], Loss: 0.2936\n",
      "Epoch [3/10], Loss: 0.2683\n",
      "Epoch [4/10], Loss: 0.2553\n",
      "Epoch [5/10], Loss: 0.2480\n",
      "Epoch [6/10], Loss: 0.2432\n",
      "Epoch [7/10], Loss: 0.2396\n",
      "Epoch [8/10], Loss: 0.2370\n",
      "Epoch [9/10], Loss: 0.2345\n",
      "Epoch [10/10], Loss: 0.2329\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.3_alpha0.01_beta0.01.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.3_alpha0.01_beta0.01.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3149\n",
      "Epoch [2/10], Loss: 0.2596\n",
      "Epoch [3/10], Loss: 0.2471\n",
      "Epoch [4/10], Loss: 0.2414\n",
      "Epoch [5/10], Loss: 0.2383\n",
      "Epoch [6/10], Loss: 0.2364\n",
      "Epoch [7/10], Loss: 0.2351\n",
      "Epoch [8/10], Loss: 0.2341\n",
      "Epoch [9/10], Loss: 0.2333\n",
      "Epoch [10/10], Loss: 0.2328\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.3_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.6629\n",
      "Epoch [2/10], Loss: 0.5676\n",
      "Epoch [3/10], Loss: 0.5040\n",
      "Epoch [4/10], Loss: 0.4604\n",
      "Epoch [5/10], Loss: 0.4325\n",
      "Epoch [6/10], Loss: 0.4149\n",
      "Epoch [7/10], Loss: 0.4028\n",
      "Epoch [8/10], Loss: 0.3935\n",
      "Epoch [9/10], Loss: 0.3871\n",
      "Epoch [10/10], Loss: 0.3819\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.3_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4420\n",
      "Epoch [2/10], Loss: 0.3180\n",
      "Epoch [3/10], Loss: 0.2582\n",
      "Epoch [4/10], Loss: 0.2302\n",
      "Epoch [5/10], Loss: 0.2158\n",
      "Epoch [6/10], Loss: 0.2073\n",
      "Epoch [7/10], Loss: 0.2007\n",
      "Epoch [8/10], Loss: 0.1967\n",
      "Epoch [9/10], Loss: 0.1937\n",
      "Epoch [10/10], Loss: 0.1909\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.3_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.5034\n",
      "Epoch [2/10], Loss: 0.3876\n",
      "Epoch [3/10], Loss: 0.3248\n",
      "Epoch [4/10], Loss: 0.2924\n",
      "Epoch [5/10], Loss: 0.2739\n",
      "Epoch [6/10], Loss: 0.2627\n",
      "Epoch [7/10], Loss: 0.2555\n",
      "Epoch [8/10], Loss: 0.2500\n",
      "Epoch [9/10], Loss: 0.2463\n",
      "Epoch [10/10], Loss: 0.2430\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.3_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4938\n",
      "Epoch [2/10], Loss: 0.3840\n",
      "Epoch [3/10], Loss: 0.3215\n",
      "Epoch [4/10], Loss: 0.2889\n",
      "Epoch [5/10], Loss: 0.2719\n",
      "Epoch [6/10], Loss: 0.2611\n",
      "Epoch [7/10], Loss: 0.2543\n",
      "Epoch [8/10], Loss: 0.2494\n",
      "Epoch [9/10], Loss: 0.2457\n",
      "Epoch [10/10], Loss: 0.2431\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.3_alpha0.01_beta0.05.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.3_alpha0.01_beta0.05.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3930\n",
      "Epoch [2/10], Loss: 0.2781\n",
      "Epoch [3/10], Loss: 0.2582\n",
      "Epoch [4/10], Loss: 0.2505\n",
      "Epoch [5/10], Loss: 0.2464\n",
      "Epoch [6/10], Loss: 0.2439\n",
      "Epoch [7/10], Loss: 0.2422\n",
      "Epoch [8/10], Loss: 0.2406\n",
      "Epoch [9/10], Loss: 0.2395\n",
      "Epoch [10/10], Loss: 0.2387\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.3_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.6391\n",
      "Epoch [2/10], Loss: 0.5588\n",
      "Epoch [3/10], Loss: 0.5132\n",
      "Epoch [4/10], Loss: 0.4814\n",
      "Epoch [5/10], Loss: 0.4608\n",
      "Epoch [6/10], Loss: 0.4477\n",
      "Epoch [7/10], Loss: 0.4381\n",
      "Epoch [8/10], Loss: 0.4307\n",
      "Epoch [9/10], Loss: 0.4252\n",
      "Epoch [10/10], Loss: 0.4213\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.3_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4252\n",
      "Epoch [2/10], Loss: 0.3275\n",
      "Epoch [3/10], Loss: 0.2833\n",
      "Epoch [4/10], Loss: 0.2624\n",
      "Epoch [5/10], Loss: 0.2507\n",
      "Epoch [6/10], Loss: 0.2436\n",
      "Epoch [7/10], Loss: 0.2379\n",
      "Epoch [8/10], Loss: 0.2342\n",
      "Epoch [9/10], Loss: 0.2313\n",
      "Epoch [10/10], Loss: 0.2289\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.3_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4822\n",
      "Epoch [2/10], Loss: 0.3880\n",
      "Epoch [3/10], Loss: 0.3429\n",
      "Epoch [4/10], Loss: 0.3191\n",
      "Epoch [5/10], Loss: 0.3054\n",
      "Epoch [6/10], Loss: 0.2962\n",
      "Epoch [7/10], Loss: 0.2904\n",
      "Epoch [8/10], Loss: 0.2857\n",
      "Epoch [9/10], Loss: 0.2822\n",
      "Epoch [10/10], Loss: 0.2793\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.3_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4749\n",
      "Epoch [2/10], Loss: 0.3876\n",
      "Epoch [3/10], Loss: 0.3416\n",
      "Epoch [4/10], Loss: 0.3171\n",
      "Epoch [5/10], Loss: 0.3042\n",
      "Epoch [6/10], Loss: 0.2957\n",
      "Epoch [7/10], Loss: 0.2896\n",
      "Epoch [8/10], Loss: 0.2853\n",
      "Epoch [9/10], Loss: 0.2820\n",
      "Epoch [10/10], Loss: 0.2793\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.3_alpha0.05_beta0.01.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.3_alpha0.05_beta0.01.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4022\n",
      "Epoch [2/10], Loss: 0.3148\n",
      "Epoch [3/10], Loss: 0.2971\n",
      "Epoch [4/10], Loss: 0.2894\n",
      "Epoch [5/10], Loss: 0.2850\n",
      "Epoch [6/10], Loss: 0.2824\n",
      "Epoch [7/10], Loss: 0.2807\n",
      "Epoch [8/10], Loss: 0.2791\n",
      "Epoch [9/10], Loss: 0.2782\n",
      "Epoch [10/10], Loss: 0.2774\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.3_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.7962\n",
      "Epoch [2/10], Loss: 0.6946\n",
      "Epoch [3/10], Loss: 0.6090\n",
      "Epoch [4/10], Loss: 0.5477\n",
      "Epoch [5/10], Loss: 0.5082\n",
      "Epoch [6/10], Loss: 0.4827\n",
      "Epoch [7/10], Loss: 0.4650\n",
      "Epoch [8/10], Loss: 0.4518\n",
      "Epoch [9/10], Loss: 0.4425\n",
      "Epoch [10/10], Loss: 0.4358\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.3_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.5684\n",
      "Epoch [2/10], Loss: 0.4254\n",
      "Epoch [3/10], Loss: 0.3403\n",
      "Epoch [4/10], Loss: 0.2971\n",
      "Epoch [5/10], Loss: 0.2746\n",
      "Epoch [6/10], Loss: 0.2612\n",
      "Epoch [7/10], Loss: 0.2521\n",
      "Epoch [8/10], Loss: 0.2464\n",
      "Epoch [9/10], Loss: 0.2423\n",
      "Epoch [10/10], Loss: 0.2389\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.3_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.6264\n",
      "Epoch [2/10], Loss: 0.5015\n",
      "Epoch [3/10], Loss: 0.4137\n",
      "Epoch [4/10], Loss: 0.3641\n",
      "Epoch [5/10], Loss: 0.3355\n",
      "Epoch [6/10], Loss: 0.3182\n",
      "Epoch [7/10], Loss: 0.3079\n",
      "Epoch [8/10], Loss: 0.3002\n",
      "Epoch [9/10], Loss: 0.2951\n",
      "Epoch [10/10], Loss: 0.2906\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.3_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.6190\n",
      "Epoch [2/10], Loss: 0.4946\n",
      "Epoch [3/10], Loss: 0.4069\n",
      "Epoch [4/10], Loss: 0.3562\n",
      "Epoch [5/10], Loss: 0.3300\n",
      "Epoch [6/10], Loss: 0.3142\n",
      "Epoch [7/10], Loss: 0.3043\n",
      "Epoch [8/10], Loss: 0.2976\n",
      "Epoch [9/10], Loss: 0.2930\n",
      "Epoch [10/10], Loss: 0.2896\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.3_alpha0.05_beta0.05.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.3_alpha0.05_beta0.05.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4912\n",
      "Epoch [2/10], Loss: 0.3347\n",
      "Epoch [3/10], Loss: 0.3088\n",
      "Epoch [4/10], Loss: 0.2994\n",
      "Epoch [5/10], Loss: 0.2943\n",
      "Epoch [6/10], Loss: 0.2909\n",
      "Epoch [7/10], Loss: 0.2888\n",
      "Epoch [8/10], Loss: 0.2866\n",
      "Epoch [9/10], Loss: 0.2852\n",
      "Epoch [10/10], Loss: 0.2842\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.3_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.5464\n",
      "Epoch [2/10], Loss: 0.4801\n",
      "Epoch [3/10], Loss: 0.4529\n",
      "Epoch [4/10], Loss: 0.4330\n",
      "Epoch [5/10], Loss: 0.4172\n",
      "Epoch [6/10], Loss: 0.4055\n",
      "Epoch [7/10], Loss: 0.3967\n",
      "Epoch [8/10], Loss: 0.3906\n",
      "Epoch [9/10], Loss: 0.3853\n",
      "Epoch [10/10], Loss: 0.3810\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.5_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3431\n",
      "Epoch [2/10], Loss: 0.2747\n",
      "Epoch [3/10], Loss: 0.2413\n",
      "Epoch [4/10], Loss: 0.2217\n",
      "Epoch [5/10], Loss: 0.2107\n",
      "Epoch [6/10], Loss: 0.2032\n",
      "Epoch [7/10], Loss: 0.1976\n",
      "Epoch [8/10], Loss: 0.1933\n",
      "Epoch [9/10], Loss: 0.1904\n",
      "Epoch [10/10], Loss: 0.1873\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.5_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3982\n",
      "Epoch [2/10], Loss: 0.3261\n",
      "Epoch [3/10], Loss: 0.2954\n",
      "Epoch [4/10], Loss: 0.2764\n",
      "Epoch [5/10], Loss: 0.2652\n",
      "Epoch [6/10], Loss: 0.2572\n",
      "Epoch [7/10], Loss: 0.2513\n",
      "Epoch [8/10], Loss: 0.2468\n",
      "Epoch [9/10], Loss: 0.2431\n",
      "Epoch [10/10], Loss: 0.2403\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.5_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3922\n",
      "Epoch [2/10], Loss: 0.3268\n",
      "Epoch [3/10], Loss: 0.2941\n",
      "Epoch [4/10], Loss: 0.2755\n",
      "Epoch [5/10], Loss: 0.2641\n",
      "Epoch [6/10], Loss: 0.2570\n",
      "Epoch [7/10], Loss: 0.2513\n",
      "Epoch [8/10], Loss: 0.2472\n",
      "Epoch [9/10], Loss: 0.2437\n",
      "Epoch [10/10], Loss: 0.2409\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.5_alpha0.01_beta0.01.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.5_alpha0.01_beta0.01.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.3427\n",
      "Epoch [2/10], Loss: 0.2737\n",
      "Epoch [3/10], Loss: 0.2553\n",
      "Epoch [4/10], Loss: 0.2467\n",
      "Epoch [5/10], Loss: 0.2425\n",
      "Epoch [6/10], Loss: 0.2402\n",
      "Epoch [7/10], Loss: 0.2388\n",
      "Epoch [8/10], Loss: 0.2379\n",
      "Epoch [9/10], Loss: 0.2370\n",
      "Epoch [10/10], Loss: 0.2365\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.5_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.6945\n",
      "Epoch [2/10], Loss: 0.6209\n",
      "Epoch [3/10], Loss: 0.5686\n",
      "Epoch [4/10], Loss: 0.5215\n",
      "Epoch [5/10], Loss: 0.4842\n",
      "Epoch [6/10], Loss: 0.4579\n",
      "Epoch [7/10], Loss: 0.4381\n",
      "Epoch [8/10], Loss: 0.4237\n",
      "Epoch [9/10], Loss: 0.4126\n",
      "Epoch [10/10], Loss: 0.4042\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.5_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4824\n",
      "Epoch [2/10], Loss: 0.3824\n",
      "Epoch [3/10], Loss: 0.3115\n",
      "Epoch [4/10], Loss: 0.2673\n",
      "Epoch [5/10], Loss: 0.2424\n",
      "Epoch [6/10], Loss: 0.2274\n",
      "Epoch [7/10], Loss: 0.2175\n",
      "Epoch [8/10], Loss: 0.2109\n",
      "Epoch [9/10], Loss: 0.2067\n",
      "Epoch [10/10], Loss: 0.2025\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.5_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.5361\n",
      "Epoch [2/10], Loss: 0.4476\n",
      "Epoch [3/10], Loss: 0.3822\n",
      "Epoch [4/10], Loss: 0.3364\n",
      "Epoch [5/10], Loss: 0.3072\n",
      "Epoch [6/10], Loss: 0.2884\n",
      "Epoch [7/10], Loss: 0.2760\n",
      "Epoch [8/10], Loss: 0.2675\n",
      "Epoch [9/10], Loss: 0.2614\n",
      "Epoch [10/10], Loss: 0.2569\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.5_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.5293\n",
      "Epoch [2/10], Loss: 0.4433\n",
      "Epoch [3/10], Loss: 0.3757\n",
      "Epoch [4/10], Loss: 0.3292\n",
      "Epoch [5/10], Loss: 0.3011\n",
      "Epoch [6/10], Loss: 0.2842\n",
      "Epoch [7/10], Loss: 0.2731\n",
      "Epoch [8/10], Loss: 0.2658\n",
      "Epoch [9/10], Loss: 0.2603\n",
      "Epoch [10/10], Loss: 0.2561\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.5_alpha0.01_beta0.05.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.5_alpha0.01_beta0.05.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4388\n",
      "Epoch [2/10], Loss: 0.3015\n",
      "Epoch [3/10], Loss: 0.2721\n",
      "Epoch [4/10], Loss: 0.2612\n",
      "Epoch [5/10], Loss: 0.2558\n",
      "Epoch [6/10], Loss: 0.2524\n",
      "Epoch [7/10], Loss: 0.2500\n",
      "Epoch [8/10], Loss: 0.2484\n",
      "Epoch [9/10], Loss: 0.2468\n",
      "Epoch [10/10], Loss: 0.2459\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.5_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.6708\n",
      "Epoch [2/10], Loss: 0.6037\n",
      "Epoch [3/10], Loss: 0.5644\n",
      "Epoch [4/10], Loss: 0.5305\n",
      "Epoch [5/10], Loss: 0.5035\n",
      "Epoch [6/10], Loss: 0.4834\n",
      "Epoch [7/10], Loss: 0.4680\n",
      "Epoch [8/10], Loss: 0.4571\n",
      "Epoch [9/10], Loss: 0.4478\n",
      "Epoch [10/10], Loss: 0.4407\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.5_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4623\n",
      "Epoch [2/10], Loss: 0.3796\n",
      "Epoch [3/10], Loss: 0.3269\n",
      "Epoch [4/10], Loss: 0.2937\n",
      "Epoch [5/10], Loss: 0.2738\n",
      "Epoch [6/10], Loss: 0.2613\n",
      "Epoch [7/10], Loss: 0.2527\n",
      "Epoch [8/10], Loss: 0.2463\n",
      "Epoch [9/10], Loss: 0.2424\n",
      "Epoch [10/10], Loss: 0.2384\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.5_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.5133\n",
      "Epoch [2/10], Loss: 0.4352\n",
      "Epoch [3/10], Loss: 0.3875\n",
      "Epoch [4/10], Loss: 0.3541\n",
      "Epoch [5/10], Loss: 0.3328\n",
      "Epoch [6/10], Loss: 0.3182\n",
      "Epoch [7/10], Loss: 0.3079\n",
      "Epoch [8/10], Loss: 0.3003\n",
      "Epoch [9/10], Loss: 0.2950\n",
      "Epoch [10/10], Loss: 0.2907\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.5_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.5083\n",
      "Epoch [2/10], Loss: 0.4346\n",
      "Epoch [3/10], Loss: 0.3847\n",
      "Epoch [4/10], Loss: 0.3507\n",
      "Epoch [5/10], Loss: 0.3292\n",
      "Epoch [6/10], Loss: 0.3158\n",
      "Epoch [7/10], Loss: 0.3062\n",
      "Epoch [8/10], Loss: 0.2995\n",
      "Epoch [9/10], Loss: 0.2942\n",
      "Epoch [10/10], Loss: 0.2899\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.5_alpha0.05_beta0.01.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.5_alpha0.05_beta0.01.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.4401\n",
      "Epoch [2/10], Loss: 0.3352\n",
      "Epoch [3/10], Loss: 0.3085\n",
      "Epoch [4/10], Loss: 0.2975\n",
      "Epoch [5/10], Loss: 0.2920\n",
      "Epoch [6/10], Loss: 0.2888\n",
      "Epoch [7/10], Loss: 0.2869\n",
      "Epoch [8/10], Loss: 0.2853\n",
      "Epoch [9/10], Loss: 0.2840\n",
      "Epoch [10/10], Loss: 0.2833\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.5_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.8244\n",
      "Epoch [2/10], Loss: 0.7542\n",
      "Epoch [3/10], Loss: 0.6883\n",
      "Epoch [4/10], Loss: 0.6229\n",
      "Epoch [5/10], Loss: 0.5726\n",
      "Epoch [6/10], Loss: 0.5369\n",
      "Epoch [7/10], Loss: 0.5098\n",
      "Epoch [8/10], Loss: 0.4910\n",
      "Epoch [9/10], Loss: 0.4759\n",
      "Epoch [10/10], Loss: 0.4652\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.5_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.6074\n",
      "Epoch [2/10], Loss: 0.4987\n",
      "Epoch [3/10], Loss: 0.4058\n",
      "Epoch [4/10], Loss: 0.3451\n",
      "Epoch [5/10], Loss: 0.3096\n",
      "Epoch [6/10], Loss: 0.2887\n",
      "Epoch [7/10], Loss: 0.2759\n",
      "Epoch [8/10], Loss: 0.2667\n",
      "Epoch [9/10], Loss: 0.2613\n",
      "Epoch [10/10], Loss: 0.2560\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.5_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.6562\n",
      "Epoch [2/10], Loss: 0.5699\n",
      "Epoch [3/10], Loss: 0.4878\n",
      "Epoch [4/10], Loss: 0.4230\n",
      "Epoch [5/10], Loss: 0.3810\n",
      "Epoch [6/10], Loss: 0.3539\n",
      "Epoch [7/10], Loss: 0.3359\n",
      "Epoch [8/10], Loss: 0.3236\n",
      "Epoch [9/10], Loss: 0.3161\n",
      "Epoch [10/10], Loss: 0.3099\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.5_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.6529\n",
      "Epoch [2/10], Loss: 0.5618\n",
      "Epoch [3/10], Loss: 0.4754\n",
      "Epoch [4/10], Loss: 0.4096\n",
      "Epoch [5/10], Loss: 0.3689\n",
      "Epoch [6/10], Loss: 0.3449\n",
      "Epoch [7/10], Loss: 0.3296\n",
      "Epoch [8/10], Loss: 0.3200\n",
      "Epoch [9/10], Loss: 0.3131\n",
      "Epoch [10/10], Loss: 0.3077\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.5_alpha0.05_beta0.05.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.5_alpha0.05_beta0.05.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/10], Loss: 0.5462\n",
      "Epoch [2/10], Loss: 0.3658\n",
      "Epoch [3/10], Loss: 0.3285\n",
      "Epoch [4/10], Loss: 0.3156\n",
      "Epoch [5/10], Loss: 0.3081\n",
      "Epoch [6/10], Loss: 0.3030\n",
      "Epoch [7/10], Loss: 0.3000\n",
      "Epoch [8/10], Loss: 0.2976\n",
      "Epoch [9/10], Loss: 0.2956\n",
      "Epoch [10/10], Loss: 0.2943\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs10_lr0.0001_embedding_dim128_dropout0.5_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/20], Loss: 0.4294\n",
      "Epoch [2/20], Loss: 0.3764\n",
      "Epoch [3/20], Loss: 0.3639\n",
      "Epoch [4/20], Loss: 0.3581\n",
      "Epoch [5/20], Loss: 0.3541\n",
      "Epoch [6/20], Loss: 0.3520\n",
      "Epoch [7/20], Loss: 0.3497\n",
      "Epoch [8/20], Loss: 0.3471\n",
      "Epoch [9/20], Loss: 0.3447\n",
      "Epoch [10/20], Loss: 0.3418\n",
      "Epoch [11/20], Loss: 0.3388\n",
      "Epoch [12/20], Loss: 0.3355\n",
      "Epoch [13/20], Loss: 0.3320\n",
      "Epoch [14/20], Loss: 0.3274\n",
      "Epoch [15/20], Loss: 0.3235\n",
      "Epoch [16/20], Loss: 0.3197\n",
      "Epoch [17/20], Loss: 0.3149\n",
      "Epoch [18/20], Loss: 0.3099\n",
      "Epoch [19/20], Loss: 0.3048\n",
      "Epoch [20/20], Loss: 0.3001\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs20_lr0.001_embedding_dim64_dropout0.3_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/20], Loss: 0.2275\n",
      "Epoch [2/20], Loss: 0.1863\n",
      "Epoch [3/20], Loss: 0.1792\n",
      "Epoch [4/20], Loss: 0.1766\n",
      "Epoch [5/20], Loss: 0.1750\n",
      "Epoch [6/20], Loss: 0.1737\n",
      "Epoch [7/20], Loss: 0.1727\n",
      "Epoch [8/20], Loss: 0.1713\n",
      "Epoch [9/20], Loss: 0.1700\n",
      "Epoch [10/20], Loss: 0.1688\n",
      "Epoch [11/20], Loss: 0.1674\n",
      "Epoch [12/20], Loss: 0.1660\n",
      "Epoch [13/20], Loss: 0.1646\n",
      "Epoch [14/20], Loss: 0.1627\n",
      "Epoch [15/20], Loss: 0.1609\n",
      "Epoch [16/20], Loss: 0.1590\n",
      "Epoch [17/20], Loss: 0.1571\n",
      "Epoch [18/20], Loss: 0.1551\n",
      "Epoch [19/20], Loss: 0.1528\n",
      "Epoch [20/20], Loss: 0.1505\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs20_lr0.001_embedding_dim64_dropout0.3_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/20], Loss: 0.2831\n",
      "Epoch [2/20], Loss: 0.2398\n",
      "Epoch [3/20], Loss: 0.2309\n",
      "Epoch [4/20], Loss: 0.2270\n",
      "Epoch [5/20], Loss: 0.2253\n",
      "Epoch [6/20], Loss: 0.2238\n",
      "Epoch [7/20], Loss: 0.2226\n",
      "Epoch [8/20], Loss: 0.2212\n",
      "Epoch [9/20], Loss: 0.2196\n",
      "Epoch [10/20], Loss: 0.2179\n",
      "Epoch [11/20], Loss: 0.2160\n",
      "Epoch [12/20], Loss: 0.2139\n",
      "Epoch [13/20], Loss: 0.2112\n",
      "Epoch [14/20], Loss: 0.2089\n",
      "Epoch [15/20], Loss: 0.2059\n",
      "Epoch [16/20], Loss: 0.2028\n",
      "Epoch [17/20], Loss: 0.1995\n",
      "Epoch [18/20], Loss: 0.1961\n",
      "Epoch [19/20], Loss: 0.1925\n",
      "Epoch [20/20], Loss: 0.1887\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs20_lr0.001_embedding_dim64_dropout0.3_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/20], Loss: 0.2831\n",
      "Epoch [2/20], Loss: 0.2402\n",
      "Epoch [3/20], Loss: 0.2321\n",
      "Epoch [4/20], Loss: 0.2286\n",
      "Epoch [5/20], Loss: 0.2268\n",
      "Epoch [6/20], Loss: 0.2251\n",
      "Epoch [7/20], Loss: 0.2238\n",
      "Epoch [8/20], Loss: 0.2223\n",
      "Epoch [9/20], Loss: 0.2209\n",
      "Epoch [10/20], Loss: 0.2188\n",
      "Epoch [11/20], Loss: 0.2169\n",
      "Epoch [12/20], Loss: 0.2148\n",
      "Epoch [13/20], Loss: 0.2123\n",
      "Epoch [14/20], Loss: 0.2100\n",
      "Epoch [15/20], Loss: 0.2064\n",
      "Epoch [16/20], Loss: 0.2033\n",
      "Epoch [17/20], Loss: 0.2002\n",
      "Epoch [18/20], Loss: 0.1967\n",
      "Epoch [19/20], Loss: 0.1936\n",
      "Epoch [20/20], Loss: 0.1898\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs20_lr0.001_embedding_dim64_dropout0.3_alpha0.01_beta0.01.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs20_lr0.001_embedding_dim64_dropout0.3_alpha0.01_beta0.01.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/20], Loss: 0.2599\n",
      "Epoch [2/20], Loss: 0.2373\n",
      "Epoch [3/20], Loss: 0.2349\n",
      "Epoch [4/20], Loss: 0.2333\n",
      "Epoch [5/20], Loss: 0.2321\n",
      "Epoch [6/20], Loss: 0.2309\n",
      "Epoch [7/20], Loss: 0.2296\n",
      "Epoch [8/20], Loss: 0.2281\n",
      "Epoch [9/20], Loss: 0.2263\n",
      "Epoch [10/20], Loss: 0.2244\n",
      "Epoch [11/20], Loss: 0.2222\n",
      "Epoch [12/20], Loss: 0.2199\n",
      "Epoch [13/20], Loss: 0.2176\n",
      "Epoch [14/20], Loss: 0.2150\n",
      "Epoch [15/20], Loss: 0.2126\n",
      "Epoch [16/20], Loss: 0.2099\n",
      "Epoch [17/20], Loss: 0.2074\n",
      "Epoch [18/20], Loss: 0.2049\n",
      "Epoch [19/20], Loss: 0.2025\n",
      "Epoch [20/20], Loss: 0.2001\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs20_lr0.001_embedding_dim64_dropout0.3_alpha0.01_beta0.01.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/20], Loss: 0.5165\n",
      "Epoch [2/20], Loss: 0.3876\n",
      "Epoch [3/20], Loss: 0.3731\n",
      "Epoch [4/20], Loss: 0.3670\n",
      "Epoch [5/20], Loss: 0.3625\n",
      "Epoch [6/20], Loss: 0.3598\n",
      "Epoch [7/20], Loss: 0.3568\n",
      "Epoch [8/20], Loss: 0.3542\n",
      "Epoch [9/20], Loss: 0.3518\n",
      "Epoch [10/20], Loss: 0.3489\n",
      "Epoch [11/20], Loss: 0.3460\n",
      "Epoch [12/20], Loss: 0.3429\n",
      "Epoch [13/20], Loss: 0.3397\n",
      "Epoch [14/20], Loss: 0.3365\n",
      "Epoch [15/20], Loss: 0.3329\n",
      "Epoch [16/20], Loss: 0.3292\n",
      "Epoch [17/20], Loss: 0.3258\n",
      "Epoch [18/20], Loss: 0.3211\n",
      "Epoch [19/20], Loss: 0.3172\n",
      "Epoch [20/20], Loss: 0.3136\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs20_lr0.001_embedding_dim64_dropout0.3_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/20], Loss: 0.2820\n",
      "Epoch [2/20], Loss: 0.1959\n",
      "Epoch [3/20], Loss: 0.1880\n",
      "Epoch [4/20], Loss: 0.1842\n",
      "Epoch [5/20], Loss: 0.1819\n",
      "Epoch [6/20], Loss: 0.1801\n",
      "Epoch [7/20], Loss: 0.1787\n",
      "Epoch [8/20], Loss: 0.1771\n",
      "Epoch [9/20], Loss: 0.1757\n",
      "Epoch [10/20], Loss: 0.1746\n",
      "Epoch [11/20], Loss: 0.1732\n",
      "Epoch [12/20], Loss: 0.1719\n",
      "Epoch [13/20], Loss: 0.1705\n",
      "Epoch [14/20], Loss: 0.1689\n",
      "Epoch [15/20], Loss: 0.1672\n",
      "Epoch [16/20], Loss: 0.1657\n",
      "Epoch [17/20], Loss: 0.1643\n",
      "Epoch [18/20], Loss: 0.1622\n",
      "Epoch [19/20], Loss: 0.1603\n",
      "Epoch [20/20], Loss: 0.1583\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs20_lr0.001_embedding_dim64_dropout0.3_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/20], Loss: 0.3465\n",
      "Epoch [2/20], Loss: 0.2492\n",
      "Epoch [3/20], Loss: 0.2398\n",
      "Epoch [4/20], Loss: 0.2358\n",
      "Epoch [5/20], Loss: 0.2332\n",
      "Epoch [6/20], Loss: 0.2307\n",
      "Epoch [7/20], Loss: 0.2292\n",
      "Epoch [8/20], Loss: 0.2273\n",
      "Epoch [9/20], Loss: 0.2255\n",
      "Epoch [10/20], Loss: 0.2239\n",
      "Epoch [11/20], Loss: 0.2221\n",
      "Epoch [12/20], Loss: 0.2203\n",
      "Epoch [13/20], Loss: 0.2179\n",
      "Epoch [14/20], Loss: 0.2160\n",
      "Epoch [15/20], Loss: 0.2135\n",
      "Epoch [16/20], Loss: 0.2116\n",
      "Epoch [17/20], Loss: 0.2087\n",
      "Epoch [18/20], Loss: 0.2061\n",
      "Epoch [19/20], Loss: 0.2034\n",
      "Epoch [20/20], Loss: 0.2001\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs20_lr0.001_embedding_dim64_dropout0.3_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/20], Loss: 0.3407\n",
      "Epoch [2/20], Loss: 0.2502\n",
      "Epoch [3/20], Loss: 0.2414\n",
      "Epoch [4/20], Loss: 0.2370\n",
      "Epoch [5/20], Loss: 0.2342\n",
      "Epoch [6/20], Loss: 0.2317\n",
      "Epoch [7/20], Loss: 0.2300\n",
      "Epoch [8/20], Loss: 0.2283\n",
      "Epoch [9/20], Loss: 0.2266\n",
      "Epoch [10/20], Loss: 0.2247\n",
      "Epoch [11/20], Loss: 0.2226\n",
      "Epoch [12/20], Loss: 0.2209\n",
      "Epoch [13/20], Loss: 0.2190\n",
      "Epoch [14/20], Loss: 0.2172\n",
      "Epoch [15/20], Loss: 0.2140\n",
      "Epoch [16/20], Loss: 0.2117\n",
      "Epoch [17/20], Loss: 0.2091\n",
      "Epoch [18/20], Loss: 0.2064\n",
      "Epoch [19/20], Loss: 0.2037\n",
      "Epoch [20/20], Loss: 0.2004\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs20_lr0.001_embedding_dim64_dropout0.3_alpha0.01_beta0.05.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs20_lr0.001_embedding_dim64_dropout0.3_alpha0.01_beta0.05.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/20], Loss: 0.2817\n",
      "Epoch [2/20], Loss: 0.2432\n",
      "Epoch [3/20], Loss: 0.2392\n",
      "Epoch [4/20], Loss: 0.2371\n",
      "Epoch [5/20], Loss: 0.2360\n",
      "Epoch [6/20], Loss: 0.2348\n",
      "Epoch [7/20], Loss: 0.2334\n",
      "Epoch [8/20], Loss: 0.2319\n",
      "Epoch [9/20], Loss: 0.2302\n",
      "Epoch [10/20], Loss: 0.2286\n",
      "Epoch [11/20], Loss: 0.2265\n",
      "Epoch [12/20], Loss: 0.2244\n",
      "Epoch [13/20], Loss: 0.2224\n",
      "Epoch [14/20], Loss: 0.2202\n",
      "Epoch [15/20], Loss: 0.2179\n",
      "Epoch [16/20], Loss: 0.2158\n",
      "Epoch [17/20], Loss: 0.2134\n",
      "Epoch [18/20], Loss: 0.2113\n",
      "Epoch [19/20], Loss: 0.2088\n",
      "Epoch [20/20], Loss: 0.2068\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs20_lr0.001_embedding_dim64_dropout0.3_alpha0.01_beta0.05.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/20], Loss: 0.5253\n",
      "Epoch [2/20], Loss: 0.4291\n",
      "Epoch [3/20], Loss: 0.4139\n",
      "Epoch [4/20], Loss: 0.4075\n",
      "Epoch [5/20], Loss: 0.4032\n",
      "Epoch [6/20], Loss: 0.4004\n",
      "Epoch [7/20], Loss: 0.3974\n",
      "Epoch [8/20], Loss: 0.3947\n",
      "Epoch [9/20], Loss: 0.3920\n",
      "Epoch [10/20], Loss: 0.3888\n",
      "Epoch [11/20], Loss: 0.3856\n",
      "Epoch [12/20], Loss: 0.3824\n",
      "Epoch [13/20], Loss: 0.3784\n",
      "Epoch [14/20], Loss: 0.3744\n",
      "Epoch [15/20], Loss: 0.3703\n",
      "Epoch [16/20], Loss: 0.3660\n",
      "Epoch [17/20], Loss: 0.3618\n",
      "Epoch [18/20], Loss: 0.3570\n",
      "Epoch [19/20], Loss: 0.3523\n",
      "Epoch [20/20], Loss: 0.3474\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs20_lr0.001_embedding_dim64_dropout0.3_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/20], Loss: 0.3024\n",
      "Epoch [2/20], Loss: 0.2338\n",
      "Epoch [3/20], Loss: 0.2259\n",
      "Epoch [4/20], Loss: 0.2222\n",
      "Epoch [5/20], Loss: 0.2201\n",
      "Epoch [6/20], Loss: 0.2181\n",
      "Epoch [7/20], Loss: 0.2169\n",
      "Epoch [8/20], Loss: 0.2152\n",
      "Epoch [9/20], Loss: 0.2133\n",
      "Epoch [10/20], Loss: 0.2120\n",
      "Epoch [11/20], Loss: 0.2100\n",
      "Epoch [12/20], Loss: 0.2083\n",
      "Epoch [13/20], Loss: 0.2065\n",
      "Epoch [14/20], Loss: 0.2042\n",
      "Epoch [15/20], Loss: 0.2020\n",
      "Epoch [16/20], Loss: 0.2000\n",
      "Epoch [17/20], Loss: 0.1980\n",
      "Epoch [18/20], Loss: 0.1954\n",
      "Epoch [19/20], Loss: 0.1928\n",
      "Epoch [20/20], Loss: 0.1904\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs20_lr0.001_embedding_dim64_dropout0.3_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/20], Loss: 0.3619\n",
      "Epoch [2/20], Loss: 0.2861\n",
      "Epoch [3/20], Loss: 0.2764\n",
      "Epoch [4/20], Loss: 0.2723\n",
      "Epoch [5/20], Loss: 0.2698\n",
      "Epoch [6/20], Loss: 0.2674\n",
      "Epoch [7/20], Loss: 0.2662\n",
      "Epoch [8/20], Loss: 0.2639\n",
      "Epoch [9/20], Loss: 0.2619\n",
      "Epoch [10/20], Loss: 0.2600\n",
      "Epoch [11/20], Loss: 0.2580\n",
      "Epoch [12/20], Loss: 0.2559\n",
      "Epoch [13/20], Loss: 0.2531\n",
      "Epoch [14/20], Loss: 0.2504\n",
      "Epoch [15/20], Loss: 0.2476\n",
      "Epoch [16/20], Loss: 0.2447\n",
      "Epoch [17/20], Loss: 0.2417\n",
      "Epoch [18/20], Loss: 0.2387\n",
      "Epoch [19/20], Loss: 0.2354\n",
      "Epoch [20/20], Loss: 0.2314\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs20_lr0.001_embedding_dim64_dropout0.3_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/20], Loss: 0.3591\n",
      "Epoch [2/20], Loss: 0.2866\n",
      "Epoch [3/20], Loss: 0.2776\n",
      "Epoch [4/20], Loss: 0.2733\n",
      "Epoch [5/20], Loss: 0.2708\n",
      "Epoch [6/20], Loss: 0.2683\n",
      "Epoch [7/20], Loss: 0.2669\n",
      "Epoch [8/20], Loss: 0.2648\n",
      "Epoch [9/20], Loss: 0.2628\n",
      "Epoch [10/20], Loss: 0.2602\n",
      "Epoch [11/20], Loss: 0.2582\n",
      "Epoch [12/20], Loss: 0.2560\n",
      "Epoch [13/20], Loss: 0.2534\n",
      "Epoch [14/20], Loss: 0.2514\n",
      "Epoch [15/20], Loss: 0.2477\n",
      "Epoch [16/20], Loss: 0.2450\n",
      "Epoch [17/20], Loss: 0.2418\n",
      "Epoch [18/20], Loss: 0.2385\n",
      "Epoch [19/20], Loss: 0.2351\n",
      "Epoch [20/20], Loss: 0.2315\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs20_lr0.001_embedding_dim64_dropout0.3_alpha0.05_beta0.01.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs20_lr0.001_embedding_dim64_dropout0.3_alpha0.05_beta0.01.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/20], Loss: 0.3153\n",
      "Epoch [2/20], Loss: 0.2828\n",
      "Epoch [3/20], Loss: 0.2790\n",
      "Epoch [4/20], Loss: 0.2768\n",
      "Epoch [5/20], Loss: 0.2756\n",
      "Epoch [6/20], Loss: 0.2740\n",
      "Epoch [7/20], Loss: 0.2724\n",
      "Epoch [8/20], Loss: 0.2703\n",
      "Epoch [9/20], Loss: 0.2685\n",
      "Epoch [10/20], Loss: 0.2661\n",
      "Epoch [11/20], Loss: 0.2638\n",
      "Epoch [12/20], Loss: 0.2616\n",
      "Epoch [13/20], Loss: 0.2591\n",
      "Epoch [14/20], Loss: 0.2567\n",
      "Epoch [15/20], Loss: 0.2543\n",
      "Epoch [16/20], Loss: 0.2519\n",
      "Epoch [17/20], Loss: 0.2494\n",
      "Epoch [18/20], Loss: 0.2470\n",
      "Epoch [19/20], Loss: 0.2447\n",
      "Epoch [20/20], Loss: 0.2424\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions without segmentation saved to ../results/wo_clustering_batch_size512_num_epochs20_lr0.001_embedding_dim64_dropout0.3_alpha0.05_beta0.01.csv\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/20], Loss: 0.6141\n",
      "Epoch [2/20], Loss: 0.4417\n",
      "Epoch [3/20], Loss: 0.4244\n",
      "Epoch [4/20], Loss: 0.4174\n",
      "Epoch [5/20], Loss: 0.4122\n",
      "Epoch [6/20], Loss: 0.4092\n",
      "Epoch [7/20], Loss: 0.4053\n",
      "Epoch [8/20], Loss: 0.4028\n",
      "Epoch [9/20], Loss: 0.4005\n",
      "Epoch [10/20], Loss: 0.3972\n",
      "Epoch [11/20], Loss: 0.3938\n",
      "Epoch [12/20], Loss: 0.3907\n",
      "Epoch [13/20], Loss: 0.3871\n",
      "Epoch [14/20], Loss: 0.3837\n",
      "Epoch [15/20], Loss: 0.3794\n",
      "Epoch [16/20], Loss: 0.3758\n",
      "Epoch [17/20], Loss: 0.3721\n",
      "Epoch [18/20], Loss: 0.3676\n",
      "Epoch [19/20], Loss: 0.3632\n",
      "Epoch [20/20], Loss: 0.3585\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 0 saved to ../results/cluster0_batch_size512_num_epochs20_lr0.001_embedding_dim64_dropout0.3_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/20], Loss: 0.3623\n",
      "Epoch [2/20], Loss: 0.2444\n",
      "Epoch [3/20], Loss: 0.2351\n",
      "Epoch [4/20], Loss: 0.2304\n",
      "Epoch [5/20], Loss: 0.2274\n",
      "Epoch [6/20], Loss: 0.2251\n",
      "Epoch [7/20], Loss: 0.2235\n",
      "Epoch [8/20], Loss: 0.2215\n",
      "Epoch [9/20], Loss: 0.2197\n",
      "Epoch [10/20], Loss: 0.2183\n",
      "Epoch [11/20], Loss: 0.2164\n",
      "Epoch [12/20], Loss: 0.2150\n",
      "Epoch [13/20], Loss: 0.2135\n",
      "Epoch [14/20], Loss: 0.2112\n",
      "Epoch [15/20], Loss: 0.2095\n",
      "Epoch [16/20], Loss: 0.2075\n",
      "Epoch [17/20], Loss: 0.2061\n",
      "Epoch [18/20], Loss: 0.2041\n",
      "Epoch [19/20], Loss: 0.2019\n",
      "Epoch [20/20], Loss: 0.1998\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 1 saved to ../results/cluster1_batch_size512_num_epochs20_lr0.001_embedding_dim64_dropout0.3_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/20], Loss: 0.4266\n",
      "Epoch [2/20], Loss: 0.2966\n",
      "Epoch [3/20], Loss: 0.2860\n",
      "Epoch [4/20], Loss: 0.2805\n",
      "Epoch [5/20], Loss: 0.2773\n",
      "Epoch [6/20], Loss: 0.2742\n",
      "Epoch [7/20], Loss: 0.2723\n",
      "Epoch [8/20], Loss: 0.2701\n",
      "Epoch [9/20], Loss: 0.2676\n",
      "Epoch [10/20], Loss: 0.2657\n",
      "Epoch [11/20], Loss: 0.2640\n",
      "Epoch [12/20], Loss: 0.2623\n",
      "Epoch [13/20], Loss: 0.2593\n",
      "Epoch [14/20], Loss: 0.2573\n",
      "Epoch [15/20], Loss: 0.2543\n",
      "Epoch [16/20], Loss: 0.2522\n",
      "Epoch [17/20], Loss: 0.2495\n",
      "Epoch [18/20], Loss: 0.2467\n",
      "Epoch [19/20], Loss: 0.2443\n",
      "Epoch [20/20], Loss: 0.2406\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 2 saved to ../results/cluster2_batch_size512_num_epochs20_lr0.001_embedding_dim64_dropout0.3_alpha0.05_beta0.05.csv\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/20], Loss: 0.4220\n",
      "Epoch [2/20], Loss: 0.2981\n",
      "Epoch [3/20], Loss: 0.2871\n",
      "Epoch [4/20], Loss: 0.2815\n",
      "Epoch [5/20], Loss: 0.2786\n",
      "Epoch [6/20], Loss: 0.2752\n",
      "Epoch [7/20], Loss: 0.2732\n",
      "Epoch [8/20], Loss: 0.2711\n",
      "Epoch [9/20], Loss: 0.2689\n",
      "Epoch [10/20], Loss: 0.2667\n",
      "Epoch [11/20], Loss: 0.2643\n",
      "Epoch [12/20], Loss: 0.2623\n",
      "Epoch [13/20], Loss: 0.2604\n",
      "Epoch [14/20], Loss: 0.2586\n",
      "Epoch [15/20], Loss: 0.2554\n",
      "Epoch [16/20], Loss: 0.2530\n",
      "Epoch [17/20], Loss: 0.2503\n",
      "Epoch [18/20], Loss: 0.2477\n",
      "Epoch [19/20], Loss: 0.2452\n",
      "Epoch [20/20], Loss: 0.2419\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions for cluster 3 saved to ../results/cluster3_batch_size512_num_epochs20_lr0.001_embedding_dim64_dropout0.3_alpha0.05_beta0.05.csv\n",
      "Predictions with segmentation saved to ../results/w_clustering_batch_size512_num_epochs20_lr0.001_embedding_dim64_dropout0.3_alpha0.05_beta0.05.csv\n",
      "----- Training -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/20], Loss: 0.3389\n",
      "Epoch [2/20], Loss: 0.2889\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "    'batch_size': [512, 256],\n",
    "    'num_epochs': [10, 20],\n",
    "    'lr': [0.001, 0.0001],\n",
    "    'embedding_dim': [64, 128],\n",
    "    'dropout': [0.3, 0.5],\n",
    "    'alpha': [0.01, 0.05],\n",
    "    'beta': [0.01, 0.05]\n",
    "}\n",
    "\n",
    "# Generate all possible combinations of hyperparameters\n",
    "param_combinations = list(itertools.product(*hyperparameters.values()))\n",
    "\n",
    "# Train for each combination of hyperparameters\n",
    "for params in param_combinations:\n",
    "    params_dict = {key: val for key, val in zip(hyperparameters.keys(), params)}\n",
    "    train_by_cluster_and_without(params_dict, train_processed, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NCF(nn.Module):\n",
    "#     def __init__(self, num_users, num_videos, embedding_dim, num_user_features, num_video_features):\n",
    "#         super(NCF, self).__init__()\n",
    "        \n",
    "#         # GMF Components for embeddings\n",
    "#         self.user_embeddings_gmf = nn.Embedding(num_users, embedding_dim)\n",
    "#         self.video_embeddings_gmf = nn.Embedding(num_videos, embedding_dim)\n",
    "\n",
    "#         # MLP Components for embeddings\n",
    "#         self.user_embeddings_mlp = nn.Embedding(num_users, embedding_dim)\n",
    "#         self.video_embeddings_mlp = nn.Embedding(num_videos, embedding_dim)\n",
    "\n",
    "#         # MLP layers for user and video embeddings\n",
    "#         self.fc1_mlp = nn.Linear(2 * embedding_dim + num_user_features + num_video_features, 128)\n",
    "#         self.fc2_mlp = nn.Linear(128, 64)\n",
    "\n",
    "#         # Final layers combining GMF, MLP for embeddings, and additional features\n",
    "#         self.fc1_combined = nn.Linear(embedding_dim + 64, 128)\n",
    "#         self.fc2_combined = nn.Linear(128, 1)\n",
    "\n",
    "#     def forward(self, user_id, video_id, user_features, video_features, dropout=0.5):\n",
    "#         ##### GMF Embedding branch\n",
    "#         user_emb_gmf = self.user_embeddings_gmf(user_id)\n",
    "#         video_emb_gmf = self.video_embeddings_gmf(video_id)\n",
    "#         gmf_output = user_emb_gmf * video_emb_gmf   # dimension: (batch_size, embedding_dim)\n",
    "\n",
    "#         ##### MLP Embedding branch\n",
    "#         user_emb_mlp = self.user_embeddings_mlp(user_id)\n",
    "#         video_emb_mlp = self.video_embeddings_mlp(video_id)\n",
    "\n",
    "#         mlp_input = torch.cat([user_emb_mlp, video_emb_mlp, user_features, video_features], dim=-1) \n",
    "\n",
    "#         # First fully connected layer with BatchNorm and ReLU\n",
    "#         mlp_output = self.fc1_mlp(mlp_input)\n",
    "#         # if self.training:\n",
    "#         #     mlp_output = nn.BatchNorm1d(128)(mlp_output)\n",
    "#         mlp_output = torch.relu(mlp_output)\n",
    "#         mlp_output = nn.Dropout(dropout)(mlp_output)\n",
    "\n",
    "#         # Second fully connected layer with BatchNorm and ReLU\n",
    "#         mlp_output = self.fc2_mlp(mlp_output)\n",
    "#         # if self.training:\n",
    "#         #     mlp_output = nn.BatchNorm1d(64)(mlp_output)\n",
    "#         mlp_output = torch.relu(mlp_output)\n",
    "#         mlp_output = nn.Dropout(dropout)(mlp_output)\n",
    "\n",
    "#         ##### Combine GMF, MLP\n",
    "#         combined_input = torch.cat([gmf_output, mlp_output], dim=-1)\n",
    "#         combined_output = self.fc1_combined(combined_input)\n",
    "#         # if self.training:\n",
    "#         #     combined_output = nn.BatchNorm1d(128)(combined_output)\n",
    "#         combined_output = torch.relu(combined_output)\n",
    "#         combined_output = nn.Dropout(dropout)(combined_output)\n",
    "\n",
    "#         ##### Scale wach_ratio to range [0, 5]\n",
    "#         combined_output = self.fc2_combined(combined_output)\n",
    "#         combined_output = torch.relu(combined_output)\n",
    "\n",
    "#         return combined_output.squeeze()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
