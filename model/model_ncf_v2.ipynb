{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the train and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training data: 2552082\n",
      "Total number of validation data: 1376299\n"
     ]
    }
   ],
   "source": [
    "root = '../KuaiRec 2.0/'\n",
    "\n",
    "# Training data\n",
    "train = pd.read_csv(root + \"data_exports/joined_train_data_segmented.csv\")\n",
    "val = pd.read_csv(root + \"data_exports/joined_val_data_FE.csv\")\n",
    "\n",
    "print(f'Total number of training data: {len(train)}')\n",
    "print(f'Total number of validation data: {len(val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>time</th>\n",
       "      <th>watch_ratio</th>\n",
       "      <th>user_active_degree</th>\n",
       "      <th>is_lowactive_period</th>\n",
       "      <th>is_live_streamer</th>\n",
       "      <th>is_video_author</th>\n",
       "      <th>follow_user_num</th>\n",
       "      <th>fans_user_num</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_daily_watch_time</th>\n",
       "      <th>top_3_categories</th>\n",
       "      <th>cluster</th>\n",
       "      <th>News_Politics</th>\n",
       "      <th>Auto_Tech</th>\n",
       "      <th>Lifestyle</th>\n",
       "      <th>Sports_Fitness</th>\n",
       "      <th>Entertainment</th>\n",
       "      <th>Culture</th>\n",
       "      <th>Others</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>148</td>\n",
       "      <td>2020-07-05 05:27:48.378</td>\n",
       "      <td>0.722103</td>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>8.360719e+12</td>\n",
       "      <td>['Car', 'Pets', 'Real estate家居']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>183</td>\n",
       "      <td>2020-07-05 05:28:00.057</td>\n",
       "      <td>1.907377</td>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>8.360719e+12</td>\n",
       "      <td>['Car', 'Pets', 'Real estate家居']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>3649</td>\n",
       "      <td>2020-07-05 05:29:09.479</td>\n",
       "      <td>2.063311</td>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>8.360719e+12</td>\n",
       "      <td>['Car', 'Pets', 'Real estate家居']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>5262</td>\n",
       "      <td>2020-07-05 05:30:43.285</td>\n",
       "      <td>0.566388</td>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>8.360719e+12</td>\n",
       "      <td>['Car', 'Pets', 'Real estate家居']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>8234</td>\n",
       "      <td>2020-07-05 05:35:43.459</td>\n",
       "      <td>0.418364</td>\n",
       "      <td>full_active</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>8.360719e+12</td>\n",
       "      <td>['Car', 'Pets', 'Real estate家居']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  video_id                     time  watch_ratio user_active_degree  \\\n",
       "0       14       148  2020-07-05 05:27:48.378     0.722103        full_active   \n",
       "1       14       183  2020-07-05 05:28:00.057     1.907377        full_active   \n",
       "2       14      3649  2020-07-05 05:29:09.479     2.063311        full_active   \n",
       "3       14      5262  2020-07-05 05:30:43.285     0.566388        full_active   \n",
       "4       14      8234  2020-07-05 05:35:43.459     0.418364        full_active   \n",
       "\n",
       "   is_lowactive_period  is_live_streamer  is_video_author  follow_user_num  \\\n",
       "0                    0                 0                1               73   \n",
       "1                    0                 0                1               73   \n",
       "2                    0                 0                1               73   \n",
       "3                    0                 0                1               73   \n",
       "4                    0                 0                1               73   \n",
       "\n",
       "   fans_user_num  ...  avg_daily_watch_time                  top_3_categories  \\\n",
       "0              6  ...          8.360719e+12  ['Car', 'Pets', 'Real estate家居']   \n",
       "1              6  ...          8.360719e+12  ['Car', 'Pets', 'Real estate家居']   \n",
       "2              6  ...          8.360719e+12  ['Car', 'Pets', 'Real estate家居']   \n",
       "3              6  ...          8.360719e+12  ['Car', 'Pets', 'Real estate家居']   \n",
       "4              6  ...          8.360719e+12  ['Car', 'Pets', 'Real estate家居']   \n",
       "\n",
       "   cluster News_Politics Auto_Tech  Lifestyle  Sports_Fitness  Entertainment  \\\n",
       "0        0             0         1          1               0              0   \n",
       "1        0             0         1          1               0              0   \n",
       "2        0             0         1          1               0              0   \n",
       "3        0             0         1          1               0              0   \n",
       "4        0             0         1          1               0              0   \n",
       "\n",
       "   Culture  Others  \n",
       "0        0       1  \n",
       "1        0       1  \n",
       "2        0       1  \n",
       "3        0       1  \n",
       "4        0       1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the current date\n",
    "This is necessary to calculate the age of the videos, which will be used for the time decay component of our model.\n",
    "We assume it to be the day of the latest interaction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current date: 2020-08-03\n",
      "Current date: 2020-08-19\n"
     ]
    }
   ],
   "source": [
    "# Convert type to datetime\n",
    "train['time'] = pd.to_datetime(train['time'])\n",
    "\n",
    "# Assume current date is the next day of the last date\n",
    "CURRENT_DATE_TRAIN = train['time'].dt.date.max()\n",
    "\n",
    "# Just the date portion\n",
    "print(f'Current date: {CURRENT_DATE_TRAIN}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate age of video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_info = pd.read_csv(root + 'data/item_daily_features.csv', usecols=['video_id', 'upload_dt']).drop_duplicates()\n",
    "\n",
    "video_info['upload_dt'] = pd.to_datetime(video_info['upload_dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chong\\AppData\\Local\\Temp\\ipykernel_49568\\1499493364.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  video_info_train['video_age'] = (CURRENT_DATE_TRAIN - video_info_train['upload_dt'].dt.date).dt.days\n"
     ]
    }
   ],
   "source": [
    "# Get video age for training data\n",
    "video_info_train = video_info[video_info['video_id'].isin(train['video_id'].unique())]\n",
    "video_info_train['video_age'] = (CURRENT_DATE_TRAIN - video_info_train['upload_dt'].dt.date).dt.days\n",
    "video_age_dict = video_info_train.set_index('video_id')['video_age'].to_dict()    # Convert to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'video_id', 'time', 'watch_ratio', 'user_active_degree',\n",
       "       'is_lowactive_period', 'is_live_streamer', 'is_video_author',\n",
       "       'follow_user_num', 'fans_user_num', 'friend_user_num', 'register_days',\n",
       "       'author_id', 'video_type', 'video_tag_name', 'video_duration',\n",
       "       'show_cnt', 'play_cnt', 'play_duration', 'like_cnt', 'comment_cnt',\n",
       "       'share_cnt', 'follow_cnt', 'collect_cnt', 'manual_cover_text',\n",
       "       'caption', 'topic_tag', 'first_level_category_name',\n",
       "       'second_level_category_name', 'third_level_category_name',\n",
       "       'english_caption', 'english_first_level_category_name',\n",
       "       'english_second_level_category_name',\n",
       "       'english_third_level_category_name', 'english_topic_tag', 'is_new_user',\n",
       "       'total_connections', 'is_content_creator', 'hour', 'day_of_week',\n",
       "       'watch_frequency', 'is_weekend_interaction', 'is_weekend',\n",
       "       'time_period', 'count_afternoon_views', 'count_evening_views',\n",
       "       'count_midnight_views', 'count_morning_views', 'avg_daily_watch_time',\n",
       "       'top_3_categories', 'cluster', 'News_Politics', 'Auto_Tech',\n",
       "       'Lifestyle', 'Sports_Fitness', 'Entertainment', 'Culture', 'Others'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for feeding into Neural Network portion of NCF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'video_id', 'time', 'watch_ratio', 'user_active_degree',\n",
       "       'is_lowactive_period', 'is_live_streamer', 'is_video_author',\n",
       "       'follow_user_num', 'fans_user_num', 'friend_user_num', 'register_days',\n",
       "       'author_id', 'video_type', 'video_tag_name', 'video_duration',\n",
       "       'show_cnt', 'play_cnt', 'play_duration', 'like_cnt', 'comment_cnt',\n",
       "       'share_cnt', 'follow_cnt', 'collect_cnt', 'manual_cover_text',\n",
       "       'caption', 'topic_tag', 'first_level_category_name',\n",
       "       'second_level_category_name', 'third_level_category_name',\n",
       "       'english_caption', 'english_first_level_category_name',\n",
       "       'english_second_level_category_name',\n",
       "       'english_third_level_category_name', 'english_topic_tag', 'is_new_user',\n",
       "       'total_connections', 'is_content_creator', 'hour', 'day_of_week',\n",
       "       'watch_frequency', 'is_weekend_interaction', 'is_weekend',\n",
       "       'time_period', 'count_afternoon_views', 'count_evening_views',\n",
       "       'count_midnight_views', 'count_morning_views', 'avg_daily_watch_time',\n",
       "       'top_3_categories', 'cluster', 'News_Politics', 'Auto_Tech',\n",
       "       'Lifestyle', 'Sports_Fitness', 'Entertainment', 'Culture', 'Others'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode 'user_active_degree', 'time_period'\n",
    "train_processed = pd.get_dummies(train, columns=['user_active_degree', 'time_period'])\n",
    "\n",
    "# Remove the column for user_active_degree = UNKNOWN\n",
    "train_processed = train_processed.drop(columns=['user_active_degree_UNKNOWN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed = train_processed.drop(columns=['author_id', 'video_type', \n",
    "                                    'video_tag_name', \n",
    "                                    'manual_cover_text', 'caption', 'topic_tag', \n",
    "                                    'first_level_category_name', 'second_level_category_name', 'third_level_category_name',\n",
    "                                    'english_caption', 'english_first_level_category_name',\n",
    "                                    'english_second_level_category_name',\n",
    "                                    'english_third_level_category_name', 'english_topic_tag',\n",
    "                                    'top_3_categories',\n",
    "                                    'play_duration', 'hour', 'day_of_week'\n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale continuous variables\n",
    "\n",
    "Below, we can see that the values are all on different scales. For example, follow_user_num is in the tens-thousands while like_cnt can range form millions to billions. This will affect the training of the model, therefore scaling is needed. \n",
    "\n",
    "Note that we scale the validation data with the scaler fitted to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>follow_user_num</th>\n",
       "      <th>fans_user_num</th>\n",
       "      <th>friend_user_num</th>\n",
       "      <th>register_days</th>\n",
       "      <th>video_duration</th>\n",
       "      <th>show_cnt</th>\n",
       "      <th>play_cnt</th>\n",
       "      <th>like_cnt</th>\n",
       "      <th>comment_cnt</th>\n",
       "      <th>share_cnt</th>\n",
       "      <th>follow_cnt</th>\n",
       "      <th>collect_cnt</th>\n",
       "      <th>count_afternoon_views</th>\n",
       "      <th>count_evening_views</th>\n",
       "      <th>count_midnight_views</th>\n",
       "      <th>count_morning_views</th>\n",
       "      <th>avg_daily_watch_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.381411e+01</td>\n",
       "      <td>3.872561e+00</td>\n",
       "      <td>1.331606e+00</td>\n",
       "      <td>2.653341e+02</td>\n",
       "      <td>1.164791e+04</td>\n",
       "      <td>6.959049e+06</td>\n",
       "      <td>7.052437e+06</td>\n",
       "      <td>2.044780e+05</td>\n",
       "      <td>8.935899e+03</td>\n",
       "      <td>3.805251e+03</td>\n",
       "      <td>2.093272e+04</td>\n",
       "      <td>2.858760e+02</td>\n",
       "      <td>4.658341e+02</td>\n",
       "      <td>2.809108e+02</td>\n",
       "      <td>4.579366e+02</td>\n",
       "      <td>6.100598e+02</td>\n",
       "      <td>8.062631e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.418902e+02</td>\n",
       "      <td>9.716679e+00</td>\n",
       "      <td>4.924868e+00</td>\n",
       "      <td>2.640708e+02</td>\n",
       "      <td>1.344116e+04</td>\n",
       "      <td>9.275605e+06</td>\n",
       "      <td>9.511481e+06</td>\n",
       "      <td>3.209431e+05</td>\n",
       "      <td>2.111983e+04</td>\n",
       "      <td>1.269530e+04</td>\n",
       "      <td>6.331006e+04</td>\n",
       "      <td>1.337505e+03</td>\n",
       "      <td>2.844922e+02</td>\n",
       "      <td>2.385123e+02</td>\n",
       "      <td>4.339834e+02</td>\n",
       "      <td>3.305712e+02</td>\n",
       "      <td>7.068827e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>3.066000e+03</td>\n",
       "      <td>6.440000e+02</td>\n",
       "      <td>3.310000e+02</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.632392e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.190000e+02</td>\n",
       "      <td>7.333000e+03</td>\n",
       "      <td>8.329130e+05</td>\n",
       "      <td>7.629220e+05</td>\n",
       "      <td>1.552800e+04</td>\n",
       "      <td>3.450000e+02</td>\n",
       "      <td>6.400000e+01</td>\n",
       "      <td>1.002000e+03</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>2.490000e+02</td>\n",
       "      <td>8.400000e+01</td>\n",
       "      <td>5.300000e+01</td>\n",
       "      <td>3.740000e+02</td>\n",
       "      <td>7.686325e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.500000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+02</td>\n",
       "      <td>9.383000e+03</td>\n",
       "      <td>3.127692e+06</td>\n",
       "      <td>3.071419e+06</td>\n",
       "      <td>7.359000e+04</td>\n",
       "      <td>2.171000e+03</td>\n",
       "      <td>4.140000e+02</td>\n",
       "      <td>4.968000e+03</td>\n",
       "      <td>2.800000e+01</td>\n",
       "      <td>4.440000e+02</td>\n",
       "      <td>2.250000e+02</td>\n",
       "      <td>3.560000e+02</td>\n",
       "      <td>5.690000e+02</td>\n",
       "      <td>8.158000e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.300000e+01</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.020000e+02</td>\n",
       "      <td>1.150000e+04</td>\n",
       "      <td>9.372330e+06</td>\n",
       "      <td>9.544620e+06</td>\n",
       "      <td>2.512090e+05</td>\n",
       "      <td>8.918000e+03</td>\n",
       "      <td>2.275000e+03</td>\n",
       "      <td>1.797800e+04</td>\n",
       "      <td>1.330000e+02</td>\n",
       "      <td>6.560000e+02</td>\n",
       "      <td>4.190000e+02</td>\n",
       "      <td>7.480000e+02</td>\n",
       "      <td>8.060000e+02</td>\n",
       "      <td>8.518700e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.811000e+03</td>\n",
       "      <td>2.510000e+02</td>\n",
       "      <td>7.100000e+01</td>\n",
       "      <td>2.002000e+03</td>\n",
       "      <td>2.945200e+05</td>\n",
       "      <td>6.525508e+07</td>\n",
       "      <td>6.479578e+07</td>\n",
       "      <td>2.762854e+06</td>\n",
       "      <td>3.383650e+05</td>\n",
       "      <td>2.061050e+05</td>\n",
       "      <td>1.215372e+06</td>\n",
       "      <td>2.919700e+04</td>\n",
       "      <td>1.477000e+03</td>\n",
       "      <td>1.435000e+03</td>\n",
       "      <td>1.852000e+03</td>\n",
       "      <td>1.727000e+03</td>\n",
       "      <td>1.277244e+13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       follow_user_num  fans_user_num  friend_user_num  register_days  \\\n",
       "count     2.552082e+06   2.552082e+06     2.552082e+06   2.552082e+06   \n",
       "mean      5.381411e+01   3.872561e+00     1.331606e+00   2.653341e+02   \n",
       "std       1.418902e+02   9.716679e+00     4.924868e+00   2.640708e+02   \n",
       "min       0.000000e+00   0.000000e+00     0.000000e+00   8.000000e+00   \n",
       "25%       7.000000e+00   0.000000e+00     0.000000e+00   1.190000e+02   \n",
       "50%       1.500000e+01   1.000000e+00     0.000000e+00   2.000000e+02   \n",
       "75%       4.300000e+01   4.000000e+00     1.000000e+00   3.020000e+02   \n",
       "max       1.811000e+03   2.510000e+02     7.100000e+01   2.002000e+03   \n",
       "\n",
       "       video_duration      show_cnt      play_cnt      like_cnt   comment_cnt  \\\n",
       "count    2.552082e+06  2.552082e+06  2.552082e+06  2.552082e+06  2.552082e+06   \n",
       "mean     1.164791e+04  6.959049e+06  7.052437e+06  2.044780e+05  8.935899e+03   \n",
       "std      1.344116e+04  9.275605e+06  9.511481e+06  3.209431e+05  2.111983e+04   \n",
       "min      3.066000e+03  6.440000e+02  3.310000e+02  2.000000e+00  0.000000e+00   \n",
       "25%      7.333000e+03  8.329130e+05  7.629220e+05  1.552800e+04  3.450000e+02   \n",
       "50%      9.383000e+03  3.127692e+06  3.071419e+06  7.359000e+04  2.171000e+03   \n",
       "75%      1.150000e+04  9.372330e+06  9.544620e+06  2.512090e+05  8.918000e+03   \n",
       "max      2.945200e+05  6.525508e+07  6.479578e+07  2.762854e+06  3.383650e+05   \n",
       "\n",
       "          share_cnt    follow_cnt   collect_cnt  count_afternoon_views  \\\n",
       "count  2.552082e+06  2.552082e+06  2.552082e+06           2.552082e+06   \n",
       "mean   3.805251e+03  2.093272e+04  2.858760e+02           4.658341e+02   \n",
       "std    1.269530e+04  6.331006e+04  1.337505e+03           2.844922e+02   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00           0.000000e+00   \n",
       "25%    6.400000e+01  1.002000e+03  5.000000e+00           2.490000e+02   \n",
       "50%    4.140000e+02  4.968000e+03  2.800000e+01           4.440000e+02   \n",
       "75%    2.275000e+03  1.797800e+04  1.330000e+02           6.560000e+02   \n",
       "max    2.061050e+05  1.215372e+06  2.919700e+04           1.477000e+03   \n",
       "\n",
       "       count_evening_views  count_midnight_views  count_morning_views  \\\n",
       "count         2.552082e+06          2.552082e+06         2.552082e+06   \n",
       "mean          2.809108e+02          4.579366e+02         6.100598e+02   \n",
       "std           2.385123e+02          4.339834e+02         3.305712e+02   \n",
       "min           0.000000e+00          0.000000e+00         0.000000e+00   \n",
       "25%           8.400000e+01          5.300000e+01         3.740000e+02   \n",
       "50%           2.250000e+02          3.560000e+02         5.690000e+02   \n",
       "75%           4.190000e+02          7.480000e+02         8.060000e+02   \n",
       "max           1.435000e+03          1.852000e+03         1.727000e+03   \n",
       "\n",
       "       avg_daily_watch_time  \n",
       "count          2.552082e+06  \n",
       "mean           8.062631e+12  \n",
       "std            7.068827e+11  \n",
       "min            4.632392e+12  \n",
       "25%            7.686325e+12  \n",
       "50%            8.158000e+12  \n",
       "75%            8.518700e+12  \n",
       "max            1.277244e+13  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_processed[['follow_user_num',\n",
    "       'fans_user_num', 'friend_user_num', 'register_days', 'video_duration',\n",
    "       'show_cnt', 'play_cnt', 'like_cnt', 'comment_cnt',\n",
    "       'share_cnt', 'follow_cnt', 'collect_cnt', 'count_afternoon_views', 'count_evening_views', 'count_midnight_views',\n",
    "       'count_morning_views', 'avg_daily_watch_time']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "columns_to_scale = ['follow_user_num',\n",
    "       'fans_user_num', 'friend_user_num', 'register_days', 'video_duration',\n",
    "       'show_cnt', 'play_cnt', \n",
    "       'like_cnt', 'comment_cnt',\n",
    "       'share_cnt', 'follow_cnt', 'collect_cnt', \n",
    "       'total_connections',\n",
    "       'watch_frequency', \n",
    "       'count_afternoon_views', 'count_evening_views', 'count_midnight_views',\n",
    "       'count_morning_views', \n",
    "       'avg_daily_watch_time', \n",
    "       ]\n",
    "\n",
    "train_processed[columns_to_scale] = scaler.fit_transform(train_processed[columns_to_scale])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see that the mean of all the columns is (close to) 0 and the standard deviation is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>follow_user_num</th>\n",
       "      <th>fans_user_num</th>\n",
       "      <th>friend_user_num</th>\n",
       "      <th>register_days</th>\n",
       "      <th>video_duration</th>\n",
       "      <th>show_cnt</th>\n",
       "      <th>play_cnt</th>\n",
       "      <th>like_cnt</th>\n",
       "      <th>comment_cnt</th>\n",
       "      <th>share_cnt</th>\n",
       "      <th>follow_cnt</th>\n",
       "      <th>collect_cnt</th>\n",
       "      <th>count_afternoon_views</th>\n",
       "      <th>count_evening_views</th>\n",
       "      <th>count_midnight_views</th>\n",
       "      <th>count_morning_views</th>\n",
       "      <th>avg_daily_watch_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "      <td>2.552082e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.875563e-18</td>\n",
       "      <td>-3.162816e-17</td>\n",
       "      <td>1.933327e-17</td>\n",
       "      <td>-6.521637e-17</td>\n",
       "      <td>-2.266133e-16</td>\n",
       "      <td>4.790998e-17</td>\n",
       "      <td>8.174320e-18</td>\n",
       "      <td>-2.525798e-17</td>\n",
       "      <td>-1.905485e-17</td>\n",
       "      <td>-1.854256e-18</td>\n",
       "      <td>-9.449469e-18</td>\n",
       "      <td>1.789107e-17</td>\n",
       "      <td>-5.862346e-17</td>\n",
       "      <td>-3.456824e-17</td>\n",
       "      <td>2.940082e-17</td>\n",
       "      <td>1.021901e-16</td>\n",
       "      <td>-2.135391e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.792660e-01</td>\n",
       "      <td>-3.985478e-01</td>\n",
       "      <td>-2.703841e-01</td>\n",
       "      <td>-9.744891e-01</td>\n",
       "      <td>-6.384799e-01</td>\n",
       "      <td>-7.501836e-01</td>\n",
       "      <td>-7.414311e-01</td>\n",
       "      <td>-6.371098e-01</td>\n",
       "      <td>-4.231048e-01</td>\n",
       "      <td>-2.997370e-01</td>\n",
       "      <td>-3.306382e-01</td>\n",
       "      <td>-2.137384e-01</td>\n",
       "      <td>-1.637424e+00</td>\n",
       "      <td>-1.177763e+00</td>\n",
       "      <td>-1.055194e+00</td>\n",
       "      <td>-1.845472e+00</td>\n",
       "      <td>-4.852628e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-3.299320e-01</td>\n",
       "      <td>-3.985478e-01</td>\n",
       "      <td>-2.703841e-01</td>\n",
       "      <td>-5.541473e-01</td>\n",
       "      <td>-3.210220e-01</td>\n",
       "      <td>-6.604569e-01</td>\n",
       "      <td>-6.612552e-01</td>\n",
       "      <td>-5.887337e-01</td>\n",
       "      <td>-4.067695e-01</td>\n",
       "      <td>-2.946957e-01</td>\n",
       "      <td>-3.148114e-01</td>\n",
       "      <td>-2.100001e-01</td>\n",
       "      <td>-7.621797e-01</td>\n",
       "      <td>-8.255794e-01</td>\n",
       "      <td>-9.330694e-01</td>\n",
       "      <td>-7.140968e-01</td>\n",
       "      <td>-5.323456e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-2.735504e-01</td>\n",
       "      <td>-2.956320e-01</td>\n",
       "      <td>-2.703841e-01</td>\n",
       "      <td>-2.474113e-01</td>\n",
       "      <td>-1.685054e-01</td>\n",
       "      <td>-4.130575e-01</td>\n",
       "      <td>-4.185488e-01</td>\n",
       "      <td>-4.078230e-01</td>\n",
       "      <td>-3.203104e-01</td>\n",
       "      <td>-2.671265e-01</td>\n",
       "      <td>-2.521673e-01</td>\n",
       "      <td>-1.928039e-01</td>\n",
       "      <td>-7.674775e-02</td>\n",
       "      <td>-2.344147e-01</td>\n",
       "      <td>-2.348860e-01</td>\n",
       "      <td>-1.242086e-01</td>\n",
       "      <td>1.349154e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-7.621467e-02</td>\n",
       "      <td>1.311554e-02</td>\n",
       "      <td>-6.733294e-02</td>\n",
       "      <td>1.388488e-01</td>\n",
       "      <td>-1.100400e-02</td>\n",
       "      <td>2.601751e-01</td>\n",
       "      <td>2.620184e-01</td>\n",
       "      <td>1.456054e-01</td>\n",
       "      <td>-8.475032e-04</td>\n",
       "      <td>-1.205368e-01</td>\n",
       "      <td>-4.667066e-02</td>\n",
       "      <td>-1.142995e-01</td>\n",
       "      <td>6.684398e-01</td>\n",
       "      <td>5.789608e-01</td>\n",
       "      <td>6.683745e-01</td>\n",
       "      <td>5.927325e-01</td>\n",
       "      <td>6.451838e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.238413e+01</td>\n",
       "      <td>2.543333e+01</td>\n",
       "      <td>1.414625e+01</td>\n",
       "      <td>6.576517e+00</td>\n",
       "      <td>2.104522e+01</td>\n",
       "      <td>6.284877e+00</td>\n",
       "      <td>6.070911e+00</td>\n",
       "      <td>7.971434e+00</td>\n",
       "      <td>1.559810e+01</td>\n",
       "      <td>1.593501e+01</td>\n",
       "      <td>1.886651e+01</td>\n",
       "      <td>2.161572e+01</td>\n",
       "      <td>3.554284e+00</td>\n",
       "      <td>4.838701e+00</td>\n",
       "      <td>3.212251e+00</td>\n",
       "      <td>3.378820e+00</td>\n",
       "      <td>6.662791e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       follow_user_num  fans_user_num  friend_user_num  register_days  \\\n",
       "count     2.552082e+06   2.552082e+06     2.552082e+06   2.552082e+06   \n",
       "mean      3.875563e-18  -3.162816e-17     1.933327e-17  -6.521637e-17   \n",
       "std       1.000000e+00   1.000000e+00     1.000000e+00   1.000000e+00   \n",
       "min      -3.792660e-01  -3.985478e-01    -2.703841e-01  -9.744891e-01   \n",
       "25%      -3.299320e-01  -3.985478e-01    -2.703841e-01  -5.541473e-01   \n",
       "50%      -2.735504e-01  -2.956320e-01    -2.703841e-01  -2.474113e-01   \n",
       "75%      -7.621467e-02   1.311554e-02    -6.733294e-02   1.388488e-01   \n",
       "max       1.238413e+01   2.543333e+01     1.414625e+01   6.576517e+00   \n",
       "\n",
       "       video_duration      show_cnt      play_cnt      like_cnt   comment_cnt  \\\n",
       "count    2.552082e+06  2.552082e+06  2.552082e+06  2.552082e+06  2.552082e+06   \n",
       "mean    -2.266133e-16  4.790998e-17  8.174320e-18 -2.525798e-17 -1.905485e-17   \n",
       "std      1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min     -6.384799e-01 -7.501836e-01 -7.414311e-01 -6.371098e-01 -4.231048e-01   \n",
       "25%     -3.210220e-01 -6.604569e-01 -6.612552e-01 -5.887337e-01 -4.067695e-01   \n",
       "50%     -1.685054e-01 -4.130575e-01 -4.185488e-01 -4.078230e-01 -3.203104e-01   \n",
       "75%     -1.100400e-02  2.601751e-01  2.620184e-01  1.456054e-01 -8.475032e-04   \n",
       "max      2.104522e+01  6.284877e+00  6.070911e+00  7.971434e+00  1.559810e+01   \n",
       "\n",
       "          share_cnt    follow_cnt   collect_cnt  count_afternoon_views  \\\n",
       "count  2.552082e+06  2.552082e+06  2.552082e+06           2.552082e+06   \n",
       "mean  -1.854256e-18 -9.449469e-18  1.789107e-17          -5.862346e-17   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00           1.000000e+00   \n",
       "min   -2.997370e-01 -3.306382e-01 -2.137384e-01          -1.637424e+00   \n",
       "25%   -2.946957e-01 -3.148114e-01 -2.100001e-01          -7.621797e-01   \n",
       "50%   -2.671265e-01 -2.521673e-01 -1.928039e-01          -7.674775e-02   \n",
       "75%   -1.205368e-01 -4.667066e-02 -1.142995e-01           6.684398e-01   \n",
       "max    1.593501e+01  1.886651e+01  2.161572e+01           3.554284e+00   \n",
       "\n",
       "       count_evening_views  count_midnight_views  count_morning_views  \\\n",
       "count         2.552082e+06          2.552082e+06         2.552082e+06   \n",
       "mean         -3.456824e-17          2.940082e-17         1.021901e-16   \n",
       "std           1.000000e+00          1.000000e+00         1.000000e+00   \n",
       "min          -1.177763e+00         -1.055194e+00        -1.845472e+00   \n",
       "25%          -8.255794e-01         -9.330694e-01        -7.140968e-01   \n",
       "50%          -2.344147e-01         -2.348860e-01        -1.242086e-01   \n",
       "75%           5.789608e-01          6.683745e-01         5.927325e-01   \n",
       "max           4.838701e+00          3.212251e+00         3.378820e+00   \n",
       "\n",
       "       avg_daily_watch_time  \n",
       "count          2.552082e+06  \n",
       "mean          -2.135391e-15  \n",
       "std            1.000000e+00  \n",
       "min           -4.852628e+00  \n",
       "25%           -5.323456e-01  \n",
       "50%            1.349154e-01  \n",
       "75%            6.451838e-01  \n",
       "max            6.662791e+00  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_processed[['follow_user_num',\n",
    "       'fans_user_num', 'friend_user_num', 'register_days', 'video_duration',\n",
    "       'show_cnt', 'play_cnt', 'like_cnt', 'comment_cnt',\n",
    "       'share_cnt', 'follow_cnt', 'collect_cnt', 'count_afternoon_views', 'count_evening_views', 'count_midnight_views',\n",
    "       'count_morning_views', 'avg_daily_watch_time']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KuaiShouDataset(Dataset):\n",
    "    def __init__(self, data, user_id_col, video_id_col, user_feature_cols, video_feature_cols, watch_ratio_col, video_age_dict):\n",
    "        self.user_feature_cols = user_feature_cols\n",
    "        self.video_feature_cols = video_feature_cols\n",
    "\n",
    "        # Initialise and fit LabelEncoders\n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.video_encoder = LabelEncoder()\n",
    "        \n",
    "        self.user_indices = torch.tensor(self.user_encoder.fit_transform(data[user_id_col]), dtype=torch.long)\n",
    "        self.video_indices = torch.tensor(self.video_encoder.fit_transform(data[video_id_col]), dtype=torch.long)\n",
    "\n",
    "        # Convert user and video features and watch ratios to tensors\n",
    "        self.user_features = torch.tensor(data[user_feature_cols].values, dtype=torch.float32)\n",
    "        self.video_features = torch.tensor(data[video_feature_cols].values, dtype=torch.float32)\n",
    "        self.watch_ratios = torch.tensor(data[watch_ratio_col].values, dtype=torch.float32)\n",
    "\n",
    "        # Time related features\n",
    "        self.video_age_dict = video_age_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.user_indices[idx], self.video_indices[idx], self.user_features[idx], self.video_features[idx], self.watch_ratios[idx]\n",
    "\n",
    "    def inverse_transform_user_ids(self, encoded_user_idx):\n",
    "        \"\"\"Decode encoded user indices to original user_ids.\"\"\"\n",
    "        return self.user_encoder.inverse_transform(encoded_user_idx)\n",
    "    \n",
    "    def inverse_transform_video_ids(self, encoded_video_idx):\n",
    "        \"\"\"Decode encoded video indices to original video_ids.\"\"\"\n",
    "        return self.video_encoder.inverse_transform(encoded_video_idx)\n",
    "    \n",
    "    def get_video_age(self, video_idx):\n",
    "        \"\"\"Get video age.\"\"\"\n",
    "        video_ids = self.inverse_transform_video_ids(video_idx)\n",
    "\n",
    "        ages = []\n",
    "        for i in range(len(video_idx)):\n",
    "            ages.append(self.video_age_dict[video_ids[i]])\n",
    "        return torch.tensor(ages, dtype=torch.float32)\n",
    "    \n",
    "    def get_decoded_user_video_pairs(self):\n",
    "        \"\"\"Get decoded user-video pairs.\"\"\"\n",
    "        return self.inverse_transform_user_ids(self.user_indices), self.inverse_transform_video_ids(self.video_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Infused Neural Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_videos, embedding_dim, num_user_features, num_video_features, dropout):\n",
    "        super(NCF, self).__init__()\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # GMF Components for embeddings\n",
    "        self.user_embeddings_gmf = nn.Embedding(num_users, embedding_dim)\n",
    "        self.video_embeddings_gmf = nn.Embedding(num_videos, embedding_dim)\n",
    "\n",
    "        # MLP Components for embeddings\n",
    "        self.user_embeddings_mlp = nn.Embedding(num_users, embedding_dim)\n",
    "        self.video_embeddings_mlp = nn.Embedding(num_videos, embedding_dim)\n",
    "\n",
    "        # MLP layers for user and video embeddings\n",
    "        self.fc1_mlp = nn.Linear(2 * embedding_dim, 128)\n",
    "        self.fc2_mlp = nn.Linear(128, 64)\n",
    "\n",
    "        # MLP layers for user and video features\n",
    "        self.user_video_features_fc = nn.Linear(num_user_features + num_video_features, 64)\n",
    "\n",
    "        # Final layers combining GMF, MLP for embeddings, and additional features\n",
    "        self.fc1_combined = nn.Linear(embedding_dim + 64 + 64, 128)\n",
    "        self.fc2_combined = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, user_idx, video_idx, user_features, video_features):\n",
    "        ####### GMF Embedding branch #######\n",
    "        user_emb_gmf = self.user_embeddings_gmf(user_idx)\n",
    "        video_emb_gmf = self.video_embeddings_gmf(video_idx)\n",
    "        gmf_output = user_emb_gmf * video_emb_gmf                                   # dimension: (batch_size, embedding_dim)\n",
    "\n",
    "        ####### MLP Embedding branch #######\n",
    "        user_emb_mlp = self.user_embeddings_mlp(user_idx)\n",
    "        video_emb_mlp = self.video_embeddings_mlp(video_idx)\n",
    "        mlp_input = torch.cat([user_emb_mlp, video_emb_mlp], dim=-1)                # dimension: (batch_size, 2 * embedding_dim)\n",
    "\n",
    "        # First fully connected layer with BatchNorm and ReLU\n",
    "        mlp_output = self.fc1_mlp(mlp_input)\n",
    "        if self.training:\n",
    "            mlp_output = nn.BatchNorm1d(128)(mlp_output)\n",
    "        mlp_output = torch.relu(mlp_output)\n",
    "        mlp_output = nn.Dropout(self.dropout)(mlp_output)\n",
    "\n",
    "        # Second fully connected layer with BatchNorm and ReLU\n",
    "        mlp_output = self.fc2_mlp(mlp_output)                                       # dimension: (batch_size, 64)\n",
    "        if self.training:\n",
    "            mlp_output = nn.BatchNorm1d(64)(mlp_output)\n",
    "        mlp_output = torch.relu(mlp_output)\n",
    "        mlp_output = nn.Dropout(self.dropout)(mlp_output)\n",
    "\n",
    "        ####### MLP Feature processing branch #######\n",
    "        user_video_features = torch.cat([user_features, video_features], dim=-1)\n",
    "        user_video_features_processed = self.user_video_features_fc(user_video_features)  # dimension: (batch_size, 64)\n",
    "        user_video_features_processed = torch.relu(user_video_features_processed)\n",
    "        user_video_features_processed = nn.Dropout(self.dropout)(user_video_features_processed)\n",
    "\n",
    "        ####### Combine GMF, MLP, and additional features #######\n",
    "        combined_input = torch.cat([gmf_output, mlp_output, user_video_features_processed], dim=-1)\n",
    "        combined_output = self.fc1_combined(combined_input)\n",
    "        combined_output = torch.relu(combined_output)\n",
    "        combined_output = nn.Dropout(self.dropout)(combined_output)\n",
    "\n",
    "        combined_output = self.fc2_combined(combined_output)\n",
    "        combined_output = torch.sigmoid(combined_output) * 5\n",
    "        \n",
    "        return combined_output.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KuaiShou_NCF_RecSys:\n",
    "    def __init__(self, dataset_train: KuaiShouDataset, model: nn.Module, embedding_dim: int, dropout: float, decay: float):\n",
    "        self.dataset_train = dataset_train\n",
    "        self.num_users = len(dataset_train.user_encoder.classes_)\n",
    "        self.num_videos = len(dataset_train.video_encoder.classes_)\n",
    "        self.num_user_features = len(dataset_train.user_feature_cols)\n",
    "        self.num_video_features = len(dataset_train.video_feature_cols)\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Move model to GPU if available\n",
    "        \n",
    "        # Initialise the model\n",
    "        self.model: nn.Module = model(self.num_users, self.num_videos, embedding_dim, self.num_user_features, self.num_video_features, dropout)\n",
    "\n",
    "        # Time decay constants\n",
    "        self.decay = decay\n",
    "\n",
    "    def train(self, batch_size: int, num_epochs: int, lr: int, criterion, optimizer):\n",
    "        # Initialise the DataLoader\n",
    "        train_loader = DataLoader(self.dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        print(f\"Model moved to {self.device}\")\n",
    "\n",
    "        # Optimizer and loss function\n",
    "        optimizer = optimizer(self.model.parameters(), lr=lr)\n",
    "        criterion = criterion\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            \n",
    "            for user_idx, video_idx, user_features, video_features, watch_ratio in train_loader:\n",
    "                user_idx, video_idx, user_features, video_features, watch_ratio = user_idx.to(self.device), video_idx.to(self.device), user_features.to(self.device), video_features.to(self.device), watch_ratio.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(user_idx, video_idx, user_features, video_features)\n",
    "                loss = criterion(outputs, watch_ratio)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Accumulate loss for reporting\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            # Print loss for each epoch\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    def predict(self, user_ids, video_ids, batch_size=1024):\n",
    "        \"\"\"\n",
    "        Generates a dataframe with predicted watch ratios for each user-video pair in batches.\n",
    "        \"\"\"\n",
    "        self.model.eval()  # Set model to evaluation mode\n",
    "        predictions_df = pd.DataFrame(columns=['user_id', 'video_id', 'watch_ratio'])\n",
    "\n",
    "        # Transform user and video ids to the same indices as the training data\n",
    "        user_indices = self.dataset_train.user_encoder.transform(user_ids)\n",
    "        video_indices = self.dataset_train.video_encoder.transform(video_ids)\n",
    "\n",
    "        # Batch prediction\n",
    "        for start_idx in range(0, len(user_ids), batch_size):\n",
    "            end_idx = min(start_idx + batch_size, len(user_ids))\n",
    "\n",
    "            batch_user_indices = user_indices[start_idx:end_idx]\n",
    "            batch_video_indices = video_indices[start_idx:end_idx]\n",
    "\n",
    "            # Prepare batch of user and video indices\n",
    "            user_batch = torch.tensor(batch_user_indices, dtype=torch.long).to(self.device)\n",
    "            video_batch = torch.tensor(batch_video_indices, dtype=torch.long).to(self.device)\n",
    "            \n",
    "            # Get user and video features from validation data\n",
    "            user_features_batch = self.dataset_train.user_features[batch_user_indices].to(self.device)\n",
    "            video_features_batch = self.dataset_train.video_features[batch_video_indices].to(self.device)\n",
    "            \n",
    "            # Get video age in batch\n",
    "            video_age_batch = self.dataset_train.get_video_age(batch_video_indices).to(self.device)\n",
    "\n",
    "            # Predict in batch\n",
    "            with torch.no_grad():\n",
    "                predicted_watch_ratios = self.model(user_batch, video_batch, user_features_batch, video_features_batch)\n",
    "\n",
    "            # Apply time decay\n",
    "            decay_weights = self.calculate_exponential_weight(video_age_batch)\n",
    "            predicted_watch_ratios = predicted_watch_ratios * decay_weights\n",
    "\n",
    "            # Append predictions to DataFrame\n",
    "            batch_predictions_df = pd.DataFrame({'user_id': self.dataset_train.inverse_transform_user_ids(batch_user_indices),\n",
    "                                                 'video_id': self.dataset_train.inverse_transform_video_ids(batch_video_indices),\n",
    "                                                 'watch_ratio': predicted_watch_ratios.cpu().numpy()})\n",
    "            predictions_df = pd.concat([predictions_df, batch_predictions_df])\n",
    "            \n",
    "        return predictions_df\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        \"\"\"\n",
    "        Returns the model parameters.\n",
    "        \"\"\"\n",
    "        return self.model.state_dict()\n",
    "    \n",
    "    def calculate_exponential_weight(self, video_age_days):\n",
    "        \"\"\"\n",
    "        Returns the decay weight based on the defined decay constant and the number of days since the video has been uploaded.\n",
    "        \"\"\"\n",
    "        return torch.exp(-self.decay * video_age_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Training Data to the Model and Generating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns for user and video features in the user-item interaction data\n",
    "user_cols = ['is_lowactive_period',\n",
    "             'is_live_streamer', 'is_video_author', 'follow_user_num',\n",
    "             'fans_user_num', 'friend_user_num', 'register_days', 'is_new_user',\n",
    "             'total_connections', 'is_content_creator',\n",
    "             'watch_frequency', 'is_weekend_interaction', 'is_weekend',\n",
    "             'count_afternoon_views', 'count_evening_views', 'count_midnight_views', 'count_morning_views', \n",
    "             'avg_daily_watch_time', \n",
    "             'user_active_degree_full_active', 'user_active_degree_high_active', 'user_active_degree_middle_active', \n",
    "             'time_period_afternoon', 'time_period_evening', 'time_period_midnight', 'time_period_morning'\n",
    "            ]\n",
    "video_cols = ['video_duration', 'show_cnt', 'play_cnt', \n",
    "              'like_cnt', 'comment_cnt', 'share_cnt', 'follow_cnt', 'collect_cnt', \n",
    "              'News_Politics', 'Auto_Tech', 'Lifestyle', 'Sports_Fitness', 'Entertainment', 'Culture', 'Others',\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function which allows us to train and predict using the NCF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(hyperparameters: dict, train_data: pd.DataFrame, val_data: pd.DataFrame, video_age_train_dict, **kwargs):\n",
    "    cluster = kwargs.get('cluster', None)\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    BATCH_SIZE = hyperparameters['batch_size']\n",
    "    NUM_EPOCHS = hyperparameters['num_epochs']\n",
    "    LEARNING_RATE = hyperparameters['lr']\n",
    "    EMBEDDING_DIM = hyperparameters['embedding_dim']\n",
    "    DROPOUT = hyperparameters['dropout']\n",
    "    DECAY = hyperparameters['decay']\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimiser = optim.Adam\n",
    "\n",
    "    print(f\"----- Training {'' if cluster == None else f'for cluster {cluster} '}-----\")\n",
    "\n",
    "    # Create the dataset\n",
    "    dataset_train = KuaiShouDataset(train_data, 'user_id', 'video_id', user_cols, video_cols, 'watch_ratio', video_age_train_dict)\n",
    "\n",
    "    # Initialise the NCF model\n",
    "    print(\"Initialising...\")\n",
    "    ncf_rec_sys = KuaiShou_NCF_RecSys(dataset_train, NCF, EMBEDDING_DIM, DROPOUT, DECAY)\n",
    "\n",
    "    # Train on data\n",
    "    ncf_rec_sys.train(BATCH_SIZE, NUM_EPOCHS, LEARNING_RATE, criterion, optimiser)\n",
    "\n",
    "    # Generate predictions\n",
    "    print(\"Generating predictions...\")\n",
    "    \n",
    "    # Filter as we can only predict for users and videos that are in the training data\n",
    "    users = train_data['user_id'].unique()\n",
    "    videos = train_data['video_id'].unique()\n",
    "    val_data = val_data[val_data['user_id'].isin(users) & val_data['video_id'].isin(videos)]\n",
    "\n",
    "    predictions_df = ncf_rec_sys.predict(val_data['user_id'], val_data['video_id'])\n",
    "    \n",
    "    print(\"Complete!\")\n",
    "    return cluster, predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Fitting to Cluster 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/2], Loss: 0.4148\n",
      "Epoch [2/2], Loss: 0.3706\n",
      "Generating predictions...\n",
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'batch_size': 512,\n",
    "    'num_epochs': 2,\n",
    "    'lr': 0.001,\n",
    "    'embedding_dim': 64,\n",
    "    'dropout': 0.3,\n",
    "    'decay': 0.01\n",
    "}\n",
    "\n",
    "cluster = 0\n",
    "train_cluster = train_processed[train_processed['cluster'] == cluster]\n",
    "\n",
    "val_cluster = val[val['cluster'] == cluster]\n",
    "\n",
    "cluster, cluster_0_predictions, params = train_and_predict(params, train_cluster, val_cluster, video_age_dict, **{'cluster': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>watch_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>8825</td>\n",
       "      <td>1.469138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>2739</td>\n",
       "      <td>0.881861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>7328</td>\n",
       "      <td>1.561320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>2677</td>\n",
       "      <td>0.679518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>8773</td>\n",
       "      <td>1.398021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>7162</td>\n",
       "      <td>8814</td>\n",
       "      <td>1.488533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>7162</td>\n",
       "      <td>5901</td>\n",
       "      <td>1.132553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>7162</td>\n",
       "      <td>5681</td>\n",
       "      <td>1.397783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>7162</td>\n",
       "      <td>5776</td>\n",
       "      <td>1.023636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>7162</td>\n",
       "      <td>7191</td>\n",
       "      <td>0.949543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110298 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id video_id  watch_ratio\n",
       "0        14     8825     1.469138\n",
       "1        14     2739     0.881861\n",
       "2        14     7328     1.561320\n",
       "3        14     2677     0.679518\n",
       "4        14     8773     1.398021\n",
       "..      ...      ...          ...\n",
       "725    7162     8814     1.488533\n",
       "726    7162     5901     1.132553\n",
       "727    7162     5681     1.397783\n",
       "728    7162     5776     1.023636\n",
       "729    7162     7191     0.949543\n",
       "\n",
       "[110298 rows x 3 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot the distribution of predicted watch ratios\n",
    "# cluster_0_predictions['watch_ratio'].hist(bins=50)\n",
    "\n",
    "cluster_0_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there a way to parallelise this?\n",
    "def train_by_cluster_and_without(params: dict, train_data: pd.DataFrame, val_data: pd.DataFrame, video_age_dict: dict,\n",
    "                                     train_by_cluster: bool = True, train_without_clustering: bool = False):\n",
    "    param_str = '_'.join([f'{key}{val}' for key, val in params.items()])\n",
    "\n",
    "    # Train for each cluster\n",
    "    if train_by_cluster:\n",
    "        cluster_predictions = {}\n",
    "        for cluster in sorted(train_data['cluster'].unique()):\n",
    "            train_cluster = train_data[train_data['cluster'] == cluster]\n",
    "            val_cluster = val_data[val_data['cluster'] == cluster]\n",
    "\n",
    "            cluster, predictions_df = train_and_predict(params, train_cluster, val_cluster, video_age_dict, **{'cluster': cluster})\n",
    "            cluster_predictions[cluster] = predictions_df\n",
    "        \n",
    "        # Combine predictions\n",
    "        watch_ratio_predictions_df = pd.DataFrame()\n",
    "        for cluster, df in cluster_predictions.items():\n",
    "            cluster_predictions_df = df\n",
    "            cluster_predictions_df['cluster'] = cluster\n",
    "            \n",
    "            watch_ratio_predictions_df = pd.concat([watch_ratio_predictions_df, cluster_predictions_df])\n",
    "        \n",
    "        # Save predictions\n",
    "        output_file = root + f'results/w_clustering_{param_str}.csv'\n",
    "        watch_ratio_predictions_df.to_csv(output_file, index=False)\n",
    "        print(f'Predictions with segmentation saved to {output_file}')\n",
    "    \n",
    "    # Train without clustering\n",
    "    if train_without_clustering:\n",
    "        _, predictions_df = train_and_predict(params, train_data, val_data, video_age_dict)\n",
    "\n",
    "        # Save predictions\n",
    "        output_file = root + f'results/wo_clustering_{param_str}.csv'\n",
    "        predictions_df.to_csv(output_file, index=False)\n",
    "        print(f'Predictions without segmentation saved to {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hyperparameters: {'batch_size': 512, 'num_epochs': 30, 'lr': 0.001, 'embedding_dim': 64, 'dropout': 0.3, 'decay': 0.01}\n",
      "----- Training for cluster 0 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/30], Loss: 0.4148\n",
      "Epoch [2/30], Loss: 0.3706\n",
      "Epoch [3/30], Loss: 0.3628\n",
      "Epoch [4/30], Loss: 0.3572\n",
      "Epoch [5/30], Loss: 0.3528\n",
      "Epoch [6/30], Loss: 0.3492\n",
      "Epoch [7/30], Loss: 0.3465\n",
      "Epoch [8/30], Loss: 0.3433\n",
      "Epoch [9/30], Loss: 0.3405\n",
      "Epoch [10/30], Loss: 0.3368\n",
      "Epoch [11/30], Loss: 0.3333\n",
      "Epoch [12/30], Loss: 0.3294\n",
      "Epoch [13/30], Loss: 0.3258\n",
      "Epoch [14/30], Loss: 0.3220\n",
      "Epoch [15/30], Loss: 0.3178\n",
      "Epoch [16/30], Loss: 0.3139\n",
      "Epoch [17/30], Loss: 0.3097\n",
      "Epoch [18/30], Loss: 0.3056\n",
      "Epoch [19/30], Loss: 0.3013\n",
      "Epoch [20/30], Loss: 0.2969\n",
      "Epoch [21/30], Loss: 0.2928\n",
      "Epoch [22/30], Loss: 0.2886\n",
      "Epoch [23/30], Loss: 0.2853\n",
      "Epoch [24/30], Loss: 0.2813\n",
      "Epoch [25/30], Loss: 0.2772\n",
      "Epoch [26/30], Loss: 0.2735\n",
      "Epoch [27/30], Loss: 0.2706\n",
      "Epoch [28/30], Loss: 0.2667\n",
      "Epoch [29/30], Loss: 0.2638\n",
      "Epoch [30/30], Loss: 0.2603\n",
      "Generating predictions...\n",
      "Complete!\n",
      "----- Training for cluster 1 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/30], Loss: 0.2209\n",
      "Epoch [2/30], Loss: 0.1858\n",
      "Epoch [3/30], Loss: 0.1789\n",
      "Epoch [4/30], Loss: 0.1753\n",
      "Epoch [5/30], Loss: 0.1730\n",
      "Epoch [6/30], Loss: 0.1712\n",
      "Epoch [7/30], Loss: 0.1697\n",
      "Epoch [8/30], Loss: 0.1680\n",
      "Epoch [9/30], Loss: 0.1668\n",
      "Epoch [10/30], Loss: 0.1652\n",
      "Epoch [11/30], Loss: 0.1633\n",
      "Epoch [12/30], Loss: 0.1619\n",
      "Epoch [13/30], Loss: 0.1601\n",
      "Epoch [14/30], Loss: 0.1581\n",
      "Epoch [15/30], Loss: 0.1566\n",
      "Epoch [16/30], Loss: 0.1547\n",
      "Epoch [17/30], Loss: 0.1530\n",
      "Epoch [18/30], Loss: 0.1512\n",
      "Epoch [19/30], Loss: 0.1494\n",
      "Epoch [20/30], Loss: 0.1477\n",
      "Epoch [21/30], Loss: 0.1461\n",
      "Epoch [22/30], Loss: 0.1444\n",
      "Epoch [23/30], Loss: 0.1425\n",
      "Epoch [24/30], Loss: 0.1410\n",
      "Epoch [25/30], Loss: 0.1396\n",
      "Epoch [26/30], Loss: 0.1381\n",
      "Epoch [27/30], Loss: 0.1369\n",
      "Epoch [28/30], Loss: 0.1350\n",
      "Epoch [29/30], Loss: 0.1339\n",
      "Epoch [30/30], Loss: 0.1327\n",
      "Generating predictions...\n",
      "Complete!\n",
      "----- Training for cluster 2 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/30], Loss: 0.2768\n",
      "Epoch [2/30], Loss: 0.2393\n",
      "Epoch [3/30], Loss: 0.2318\n",
      "Epoch [4/30], Loss: 0.2267\n",
      "Epoch [5/30], Loss: 0.2235\n",
      "Epoch [6/30], Loss: 0.2218\n",
      "Epoch [7/30], Loss: 0.2199\n",
      "Epoch [8/30], Loss: 0.2184\n",
      "Epoch [9/30], Loss: 0.2166\n",
      "Epoch [10/30], Loss: 0.2144\n",
      "Epoch [11/30], Loss: 0.2123\n",
      "Epoch [12/30], Loss: 0.2100\n",
      "Epoch [13/30], Loss: 0.2076\n",
      "Epoch [14/30], Loss: 0.2052\n",
      "Epoch [15/30], Loss: 0.2023\n",
      "Epoch [16/30], Loss: 0.1998\n",
      "Epoch [17/30], Loss: 0.1971\n",
      "Epoch [18/30], Loss: 0.1942\n",
      "Epoch [19/30], Loss: 0.1912\n",
      "Epoch [20/30], Loss: 0.1888\n",
      "Epoch [21/30], Loss: 0.1858\n",
      "Epoch [22/30], Loss: 0.1835\n",
      "Epoch [23/30], Loss: 0.1811\n",
      "Epoch [24/30], Loss: 0.1785\n",
      "Epoch [25/30], Loss: 0.1759\n",
      "Epoch [26/30], Loss: 0.1736\n",
      "Epoch [27/30], Loss: 0.1715\n",
      "Epoch [28/30], Loss: 0.1693\n",
      "Epoch [29/30], Loss: 0.1674\n",
      "Epoch [30/30], Loss: 0.1657\n",
      "Generating predictions...\n",
      "Complete!\n",
      "----- Training for cluster 3 -----\n",
      "Initialising...\n",
      "Model moved to cpu\n",
      "Epoch [1/30], Loss: 0.2693\n",
      "Epoch [2/30], Loss: 0.2383\n",
      "Epoch [3/30], Loss: 0.2311\n",
      "Epoch [4/30], Loss: 0.2272\n",
      "Epoch [5/30], Loss: 0.2250\n",
      "Epoch [6/30], Loss: 0.2232\n",
      "Epoch [7/30], Loss: 0.2211\n",
      "Epoch [8/30], Loss: 0.2195\n",
      "Epoch [9/30], Loss: 0.2177\n",
      "Epoch [10/30], Loss: 0.2159\n",
      "Epoch [11/30], Loss: 0.2133\n",
      "Epoch [12/30], Loss: 0.2111\n",
      "Epoch [13/30], Loss: 0.2088\n",
      "Epoch [14/30], Loss: 0.2066\n",
      "Epoch [15/30], Loss: 0.2035\n",
      "Epoch [16/30], Loss: 0.2010\n",
      "Epoch [17/30], Loss: 0.1984\n",
      "Epoch [18/30], Loss: 0.1957\n",
      "Epoch [19/30], Loss: 0.1934\n",
      "Epoch [20/30], Loss: 0.1909\n",
      "Epoch [21/30], Loss: 0.1882\n",
      "Epoch [22/30], Loss: 0.1856\n",
      "Epoch [23/30], Loss: 0.1830\n",
      "Epoch [24/30], Loss: 0.1805\n",
      "Epoch [25/30], Loss: 0.1787\n",
      "Epoch [26/30], Loss: 0.1757\n",
      "Epoch [27/30], Loss: 0.1737\n",
      "Epoch [28/30], Loss: 0.1718\n",
      "Epoch [29/30], Loss: 0.1700\n",
      "Epoch [30/30], Loss: 0.1678\n",
      "Generating predictions...\n",
      "Complete!\n",
      "Predictions with segmentation saved to ../KuaiRec 2.0/results/w_clustering_batch_size512_num_epochs30_lr0.001_embedding_dim64_dropout0.3_decay0.01.csv\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters = {\n",
    "#     'batch_size': [256, 512],\n",
    "#     'num_epochs': [10],\n",
    "#     'lr': [0.001, 0.01],\n",
    "#     'embedding_dim': [32, 64],\n",
    "#     'dropout': [0.3, 0.5],\n",
    "#     'alpha': [0.01, 0.05],\n",
    "#     'beta': [0.01, 0.05]\n",
    "# }\n",
    "hyperparameters = {\n",
    "    'batch_size': [512],\n",
    "    'num_epochs': [30],\n",
    "    'lr': [0.001],\n",
    "    'embedding_dim': [64],\n",
    "    'dropout': [0.3],\n",
    "    'decay': [0.01]\n",
    "}\n",
    "\n",
    "# Generate all possible combinations of hyperparameters\n",
    "param_combinations = list(itertools.product(*hyperparameters.values()))\n",
    "\n",
    "# Train for each combination of hyperparameters\n",
    "for params in param_combinations:\n",
    "    params_dict = {key: val for key, val in zip(hyperparameters.keys(), params)}\n",
    "    print(f\"Training with hyperparameters: {params_dict}\")\n",
    "    train_by_cluster_and_without(params_dict, train_processed, val, video_age_dict, train_by_cluster=True, train_without_clustering=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size': [512],\n",
    "    'num_epochs': [10],\n",
    "    'lr': [0.001],\n",
    "    'embedding_dim': [64],\n",
    "    'dropout': [0.3],\n",
    "    'alpha': [0.001],\n",
    "    'beta': [0.001]\n",
    "}\n",
    "\n",
    "train_by_cluster_and_without(params, train_processed, train_by_cluster=True, train_without_clustering=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
